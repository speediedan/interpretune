from typing import List, Optional, Tuple, Union, Dict, NamedTuple
from collections import defaultdict
from pathlib import Path
import yaml

from tests.base_defaults import default_test_bs, default_prof_bs, default_test_task
from tests.utils import get_model_input_dtype

################################################################################
# Test Dataset Fingerprint Definitions
################################################################################

########################################################################################################################
# NOTE [Test Dataset Fingerprint]:
# A simple fingerprint of the (deterministic) test dataset used to generate the current incarnation of expected results.
# Useful for validating that the test dataset has not changed wrt the test dataset used to generate the reference
# results. A few things to note:
#   - There are two independent layers ``datasets`` caching involved in test dataset loading.
#      - The first layer is the caching is associated with the ``prepare_data`` function. The ``prepare_data`` function
#        (which can be invoked manually with ``force_prepare_data=True``) is used to prepare a given test dataset and
#        limit the samples of a sub-dataset/split to a small number (i.e., currently 10). Different test contexts will
#        require different ``model_input_names`` and so for each such context there is a corresponding separate
#        ``pytest_{task}_{tl,hf,pt}`` dataset directory.
#      - The second layer is the caching of dataset transformations that are applied to the previously prepared dataset
#        by passing a ``tokenization_func`` to ``dataset[split_name].map()``. These transformations input the
#        configured ``tokenizer`` (among other kwargs) into their associated fingerprint hash functions and so can be
#        shared among many different transformation contexts. Note since this caching layer occurs after loading the
#        the relevant custom test dataset saved from a previous ``prepare_data`` though, either the correct custom test
#        dataset (e.g. ``pytest_rte_tl``) still needs to be used or the custom test dataset needs to be regenerated by
#        deleting the local ``pytest_{task}_{tl,hf,pt}`` dataset directory or setting
#        ``BaseTestDataModule.force_prepare_data=True``. See NOTE [HF Datasets Transformation Caching] for more.
#   - The dataloader kwargs are not currently part of these fingerprint so if the loss of a given test diverges
#      from expectation, one may still need to verify shuffling of the fingerprinted dataset etc. has not been
#      introduced and compare the examples actually passed to the model in a given test/step to the ids below before
#      subsequently assessing other sources of indeterminism that could be the source of the loss change.
#   - One should see `tests.tools.core.modules.TestITDataModule.sample_dataset_state()` for the indices used to generate
#      this fingerprint
#   - The fingerprinted dataset below is not shuffled or sorted with the current dataloader configurations
#   - All current expected loss results were generated with [train|eval]_batch_size = 2
#   - All current memory profile results were generated with [train|eval]_batch_size = 1
NUM_SAMPLE_ROWS = 5
SAMPLE_POSITION = 3
example_test_datasets = ("rte", "pytest_rte_hf", "pytest_rte_pt", "pytest_rte_tl")
rte_fields = ("premise", "hypothesis")
TEST_TASK_NUM_LABELS = {k: 2 for k in example_test_datasets}
TEST_TASK_TEXT_FIELD_MAP = {k: rte_fields for k in example_test_datasets}
# note that we also sample the 'test' split after 'train' and 'validation' though we aren't yet using it
deterministic_token_ids = {
    ("rte", "GPT2TokenizerFast"): [5674, 24140, 373, 666, 2233, 303, 783, 783, 2055, 319, 373, 910, 17074, 284, 6108]
}

def get_exp_token_ids(ds_cfg: str, task_name: str, tokenizer_cls_name: str):
    expected_token_ids = deterministic_token_ids[(task_name, tokenizer_cls_name)]
    match ds_cfg:
        case "no_sample":
            return ([], [])
        case "train":
            return (expected_token_ids, expected_token_ids[:default_test_bs],)
        case "train_prof":
            return (expected_token_ids, expected_token_ids[:default_prof_bs],)
        case "test":
            return (expected_token_ids, expected_token_ids[NUM_SAMPLE_ROWS:(NUM_SAMPLE_ROWS+default_test_bs)],)
        case "test_prof":
            return (expected_token_ids, expected_token_ids[NUM_SAMPLE_ROWS:(NUM_SAMPLE_ROWS+default_prof_bs)],)


# TODO: add current dataloader kwargs to the fingerprint above? May be an excessively rigid check. Consider associating
# a fingerprint of salient config with each specific expected scalar test result in the future. At present, that
# approach seems like overkill given the current codebase.
########################################################################################################################
class MemProfResult(NamedTuple):
    # encapsulate memprofiler result defaults
    # we default to step:
    #   - 3 for cuda in the train phase
    #   - 0 for all other tests (e.g. hook-based (by default, cpu/rss-based) and all test phase assessment)
    # all tests currently default to test epoch 0 to minimize TTS
    epoch = 0
    rank = 0
    default_step = 0
    cuda_train_step = 3
    cuda_mem_keys = ('allocated_bytes.all.current', 'allocated_bytes.all.peak', 'reserved_bytes.all.peak', 'npp_diff')
    cpu_mem_keys = {"test": ('rss_diff',), "train": ('rss_diff', 'npp_diff'),}
    test_key = f'{rank}.test_step.{epoch}.{default_step}.end'
    train_keys = {"cuda": f'{rank}.training_step.{epoch}.{cuda_train_step}.end',
                  "cpu": f'{rank}.training_step.{epoch}.{default_step}.end'}

################################################################################
# Expected result generation and encapsulation
################################################################################

class TestResult(NamedTuple):
    result_alias: Optional[str] = None  # N.B. diff test aliases may map to the same result alias (e.g. parity tests)
    exact_results: Optional[Dict] = None
    close_results: Optional[Tuple] = None
    mem_results: Optional[Dict] = None
    tolerance_map: Optional[Dict[str, float]] = None
    callback_results: Optional[Dict] = None


def mem_results(results: Dict, test_alias: str):
    """Result generation function for memory profiling tests."""
    # See NOTE [Memprofiler Key Format]
    # snap keys are rank.phase.epoch_idx.step_idx.step_ctx
    expected_results = results[test_alias]
    src, phase, expected_mem = expected_results['src'], expected_results['phase'], expected_results['expected_mem']
    step_key = f'{MemProfResult.train_keys[src]}' if phase == 'train' else f'{MemProfResult.test_key}'
    # default tolerance of rtol=0.1, atol=0 for all keys unless overridden with an explicit `tolerance_map`
    tolerance_map = {'tolerance_map': {k: (0.1, 0) for k in expected_mem.keys()}}
    return {**tolerance_map, 'expected_memstats': (step_key, expected_mem)}

def close_results(close_map: Tuple, test_alias: Optional[str] = None):
    """Result generation function that packages expected close results with a provided tolerance dict or generates
    a default one based upon the test_alias."""
    expected_close = defaultdict(dict)
    close_keys = set()
    for e, k, v in close_map:
        expected_close[e][k] = v
        close_keys.add(k)
    closestats_tol = {'tolerance_map': {k: (0.1, 0) for k in close_keys}}
    return {**closestats_tol, 'expected_close': expected_close}

def exact_results(expected_exact: Tuple, test_alias: Optional[str] = None):
    """Result generation function that packages."""
    return {'expected_exact': expected_exact}

def callback_results(callback_results: Dict, test_alias: Optional[str] = None):
    """Result generation function that packages."""
    return {'callback_results': callback_results}

class DatasetState(NamedTuple):
    tokenizer_name: str
    deterministic_token_ids: List[int]
    expected_first_fwd_ids: List

def def_results(device_type: str, precision: Union[int, str], ds_cfg: str = "no_sample",
                task_name: str = default_test_task, tokenizer_cls_name: str = "GPT2TokenizerFast"):
    test_dataset_state = DatasetState(tokenizer_cls_name, *get_exp_token_ids(ds_cfg, task_name, tokenizer_cls_name))
    # wrap result dict such that only the first epoch is checked
    return {0: {"device_type": device_type, "precision": get_model_input_dtype(precision),
                "dataset_state": test_dataset_state}}

RESULT_TYPE_MAPPING = {
    "exact_results": exact_results,
    "close_results": close_results,
    "mem_results": mem_results,
    "callback_results": callback_results,
}

def parity_normalize(test_alias) -> str:
    parity_suffixes = ("_l",)
    for ps in parity_suffixes:
        if test_alias.endswith(ps):
            test_alias = test_alias[:-len(ps)]
            break
    return test_alias

def collect_results(result_map: Dict[str, Tuple], test_alias: str, normalize: bool = True):
    if normalize:
        test_alias = parity_normalize(test_alias)
    test_result: TestResult = result_map[test_alias]
    collected_results = defaultdict(dict)
    for rtype, rfunc in RESULT_TYPE_MAPPING.items():
        if rattr := getattr(test_result, rtype):
            collected_results.update(rfunc(rattr, test_alias=test_alias))
    if exp_tol := test_result.tolerance_map:
        collected_results['tolerance_map'].update(exp_tol)
    return collected_results

MEMORY_FOOTPRINTS_PATH = Path(__file__).parent / "parity_acceptance" / "profile_memory_footprints.yaml"

def load_memory_footprint_results() -> Dict:
    with open(MEMORY_FOOTPRINTS_PATH) as file:
        # Load the YAML file content
        data = yaml.safe_load(file)
        mem_footprint = {}
        for fq_alias, rv in data.items():
            # TODO: structured this way to allow us to serialize other result types in the future
            mem_footprint[fq_alias] = rv["mem_results"]
    return mem_footprint

def save_memory_footprint_results(results: Dict):
    mem_footprint = {}
    for fq_alias, rv in results.items():
        mem_footprint[fq_alias] = {"mem_results": rv}
    with open(MEMORY_FOOTPRINTS_PATH, "w") as file:
        yaml.dump(mem_footprint, file)
