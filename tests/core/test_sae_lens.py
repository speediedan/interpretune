from typing import List, Literal
from copy import deepcopy
from functools import reduce, partial
import pytest

import torch
from transformer_lens.ActivationCache import ActivationCache
from transformer_lens.utils import get_device as tl_get_device
from sae_lens.saes.sae import SAEMetadata

from interpretune.utils import MisconfigurationException
from interpretune.session import ITSession
from interpretune.config import (
    SAELensFromPretrainedConfig,
    SAELensConfig,
    SAELensCustomConfig,
    ITLensFromPretrainedConfig,
    ITLensCustomConfig,
)
from tests.utils import ablate_cls_attrs
from tests.base_defaults import default_test_task
from tests.runif import RunIf

# Add new imports for mocking
from unittest.mock import patch, MagicMock, call


# simple counter hook
class Counter:
    def __init__(self):
        self.count = 0

    def inc(self, *args, **kwargs):  # type: ignore
        self.count += 1


# TODO: change the fixtures below to be generated by a fixture factory if adding additional variants
#       (keeping explicit for now given the simplicity/efficiency tradeoff)
@pytest.fixture(scope="class")
def sl_gpt2_w_ref_logits(get_it_session__sl_gpt2__initonly):
    fixture = get_it_session__sl_gpt2__initonly
    sl_test_module = fixture.it_session.module
    return sl_test_module, TestClassSAELens.get_ref_logits(sl_test_module)


@pytest.fixture(scope="class")
def l_sl_gpt2_w_ref_logits(get_it_session__l_sl_gpt2__initonly):
    fixture = get_it_session__l_sl_gpt2__initonly
    sl_test_module = fixture.it_session.module
    return sl_test_module, TestClassSAELens.get_ref_logits(sl_test_module)


core_l_run_w_pytest_cfg = {
    "argvalues": [pytest.param("sl_gpt2_w_ref_logits"), pytest.param("l_sl_gpt2_w_ref_logits")],
    "ids": ["core", "lightning"],
}


class TestClassSAELens:
    sl_tokenizer_kwargs = {
        "add_bos_token": True,
        "local_files_only": False,
        "padding_side": "left",
        "model_input_names": ["input_ids", "attention_mask"],
    }
    test_sl_signature_columns = [
        "inputs",
        "attention_mask",
        "position_ids",
        "past_key_values",
        "inputs_embeds",
        "labels",
        "use_cache",
        "output_attentions",
        "output_hidden_states",
        "return_dict",
    ]
    test_tl_gpt2_shared_config = dict(
        task_name=default_test_task,
        tokenizer_kwargs=sl_tokenizer_kwargs,
        model_name_or_path="gpt2",
        tokenizer_id_overrides={"pad_token_id": 50256},
    )
    test_tlens_gpt2 = {
        **test_tl_gpt2_shared_config,
        "tl_cfg": ITLensFromPretrainedConfig(),
        "hf_from_pretrained_cfg": dict(
            pretrained_kwargs={"device_map": "cpu", "torch_dtype": "float32"}, model_head="transformers.GPT2LMHeadModel"
        ),
    }
    test_sl_from_pretrained = SAELensFromPretrainedConfig(
        release="gpt2-small-res-jb", sae_id="blocks.0.hook_resid_pre", device="cpu"
    )
    # tiny custom tl model to attach custom SAE to
    test_tl_cust_config = {
        "n_layers": 1,
        "d_mlp": 10,
        "d_model": 10,
        "d_head": 5,
        "n_heads": 2,
        "n_ctx": 200,
        "act_fn": "relu",
        "tokenizer_name": "gpt2",
    }

    test_sae_metadata = SAEMetadata(
        model_name="cust", hook_name="blocks.0.hook_resid_pre", hook_head_index=None, context_size=200, prepend_bos=True
    )
    test_sae_cust_config = dict(
        architecture="standard",
        d_in=10,
        d_sae=10 * 2,
        dtype="float32",
        device="cpu",
        normalize_activations="none",
        metadata=test_sae_metadata,
    )
    test_sl_cust_config = SAELensCustomConfig(cfg=test_sae_cust_config)
    test_sl_gpt2 = {**test_tl_gpt2_shared_config, **test_tlens_gpt2, "sae_cfgs": test_sl_from_pretrained}
    test_sl_cust = {
        **test_tl_gpt2_shared_config,
        "tl_cfg": ITLensCustomConfig(cfg=test_tl_cust_config),
        "sae_cfgs": test_sl_cust_config,
    }
    prompt = "hello world"

    @staticmethod
    def get_ref_logits(test_module):
        return test_module.model(TestClassSAELens.prompt)

    def test_from_pretrained_device_init(self):
        test_sl_from_pretrained = SAELensFromPretrainedConfig(
            release="gpt2-small-res-jb", sae_id="blocks.0.hook_resid_pre", device=None
        )
        assert test_sl_from_pretrained.device == str(tl_get_device())

    @pytest.mark.parametrize("session_fixture", **core_l_run_w_pytest_cfg)
    def test_run_with_saes_with_cache_fwd_bwd(self, request, session_fixture):
        sl_test_module, _ = request.getfixturevalue(session_fixture)
        assert sl_test_module.model.unembed.b_U.requires_grad, (
            "Previous fixture may have polluted this test, requires_grad should be True"
        )
        filter_sae_acts = lambda name: "hook_sae_acts_post" in name
        cache_dict = {"fwd": {}, "bwd": {}}
        # TODO: maybe add a metric to extend test/make it more realistic

        def cache_hook(act, hook, dir: Literal["fwd", "bwd"]):
            cache_dict[dir][hook.name] = act.detach()

        with sl_test_module.model.saes(saes=sl_test_module.sae_handles):
            # We add hooks to cache values from the forward and backward pass respectively
            with sl_test_module.model.hooks(
                fwd_hooks=[(filter_sae_acts, partial(cache_hook, dir="fwd"))],
                bwd_hooks=[(filter_sae_acts, partial(cache_hook, dir="bwd"))],
            ):
                # fill fwd/bwd cache, hooks then removed on cm exit
                out = sl_test_module.model(TestClassSAELens.prompt)

                # Verify gradients are enabled before backward pass
                # assert out.requires_grad, "Output tensor does not have requires_grad=True"
                out[0, -1, 42].backward()

        cache_dict = {k: ActivationCache(cache_dict[k], sl_test_module.model) for k in cache_dict.keys()}
        for cache in cache_dict.values():
            assert isinstance(cache, ActivationCache)
            for sae_handle in sl_test_module.sae_handles:
                assert sae_handle.name + ".hook_sae_acts_post" in cache

    @pytest.mark.parametrize("session_fixture", **core_l_run_w_pytest_cfg)
    def test_run_with_saes(self, request, session_fixture):
        sl_test_module, original_logits = request.getfixturevalue(session_fixture)
        assert len(sl_test_module.model.acts_to_saes) == 0
        logits_with_saes = sl_test_module.model.run_with_saes(TestClassSAELens.prompt, saes=sl_test_module.sae_handles)
        assert len(sl_test_module.model.acts_to_saes) == 0
        assert not torch.allclose(logits_with_saes, original_logits)

    @pytest.mark.parametrize("session_fixture", **core_l_run_w_pytest_cfg)
    def test_run_with_cache_with_saes(self, request, session_fixture):
        sl_test_module, original_logits = request.getfixturevalue(session_fixture)
        assert len(sl_test_module.model.acts_to_saes) == 0
        logits_with_saes, cache = sl_test_module.model.run_with_cache_with_saes(
            TestClassSAELens.prompt, saes=sl_test_module.sae_handles
        )
        assert not torch.allclose(logits_with_saes, original_logits)
        assert isinstance(cache, ActivationCache)
        for sae_handle in sl_test_module.sae_handles:
            assert sae_handle.name + ".hook_sae_acts_post" in cache

    @pytest.mark.parametrize("session_fixture", **core_l_run_w_pytest_cfg)
    def test_run_with_hooks_with_saes(self, request, session_fixture):
        sl_test_module, original_logits = request.getfixturevalue(session_fixture)
        c = Counter()
        assert len(sl_test_module.model.acts_to_saes) == 0
        logits_with_saes = sl_test_module.model.run_with_hooks_with_saes(
            TestClassSAELens.prompt,
            saes=sl_test_module.sae_handles,
            fwd_hooks=[
                (sae_handle.cfg.metadata.hook_name + ".hook_sae_acts_post", c.inc)
                for sae_handle in sl_test_module.sae_handles
            ],
        )
        assert not torch.allclose(logits_with_saes, original_logits)
        assert c.count == len(sl_test_module.sae_handles)

    @RunIf(lightning=True)
    def test_sl_module_warns(self, get_it_session__l_sl_gpt2__initonly):
        fixture = get_it_session__l_sl_gpt2__initonly
        sl_test_module = fixture.it_session.module
        with ablate_cls_attrs(sl_test_module.it_cfg, "sae_cfgs"), pytest.warns(UserWarning, match="Could not find a"):
            _ = sl_test_module.sae_cfgs

    @staticmethod
    def cfg_saes_like(orig_sae_cfg, new_hook_points: List):
        new_sae_cfgs = []
        sae_attr = "sae_id" if isinstance(orig_sae_cfg, SAELensFromPretrainedConfig) else "cfg.hook_name"
        for hook_override in new_hook_points:
            new_sae_cfg = deepcopy(orig_sae_cfg)
            attr_parts = sae_attr.rpartition(".")
            new_sae_cfg_ref = reduce(getattr, attr_parts[0].split("."), new_sae_cfg) if attr_parts[0] else new_sae_cfg
            setattr(new_sae_cfg_ref, attr_parts[-1], hook_override)
            new_sae_cfgs.append(new_sae_cfg)
        return new_sae_cfgs

    @pytest.mark.parametrize(
        "session_cfg_fixture, sl_pretrained, add_saes_on_init",
        [
            pytest.param("get_it_session_cfg__sl_gpt2", False, False),
            pytest.param("get_it_session_cfg__sl_gpt2", False, True),
            pytest.param("get_it_session_cfg__sl_gpt2", True, False),
            pytest.param("get_it_session_cfg__sl_gpt2", True, True),
            pytest.param("get_it_session_cfg__sl_cust", False, False),
            pytest.param("get_it_session_cfg__sl_cust", False, True),
        ],
        ids=[
            "tlpre_slcust_noadd",
            "tlpre_slcust_add",
            "tlpre_slgpt2_noadd",
            "tlpre_slgpt2_add",
            "tlcust_slcust_noadd",
            "tlcust_slcust_add",
        ],
    )
    def test_sl_module_init(self, request, session_cfg_fixture, sl_pretrained, add_saes_on_init):
        it_session_cfg = request.getfixturevalue(session_cfg_fixture)
        sess_cfg = deepcopy(it_session_cfg)
        if not sl_pretrained:
            sess_cfg.module_cfg.sae_cfgs = [TestClassSAELens.test_sl_cust_config]
        sess_cfg.module_cfg.add_saes_on_init = add_saes_on_init
        test_mod = ITSession(sess_cfg).module
        expected_num_saes = len(sess_cfg.module_cfg.sae_cfgs)
        if add_saes_on_init:
            assert len(test_mod.model.acts_to_saes) == expected_num_saes
            assert test_mod.model.acts_to_saes[test_mod.it_cfg.normalized_sae_cfg_refs[-1]] == test_mod.sae_handles[-1]
        else:
            assert len(test_mod.model.acts_to_saes) == 0
        assert len(test_mod.sae_handles) == len(test_mod.saes) == expected_num_saes

    def test_sl_basic_cfg(self):
        test_sl_cfg = deepcopy(TestClassSAELens.test_sl_cust)
        it_cfg = SAELensConfig(**test_sl_cfg)
        assert it_cfg
        test_sl_cfg.update({"sae_cfgs": None})
        with pytest.raises(MisconfigurationException, match="At least one `SAELens"):
            _ = SAELensConfig(**test_sl_cfg)

    @RunIf(min_cuda_gpus=1)
    def test_sl_tl_device_sync_warnings(self):
        test_sl_cfg = deepcopy(TestClassSAELens.test_sl_cust)
        with pytest.warns(UserWarning, match=r"This SAEConfig's device type \('cpu'\) does not match the"):
            it_cfg = SAELensConfig(**test_sl_cfg)
        assert isinstance(it_cfg, SAELensConfig)
        assert it_cfg.sae_cfgs[0].cfg.device in ["cuda", "mps"]
        test_sl_cfg["sae_cfgs"].cfg.device = None
        with pytest.warns(UserWarning, match=r"An SAEConfig device type was not provided"):
            it_cfg = SAELensConfig(**test_sl_cfg)
        assert isinstance(it_cfg, SAELensConfig)
        assert it_cfg.sae_cfgs[0].cfg.device in ["cuda", "mps"]

    @patch("interpretune.adapters.sae_lens.get_pretrained_saes_directory")
    @patch("interpretune.adapters.sae_lens.display")
    @patch("interpretune.adapters.sae_lens.IFrame")
    @patch("interpretune.adapters.sae_lens.print")
    def test_display_dashboard(self, mock_print, mock_iframe, mock_display, mock_get_dir):
        """Test the display_dashboard static method."""
        # Setup mock directory response
        mock_release = MagicMock()
        mock_release.neuronpedia_id = {"blocks.9.hook_resid_pre": "test-neuronpedia-id"}
        mock_get_dir.return_value = {"gpt2-small-res-jb": mock_release}

        # Call the method with default parameters
        from interpretune.adapters.sae_lens import SAEAnalysisMixin

        SAEAnalysisMixin.display_dashboard()

        # Verify URL construction and IFrame parameters
        expected_url = "https://neuronpedia.org/test-neuronpedia-id/0?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300"
        mock_print.assert_called_with(expected_url)
        mock_iframe.assert_called_with(expected_url, width=800, height=600)
        mock_display.assert_called_once()

        # Test with custom parameters
        mock_print.reset_mock()
        mock_iframe.reset_mock()
        mock_display.reset_mock()

        SAEAnalysisMixin.display_dashboard(
            sae_release="gpt2-small-res-jb", sae_id="blocks.9.hook_resid_pre", latent_idx=42, width=1000, height=700
        )

        expected_url = "https://neuronpedia.org/test-neuronpedia-id/42?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300"
        mock_print.assert_called_with(expected_url)
        mock_iframe.assert_called_with(expected_url, width=1000, height=700)
        mock_display.assert_called_once()

    @patch("interpretune.adapters.sae_lens.SAEAnalysisMixin.display_dashboard")
    @patch("interpretune.adapters.sae_lens.print")
    def test_display_latent_dashboards(self, mock_print, mock_display_dashboard):
        """Test the display_latent_dashboards static method."""
        from interpretune.adapters.sae_lens import SAEAnalysisMixin

        # Create mock metrics for testing
        mock_metrics = MagicMock()
        mock_metrics.total_effect = {
            "blocks.1.hook": torch.tensor([0.8, -0.5, 0.3, -0.9, 0.6]),
            "blocks.2.hook": torch.tensor([0.4, -0.7, 0.5, -0.2, 0.1]),
        }
        mock_metrics.num_samples_active = {
            "blocks.1.hook": torch.tensor([10, 15, 20, 25, 30]),
            "blocks.2.hook": torch.tensor([5, 10, 15, 20, 25]),
        }

        # Test with default top_k=1
        SAEAnalysisMixin.display_latent_dashboards(
            metrics=mock_metrics, title="Test Dashboard", sae_release="test-release"
        )

        # Check expected print calls and display_dashboard calls for each hook and direction
        expected_print_calls = [
            call("\nTest Dashboard for blocks.1.hook:"),
            call("\npositive:"),
            call("#0 had total effect 0.80 and was active in 10 examples"),
            call("\nnegative:"),
            call("#3 had total effect -0.90 and was active in 25 examples"),
            call("\nTest Dashboard for blocks.2.hook:"),
            call("\npositive:"),
            call("#2 had total effect 0.50 and was active in 15 examples"),
            call("\nnegative:"),
            call("#1 had total effect -0.70 and was active in 10 examples"),
        ]
        mock_print.assert_has_calls(expected_print_calls)

        # Verify display_dashboard calls
        expected_display_calls = [
            call(sae_release="test-release", sae_id="blocks.1.hook_z", latent_idx=0),
            call(sae_release="test-release", sae_id="blocks.1.hook_z", latent_idx=3),
            call(sae_release="test-release", sae_id="blocks.2.hook_z", latent_idx=2),
            call(sae_release="test-release", sae_id="blocks.2.hook_z", latent_idx=1),
        ]
        mock_display_dashboard.assert_has_calls(expected_display_calls)

        # Test with custom hook_to_sae_id function and top_k=2
        mock_print.reset_mock()
        mock_display_dashboard.reset_mock()

        custom_hook_mapper = lambda hook: f"custom_mapping_for_{hook}"
        SAEAnalysisMixin.display_latent_dashboards(
            metrics=mock_metrics,
            title="Custom Test",
            sae_release="custom-release",
            hook_to_sae_id=custom_hook_mapper,
            top_k=2,
        )

        # Verify custom mapping function was used for sae_id
        custom_display_calls = mock_display_dashboard.call_args_list
        assert any("custom_mapping_for_blocks.1.hook" in str(call) for call in custom_display_calls)
        assert any("custom_mapping_for_blocks.2.hook" in str(call) for call in custom_display_calls)

        # Verify top_k=2 resulted in more display calls (4 per hook: 2 positive + 2 negative)
        assert mock_display_dashboard.call_count == 8
