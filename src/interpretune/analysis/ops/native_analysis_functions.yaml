---
# TODO: we need to more elegantly account for global wrap_summary behavior. For example, any
# batch with an `input` could also serialize `tokens` and `prompts` columns if configured to do so which we
# explicitly define currently but should be introspected during compilation.
labels_to_ids:
  description: Convert label strings to tensor IDs
  implementation: interpretune.analysis.ops.definitions.labels_to_ids_impl
  aliases: []
  output_schema:
    label_ids:
      datasets_dtype: int64
    orig_labels:
      datasets_dtype: int64
  input_schema:
    labels:
      datasets_dtype: string
      connected_obj: datamodule

get_answer_indices:
  description: Extract answer indices from batch
  implementation: interpretune.analysis.ops.definitions.get_answer_indices_impl
  aliases: []
  input_schema:
    input:
      datasets_dtype: int64
      connected_obj: datamodule
    answer_indices:
      datasets_dtype: int64
      required: false
  output_schema:
    answer_indices:
      datasets_dtype: int64

get_alive_latents:
  description: Extract alive latents from cache
  implementation: interpretune.analysis.ops.definitions.get_alive_latents_impl
  required_ops: [get_answer_indices]
  aliases: []
  input_schema:
    cache:
      required: false
      datasets_dtype: object
      non_tensor: true
      intermediate_only: true
    answer_indices:
      required: false
      datasets_dtype: int64
  output_schema:
    alive_latents:
      required: false
      datasets_dtype: int64
      per_sae_hook: true
      non_tensor: true

model_forward:
  description: Basic model forward pass
  implementation: interpretune.analysis.ops.definitions.model_forward_impl
  required_ops: [get_answer_indices]
  aliases: []
  input_schema:
    input:
      datasets_dtype: int64
      connected_obj: datamodule
    answer_indices:
      required: false
      datasets_dtype: int64
  output_schema:
    answer_logits:
      datasets_dtype: float32
      sequence_type: false
      dyn_dim: 1
      dyn_dim_ceil: max_seq_len
      array_shape: [null, batch_size, vocab_size]
    prompts:
      datasets_dtype: string
      required: false
      non_tensor: true

model_cache_forward:
  description: Model forward pass with cache
  implementation: interpretune.analysis.ops.definitions.model_cache_forward_impl
  required_ops: [get_answer_indices, get_alive_latents]
  aliases: ['model_forward_cache']
  input_schema:
    input:
      datasets_dtype: int64
      connected_obj: datamodule
  output_schema:
    answer_logits:
      datasets_dtype: float32
      sequence_type: false
      dyn_dim: 1
      dyn_dim_ceil: max_seq_len
      array_shape: [null, batch_size, vocab_size]
    cache:
      datasets_dtype: object
      non_tensor: true
      intermediate_only: true
    prompts:
      datasets_dtype: string
      required: false
      non_tensor: true

model_ablation:
  description: Model ablation analysis
  implementation: interpretune.analysis.ops.definitions.model_ablation_impl
  importable_params:
    ablate_latent_fn: interpretune.analysis.ops.definitions.ablate_sae_latent
  required_ops: [get_answer_indices, get_alive_latents]
  aliases: []
  input_schema:
    input:
      datasets_dtype: int64
      connected_obj: datamodule
    alive_latents:
      datasets_dtype: int64
      per_sae_hook: true
      non_tensor: true
    answer_indices:
      datasets_dtype: int64
      required: false
  output_schema:
    answer_logits:
      datasets_dtype: float32
      per_latent: true
      sequence_type: false
      array_shape: [batch_size, vocab_size]

model_gradient:
  description: Model gradient-based attribution
  implementation: interpretune.analysis.ops.definitions.model_gradient_impl
  importable_params:
    logit_diff_fn: interpretune.analysis.ops.definitions.boolean_logits_to_avg_logit_diff
    get_loss_preds_diffs: interpretune.analysis.ops.definitions.get_loss_preds_diffs
  required_ops: [get_answer_indices]
  aliases: []
  input_schema:
    input:
      datasets_dtype: int64
      connected_obj: datamodule
    label_ids:
      datasets_dtype: int64
    orig_labels:
      datasets_dtype: int64
  output_schema:
    answer_logits:
      datasets_dtype: float32
      sequence_type: false
      array_shape: [batch_size, max_answer_tokens, num_classes]
    answer_indices:
      datasets_dtype: int64
    loss:
      datasets_dtype: float32
      sequence_type: false
    logit_diffs:
      datasets_dtype: float32
    preds:
      datasets_dtype: int64
    grad_cache:
      datasets_dtype: object
      non_tensor: true
      intermediate_only: true
    prompts:
      datasets_dtype: string
      required: false
      non_tensor: true

logit_diffs:
  description: Clean forward pass for computing logit differences
  implementation: interpretune.analysis.ops.definitions.logit_diffs_impl
  importable_params:
    logit_diff_fn: interpretune.analysis.ops.definitions.boolean_logits_to_avg_logit_diff
    get_loss_preds_diffs: interpretune.analysis.ops.definitions.get_loss_preds_diffs
  aliases: []
  # TODO: remove appropriate output_schema input_schema columns if/when get_loss_preds_diffs is made an op
  input_schema:
    input:
      datasets_dtype: float32
      connected_obj: datamodule
    label_ids:
      datasets_dtype: int64
    orig_labels:
      datasets_dtype: int64
    answer_logits:
      datasets_dtype: float32
      sequence_type: false
      dyn_dim: 1
      dyn_dim_ceil: max_seq_len
      array_shape: [null, batch_size, vocab_size]
    answer_indices:
      datasets_dtype: int64
  output_schema:
    loss:
      datasets_dtype: float32
      sequence_type: false
    logit_diffs:
      datasets_dtype: float32
    preds:
      datasets_dtype: int64
    answer_logits:
      datasets_dtype: float32
      sequence_type: false
      array_shape: [batch_size, max_answer_tokens, num_classes]

logit_diffs_cache:
  description: Clean forward pass for computing logit differences including cache activations (composition only)
  implementation: interpretune.analysis.ops.definitions.logit_diffs_impl
  importable_params:
    logit_diff_fn: interpretune.analysis.ops.definitions.boolean_logits_to_avg_logit_diff
    get_loss_preds_diffs: interpretune.analysis.ops.definitions.get_loss_preds_diffs
  aliases: []
  # TODO: remove appropriate output_schema input_schema columns if/when get_loss_preds_diffs is made an op
  input_schema:
    input:
      datasets_dtype: float32
      connected_obj: datamodule
    answer_logits:
      datasets_dtype: float32
      sequence_type: false
      dyn_dim: 1
      dyn_dim_ceil: max_seq_len
      array_shape: [null, batch_size, vocab_size]
    answer_indices:
      datasets_dtype: int64
    label_ids:
      datasets_dtype: int64
    orig_labels:
      datasets_dtype: int64
    cache:
      datasets_dtype: object
      non_tensor: true
      intermediate_only: true
  output_schema:
    loss:
      datasets_dtype: float32
      sequence_type: false
    logit_diffs:
      datasets_dtype: float32
    preds:
      datasets_dtype: int64
    answer_logits:
      datasets_dtype: float32
      sequence_type: false
      array_shape: [batch_size, max_answer_tokens, num_classes]

sae_correct_acts:
  description: Compute correct activations from SAE cache
  implementation: interpretune.analysis.ops.definitions.sae_correct_acts_impl
  required_ops: [get_alive_latents]
  aliases: []
  input_schema:
    logit_diffs:
      datasets_dtype: float32
    answer_indices:
      datasets_dtype: int64
    cache:
      datasets_dtype: object
      non_tensor: true
      intermediate_only: true
  output_schema:
    correct_activations:
      datasets_dtype: float32
      per_sae_hook: true

gradient_attribution:
  description: Compute attribution values from gradients
  implementation: interpretune.analysis.ops.definitions.gradient_attribution_impl
  required_ops: [get_alive_latents]
  aliases: []
  input_schema:
    input:
      datasets_dtype: float32
      connected_obj: datamodule
    answer_indices:
      datasets_dtype: int64
    logit_diffs:
      datasets_dtype: float32
    grad_cache:
      datasets_dtype: object
      non_tensor: true
      intermediate_only: true
  output_schema:
    attribution_values:
      datasets_dtype: float32
      per_sae_hook: true
    correct_activations:
      datasets_dtype: float32
      per_sae_hook: true
    prompts:
      datasets_dtype: string
      required: false
      non_tensor: true

ablation_attribution:
  description: Compute attribution values from ablation
  implementation: interpretune.analysis.ops.definitions.ablation_attribution_impl
  importable_params:
    logit_diff_fn: interpretune.analysis.ops.definitions.boolean_logits_to_avg_logit_diff
    get_loss_preds_diffs: interpretune.analysis.ops.definitions.get_loss_preds_diffs
  aliases: []
  # TODO: remove appropriate output_schema input_schema columns if/when get_loss_preds_diffs is made an op
  input_schema:
    input:
      datasets_dtype: float32
      connected_obj: datamodule
    answer_indices:
      datasets_dtype: int64
    alive_latents:
      datasets_dtype: int64
      per_sae_hook: true
      non_tensor: true
    logit_diffs:
      datasets_dtype: float32
    answer_logits:
      datasets_dtype: float32
      per_latent: true
      sequence_type: false
      array_shape: [batch_size, vocab_size]
    label_ids:
      datasets_dtype: int64
    orig_labels:
      datasets_dtype: int64
  output_schema:
    attribution_values:
      datasets_dtype: float32
      per_sae_hook: true
    logit_diffs:
      datasets_dtype: float32
      per_latent: true
    answer_logits:
      datasets_dtype: float32
      per_latent: true
      sequence_type: false
      array_shape: [batch_size, max_answer_tokens, num_classes]
    loss:
      datasets_dtype: float32
      per_latent: true
      sequence_type: false
    preds:
      datasets_dtype: int64
      per_latent: true

# Composite operations
composite_operations:
  logit_diffs_base:
    composition: labels_to_ids.model_forward.logit_diffs

  logit_diffs_sae:
    composition: labels_to_ids.model_cache_forward.logit_diffs_cache.sae_correct_acts

  logit_diffs_attr_grad:
    composition: labels_to_ids.model_gradient.gradient_attribution

  logit_diffs_attr_ablation:
    composition: labels_to_ids.model_cache_forward.logit_diffs_cache.model_ablation.ablation_attribution
    aliases: [logit_diffs_ablation]
