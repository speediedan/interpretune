import os
from typing import Optional
from functools import reduce
from copy import deepcopy

import torch
from transformers import PretrainedConfig
from transformer_lens import HookedTransformer, HookedTransformerConfig
from transformer_lens.utilities.devices import get_device_for_block_index
from transformers.tokenization_utils_base import BatchEncoding

from interpretune.adapters import CompositionRegistry, LightningDataModule, LightningModule, LightningAdapter
from interpretune.base import CoreHelperAttributes, ITDataModule, BaseITModule
from interpretune.utils import move_data_to_device, patched_generate, rank_zero_warn, rank_zero_info
from interpretune.protocol import Adapter
from interpretune.config import ITLensCustomConfig


################################################################################
# Mixins to support Transformer Lens in different adapter contexts
################################################################################

class TLensAttributeMixin:
    @property
    def tl_cfg(self) -> Optional[HookedTransformerConfig]:
        try:
            cfg = reduce(getattr, "model.cfg".split("."), self)
        except AttributeError as ae:
            rank_zero_warn(f"Could not find a `HookedTransformerConfig` reference (has it been set yet?): {ae}")
            cfg = None
        return cfg  # type: ignore[return-value]

    # TODO: we aren't using IT's Property Composition feature for TLens yet, but might be worth enabling it
    @property
    def device(self) -> Optional[torch.device]:
        try:
            device = getattr(self._it_state, "_device", None) or getattr(self.tl_cfg, "device", None) or \
                reduce(getattr, "model.device".split("."), self)
        except AttributeError as ae:
            rank_zero_warn(f"Could not find a device reference (has it been set yet?): {ae}")
            device = None
        return device

    @device.setter
    def device(self, value: Optional[str | torch.device]) -> None:
        if value is not None and not isinstance(value, torch.device):
            value = torch.device(value)
        self._it_state._device = value

    def get_tl_device(self, block_index: int) -> Optional[torch.device]:
        try:
            device = get_device_for_block_index(block_index, self.tl_cfg)
        except (AttributeError, AssertionError) as ae:
            rank_zero_warn(f"Problem determining appropriate device for block {block_index} from TransformerLens"
                           f" config. Received: {ae}")
            device = None
        return device

    @property
    def output_device(self) -> Optional[torch.device]:
        return self.get_tl_device(self.model.cfg.n_layers - 1)

    @property
    def input_device(self) -> Optional[torch.device]:
        return self.get_tl_device(0)


class BaseITLensModule(BaseITModule):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        HookedTransformer.generate = patched_generate
        self.loss_fn = None

    def auto_model_init(self) -> None:
        """Can be overridden by subclasses to automatically initialize model from a configuration (e.g.
        hf_from_pretrained_cfg, tl_from_config etc.)."""
        if self.it_cfg.hf_from_pretrained_cfg:
            self.hf_pretrained_model_init()
        else:
            self.tl_config_model_init()

    def hf_pretrained_model_init(self) -> None:
        # for TL, only a subset of the HF pretrained init flow used since the model is replaced with a HookedTransformer
        access_token = os.environ[self.it_cfg.os_env_model_auth_key.upper()] if self.it_cfg.os_env_model_auth_key \
            else None
        quantization_config = super()._hf_configure_quantization()
        super()._update_hf_pretrained_cfg(quantization_config)
        cust_config, _ = super()._hf_gen_cust_config(access_token)
        self.model = self.hf_configured_model_init(cust_config, access_token)
        self._convert_hf_to_tl()

    def hf_configured_model_init(self, cust_config: PretrainedConfig | ITLensCustomConfig,
                                  access_token: Optional[str] = None) -> torch.nn.Module:
        # usually makes sense to init the HookedTransfomer (empty) and pretrained HF model weights on cpu
        # versus moving them both to GPU (may make sense to explore meta device usage for model definition
        # in the future, only materializing parameter by parameter during loading from pretrained weights
        # to eliminate need for two copies in memory)
        # TODO: add warning that TransformerLens only specifying a single device via device  is supported
        # (though the model will automatically be moved to multiple devices if n_devices > 1)
        cust_config.num_labels = self.it_cfg.num_labels
        if (dmap := self.it_cfg.hf_from_pretrained_cfg.pretrained_kwargs.get('device_map', None)) != 'cpu':
            rank_zero_warn('Overriding `device_map` passed to TransformerLens to transform pretrained weights on'
                        f' cpu prior to moving the model to target device: {dmap}')
            self.it_cfg.hf_from_pretrained_cfg.pretrained_kwargs['device_map'] = "cpu"
        model = self.it_cfg.model_class.from_pretrained(**self.it_cfg.hf_from_pretrained_cfg.pretrained_kwargs,
                                                        config=cust_config, token=access_token)
        # perhaps explore initializing on the meta device and then materializing as needed layer by layer during
        # loading/processing into hookedtransformer
        # with torch.device("meta"):
        #     model = self.it_cfg.model_class(config=cust_config)  # token=access_token)
        return model

    def tl_config_model_init(self) -> None:
        # TODO: add note to documentation that we currently require tl_cfg to be not None (either from pretrained or
        #       custom config) based, so model_init will not be used. To fully customize TL behavior, override this
        #       method and init config-based HookedTransformer as desired
        # TODO: suppress messages from tl about no tokenizer here, we're deferring the tokenizer attach until setup
        self.model = HookedTransformer(tokenizer=self.it_cfg.tokenizer, **self.it_cfg.tl_cfg.__dict__)

    def _prune_tl_cfg_dict(self, prune_list: list = None) -> dict:
        """Prunes the tl_cfg dictionary by removing 'hf_model' and 'tokenizer' keys. Asserts that these keys have
        None values, and warns if they don't.

        Returns:
            dict: The pruned dictionary
        """
        prune_list = prune_list or  ['hf_model', 'tokenizer']
        pruned_dict = deepcopy(self.it_cfg.tl_cfg.__dict__)

        for key in prune_list:
            if key in pruned_dict:
                if pruned_dict[key] is not None:
                    rank_zero_warn(f"Found non-None value for '{key}' in tl_cfg. This may cause issues.")
                del pruned_dict[key]

        return pruned_dict

    def _convert_hf_to_tl(self) -> None:
        # TODO: decide whether to pass remaining hf_from_pretrained_cfg args to HookedTransformer
        # (other than torch_dtype which should already have been processed and removed, device_map should also be
        # removed before passing to HookedTransformer)
        # if datamodule is not attached yet, attempt to retrieve tokenizer handle directly from provided it_cfg
        tokenizer_handle = self.datamodule.tokenizer if self.datamodule else self.it_cfg.tokenizer
        hf_preconversion_config = deepcopy(self.model.config)  # capture original hf config before conversion
        pruned_cfg = self._prune_tl_cfg_dict()  # avoid edge case where conflicting keys haven't already been pruned
        self.model = HookedTransformer.from_pretrained(hf_model=self.model, tokenizer=tokenizer_handle, **pruned_cfg)
        self.model.config = hf_preconversion_config

    def _capture_hyperparameters(self) -> None:
        # override unsupported from pretrained options
        if self.hf_cfg:
            self.hf_cfg.lora_cfg = None
            self.hf_cfg.bitsandbytesconfig = None  # TODO NEXT: enable bnb now that it's supported by TransformerLens
        # TODO: refactor the captured config here to only add tl_from_pretrained, other added in superclass
        # TODO: serialize tl_config
        if self.it_cfg.hf_from_pretrained_cfg:
            self._it_state._init_hparams.update({"tl_cfg": self._make_config_serializable(self.it_cfg.tl_cfg, ['device',
                                                                                                          'dtype']),})
        else:
            serializable_tl_cfg = deepcopy(self.it_cfg.tl_cfg)
            serializable_tl_cfg.cfg = self._make_config_serializable(self.it_cfg.tl_cfg.cfg, ['device', 'dtype'])
            self._it_state._init_hparams.update({"tl_cfg": serializable_tl_cfg})
        super()._capture_hyperparameters()

    def set_input_require_grads(self) -> None:
        # not currently supported by ITLensModule
        rank_zero_info("Setting input require grads not currently supported by ITLensModule.")


################################################################################
# Transformer Lens Module Composition
################################################################################

class TransformerLensAdapter(TLensAttributeMixin):

    @classmethod
    def register_adapter_ctx(cls, adapter_ctx_registry: CompositionRegistry) -> None:
        adapter_ctx_registry.register(Adapter.transformer_lens, component_key = "datamodule",
                                        adapter_combination=(Adapter.core, Adapter.transformer_lens),
                                        composition_classes=(ITDataModule,),
                                        description="Transformer Lens adapter that can be composed with core and l...",
        )
        adapter_ctx_registry.register(Adapter.transformer_lens, component_key = "datamodule",
                                        adapter_combination=(Adapter.lightning, Adapter.transformer_lens),
                                        composition_classes=(ITDataModule, LightningDataModule),
                                        description="Transformer Lens adapter that can be composed with core and l...",
        )
        adapter_ctx_registry.register(Adapter.transformer_lens, component_key = "module",
                                        adapter_combination=(Adapter.core, Adapter.transformer_lens),
                                        composition_classes=(ITLensModule,),
                                        description="Transformer Lens adapter that can be composed with core and l...",
        )
        adapter_ctx_registry.register(Adapter.transformer_lens, component_key = "module",
                                        adapter_combination=(Adapter.lightning, Adapter.transformer_lens),
                                        composition_classes=(TLensAttributeMixin, BaseITLensModule, LightningAdapter,
                                                             BaseITModule, LightningModule),
                                        description="Transformer Lens adapter that can be composed with core and l...",
        )

    def batch_to_device(self, batch) -> BatchEncoding:
        move_data_to_device(batch, self.input_device)
        return batch

class ITLensModule(TransformerLensAdapter, CoreHelperAttributes, BaseITLensModule):
    ...
