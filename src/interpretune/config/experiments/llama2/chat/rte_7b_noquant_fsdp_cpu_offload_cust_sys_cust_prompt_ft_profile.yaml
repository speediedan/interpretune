itdm_cfg:
  model_name_or_path: meta-llama/Llama-2-7b-chat-hf
  task_name: rte
  dataset_path: /home/speediedan/.cache/huggingface/datasets/rte
  train_batch_size: 2
  eval_batch_size: 2
  cust_tokenization_pattern: llama2-chat
  prompt_cfg:
    class_path: interpretune.models.llama2.Llama2PromptConfig
    init_args:
      sys_prompt: >-
        You are a helpful, respectful and honest assistant attempting to help with a specific task. Your task is to decide
        whether a given hypothesis is logically justified based upon a provided premise. You will be provided some text
        representing a premise followed by text representing a hypothesis. Decide whether the provided hypothesis
        text is logically justified considering only the logic and assertions of the premise text.
      cust_task_prompt:
        context: '<<<PREMISE>>>:'
        question: '<<<HYPOTHESIS>>>:'
it_cfg:
  activation_checkpointing: false
  bitsandbytesconfig: {}
  lora_cfg: {}
  auto_model_cfg:
    model_head: transformers.LlamaForSequenceClassification
  debug_lm_cfg:  # enable lm debugging connector features
    enabled: true
    record_memory_history: true
    raw_debug_sequences:
      - What is the color of a banana?
      - List the first 5 letters in the alphabet.
      - How many days in a week?
      - How old is Barack Obama?
trainer:
  devices: 1
  limit_train_batches: 7
  accumulate_grad_batches: 1
  limit_val_batches: 2
  max_epochs: 4
  callbacks:
  - class_path: finetuning_scheduler.FinetuningScheduler
    init_args:
      ft_schedule: /home/speediedan/repos/interpretune/src/interpretune/config/ft_schedules/ITModule_ft_schedule_llama2_7b.yaml
      max_depth: 1
      logging_level: 10
      strategy_adapter_cfg:
        awp_overrides: ["model.score", "model.model.norm"]
  - class_path: finetuning_scheduler.FTSCheckpoint
    init_args:
      save_top_k: 1
      monitor: val_loss
      verbose: true
  - class_path: finetuning_scheduler.FTSEarlyStopping
    init_args:
      monitor: val_loss
      min_delta: 0.001
      patience: 2 # limited patience for example
      verbose: true
      mode: min
  strategy:
    class_path: lightning.pytorch.strategies.FSDPStrategy
    init_args:
      cpu_offload: true
      activation_checkpointing_policy:
        class_path: torch.distributed.fsdp.wrap.ModuleWrapPolicy
        init_args:
          # comment below to generate debugging demo
          module_classes: !!set
            ? transformers.models.llama.modeling_llama.LlamaDecoderLayer
      auto_wrap_policy:
        class_path: torch.distributed.fsdp.wrap.ModuleWrapPolicy
        init_args:
          module_classes: !!set
            ? transformers.models.llama.modeling_llama.LlamaDecoderLayer
            ? torch.nn.Embedding
  logger:
    class_path: lightning.pytorch.loggers.TensorBoardLogger
    init_args:
      name: llama2_chat_rte_7b_noquant_fsdp_cpu_offload_cust_sys_cust_prompt_ft_profile_wout_fix
  profiler:
    class_path: fts_examples.stable.extended_profiler.ExtendedPyTorchProfiler
    init_args:
      filename: llama2_chat_rte_7b_noquant_fsdp_cpu_offload_cust_sys_cust_prompt_ft_profile_wout_fix
      max_name_column_width: 100
      sort_by_key: cuda_time_total
      schedule_cfg:
        #skip_first: 20  # comment if you want to profile the first fine-tuning phase instead of the final one
        wait: 1
        warmup: 1
        active: 3
    dict_kwargs:
      with_stack: true
      profile_memory: true
      record_shapes: true
      row_limit: 50
