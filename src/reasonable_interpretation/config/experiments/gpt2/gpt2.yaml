data.class_path: reasonable_interpretation.models.gpt2.GPT2RIDataModule
model.class_path: reasonable_interpretation.models.gpt2.GPT2RIModule
ridm_cfg:
  model_name_or_path: gpt2
  task_name: rte
  # data_collator_cfg:
  #   collator_class: transformers.DataCollatorWithPadding
  #   collator_kwargs:
  #     padding: false
  #os_env_model_auth_key: null
  # tokenizer_id_overrides:
  #   pad_token_id: 50256
  #   question_token_id: 50257
  #   context_token_id: 50258
  # special_tokens_dict: {"additional_special_tokens": ["<|PREMISE|>","<|HYPOTHESIS|>"]}  # TODO: inspect sorting of this list to determine id
  prompt_cfg:
    class_path: reasonable_interpretation.models.gpt2.GPT2PromptConfig
    # init_args:
    #   cust_task_prompt:
    #     context: '<|PREMISE|>:'
    #     question: '<|HYPOTHESIS|>:'
  tokenizers_parallelism: false
  tokenizer_kwargs:
    use_fast: true  # by default true
    add_bos_token: false
    local_files_only: false
    padding_side: right
    model_input_names: ['input_ids', 'attention_mask']
  # defer_model_init: true  # uncomment along with signature_columns to test deferred model init mode
  # signature_columns: ['input_ids', 'attention_mask', 'position_ids', 'past_key_values', 'inputs_embeds', 'labels',
  #                     'use_cache', 'output_attentions', 'output_hidden_states', 'return_dict']
# ri_cfg:
#   lora_cfg:
#     r: 8
#     lora_alpha: 32
#     target_modules: ["q_proj", "v_proj"]
#     lora_dropout: 0.05
#     bias: none
#     task_type: "CAUSAL_LM"
