ridm_cfg:
  model_name_or_path: meta-llama/Llama-2-7b-hf
  task_name: rte
  dataset_path: /home/speediedan/.cache/huggingface/datasets/rte
  train_batch_size: 2
  eval_batch_size: 2
  cust_tokenization_pattern: llama2-nochat
ri_cfg:
  activation_checkpointing: false
  bitsandbytesconfig: null
  lora_cfg: null
  auto_model_cfg:
    model_head: transformers.LlamaForSequenceClassification
trainer:
  devices: 1
  callbacks:
  - class_path: finetuning_scheduler.FinetuningScheduler
    init_args:
      ft_schedule: /home/speediedan/repos/reasonable-interpretation/src/reasonable_interpretation/config/ft_schedules/RIModule_ft_schedule_llama2_7b.yaml
      max_depth: 1
  - class_path: finetuning_scheduler.FTSCheckpoint
    init_args:
      save_top_k: 1
      monitor: val_loss
      verbose: true
  - class_path: finetuning_scheduler.FTSEarlyStopping
    init_args:
      monitor: val_loss
      min_delta: 0.001
      patience: 2 # limited patience for example
      verbose: true
      mode: min
  logger:
    class_path: lightning.pytorch.loggers.TensorBoardLogger
    init_args:
      name: llama2_nochat_rte_7b_noquant_ft

