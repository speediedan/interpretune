{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eadfea7f",
   "metadata": {},
   "source": [
    "# Example Hub and Local Operation Collections\n",
    "\n",
    "This notebook demonstrates the complete workflow for uploading and downloading operations collections using the \n",
    "`HubAnalysisOpManager` and loading local operations via `IT_ANALYSIS_OP_PATHS`. The workflow includes:\n",
    "\n",
    "1. Setting up local op collection path via IT_ANALYSIS_OP_PATHS\n",
    "2. Copying the current hub_op_collection folder to /tmp/\n",
    "3. Uploading operations to HuggingFace Hub as a private repository\n",
    "4. Downloading the uploaded collection to the default cache\n",
    "5. Re-importing interpretune to verify both hub and local operations are available\n",
    "6. Testing the loaded operations\n",
    "7. Cleaning up downloaded operations and re-importing\n",
    "8. Verifying only local operations remain available\n",
    "9. Final cleanup of the local operations collection\n",
    "\n",
    "```python\n",
    "\n",
    "**Note**: This example requires HuggingFace Hub authentication and will create a private repository.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24d51b0",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395a631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Import interpretune components\n",
    "import interpretune\n",
    "from interpretune.utils import rank_zero_warn\n",
    "from interpretune.analysis.ops.hub_manager import HubAnalysisOpManager\n",
    "from interpretune.analysis import IT_ANALYSIS_CACHE, IT_ANALYSIS_HUB_CACHE, IT_ANALYSIS_OP_PATHS, IT_MODULES_CACHE\n",
    "from interpretune.base.components.cli import IT_BASE\n",
    "\n",
    "example_op_collections_dir = Path(IT_BASE / \"notebooks\" / \"example_op_collections\")\n",
    "example_hub_op_collection_dir = Path(example_op_collections_dir / \"hub_op_collection\")\n",
    "example_local_op_collection_dir = Path(example_op_collections_dir / \"local_op_collection\")\n",
    "print(f\"Interpretune version: {interpretune.version}\")\n",
    "print(f\"Current analysis cache location: {IT_ANALYSIS_CACHE}\")\n",
    "print(f\"Current modules cache location: {IT_MODULES_CACHE}\")\n",
    "print(f\"Current hub cache location: {IT_ANALYSIS_HUB_CACHE}\")\n",
    "print(f\"Current IT analysis op paths: {IT_ANALYSIS_OP_PATHS}\")\n",
    "print(f\"This notebook's example hub op collection directory: {example_hub_op_collection_dir}\")\n",
    "print(f\"This notebook's example local op collection directory: {example_local_op_collection_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19463a7b",
   "metadata": {},
   "source": [
    "## Step 1: Setup local op collection path\n",
    "\n",
    "Copy the local_op_collection to /tmp/ and add it to IT_ANALYSIS_OP_PATHS so local operations are loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76259b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define source and destination paths for local ops\n",
    "source_local_op_collection = example_local_op_collection_dir\n",
    "tmp_local_op_collection = Path(\"/tmp/local_op_collection\")\n",
    "\n",
    "print(f\"Source local op_collection: {source_local_op_collection}\")\n",
    "print(f\"Destination: {tmp_local_op_collection}\")\n",
    "\n",
    "# Check if source exists\n",
    "if not source_local_op_collection.exists():\n",
    "    raise FileNotFoundError(f\"Source local op_collection not found at {source_local_op_collection}\")\n",
    "\n",
    "# Warn if destination already exists\n",
    "if tmp_local_op_collection.exists():\n",
    "    rank_zero_warn(f\"Destination folder {tmp_local_op_collection} already exists and will be overwritten!\")\n",
    "    shutil.rmtree(tmp_local_op_collection)\n",
    "\n",
    "# Copy the local op collection folder\n",
    "shutil.copytree(source_local_op_collection, tmp_local_op_collection)\n",
    "print(f\"✓ Successfully copied local op_collection to {tmp_local_op_collection}\")\n",
    "\n",
    "# Store the original IT_ANALYSIS_OP_PATHS environment variable\n",
    "original_op_paths_env = os.environ.get('IT_ANALYSIS_OP_PATHS', '')\n",
    "print(f\"Original IT_ANALYSIS_OP_PATHS environment variable: '{original_op_paths_env}'\")\n",
    "\n",
    "# Set the IT_ANALYSIS_OP_PATHS environment variable\n",
    "# The format is a colon-separated list of paths\n",
    "new_op_paths = str(tmp_local_op_collection)\n",
    "if original_op_paths_env:\n",
    "    new_op_paths = f\"{original_op_paths_env}:{new_op_paths}\"\n",
    "\n",
    "os.environ['IT_ANALYSIS_OP_PATHS'] = new_op_paths\n",
    "print(f\"✓ Set IT_ANALYSIS_OP_PATHS environment variable to: '{new_op_paths}'\")\n",
    "\n",
    "# Also update the imported list for consistency (this was the old approach)\n",
    "if str(tmp_local_op_collection) not in IT_ANALYSIS_OP_PATHS:\n",
    "    IT_ANALYSIS_OP_PATHS.append(str(tmp_local_op_collection))\n",
    "    print(f\"✓ Also added {tmp_local_op_collection} to imported IT_ANALYSIS_OP_PATHS list\")\n",
    "\n",
    "print(f\"\\nUpdated IT_ANALYSIS_OP_PATHS list: {IT_ANALYSIS_OP_PATHS}\")\n",
    "print(f\"Current IT_ANALYSIS_OP_PATHS env var: '{os.environ.get('IT_ANALYSIS_OP_PATHS', '')}'\")\n",
    "\n",
    "# Verify contents\n",
    "print(\"\\nContents of copied local op_collection:\")\n",
    "for item in tmp_local_op_collection.iterdir():\n",
    "    print(f\"  - {item.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bc534c",
   "metadata": {},
   "source": [
    "## Step 2: Copy hub op_collection to /tmp/\n",
    "\n",
    "Copy the hub op_collection folder to /tmp/ for upload to the hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4946f954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define source and destination paths for hub ops\n",
    "source_op_collection = example_hub_op_collection_dir\n",
    "tmp_op_collection = Path(\"/tmp/hub_op_collection\")\n",
    "\n",
    "print(f\"Source hub op_collection: {source_op_collection}\")\n",
    "print(f\"Destination: {tmp_op_collection}\")\n",
    "\n",
    "# Check if source exists\n",
    "if not source_op_collection.exists():\n",
    "    raise FileNotFoundError(f\"Source op_collection not found at {source_op_collection}\")\n",
    "\n",
    "# Warn if destination already exists\n",
    "if tmp_op_collection.exists():\n",
    "    rank_zero_warn(f\"Destination folder {tmp_op_collection} already exists and will be overwritten!\")\n",
    "    shutil.rmtree(tmp_op_collection)\n",
    "\n",
    "# Copy the folder\n",
    "shutil.copytree(source_op_collection, tmp_op_collection)\n",
    "print(f\"✓ Successfully copied hub op_collection to {tmp_op_collection}\")\n",
    "\n",
    "# Verify contents\n",
    "print(\"\\nContents of copied hub op_collection:\")\n",
    "for item in tmp_op_collection.iterdir():\n",
    "    print(f\"  - {item.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5ec51b",
   "metadata": {},
   "source": [
    "## Step 3: Upload operations to HuggingFace Hub\n",
    "\n",
    "Upload the hub op_collection to HuggingFace Hub as a private repository named \"trivial_op_repo\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467c6fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import whoami\n",
    "current_user = whoami()['name']\n",
    "\n",
    "# Initialize the hub manager\n",
    "hub_manager = HubAnalysisOpManager()\n",
    "\n",
    "# Repository configuration\n",
    "repo_name = \"trivial_op_repo\"\n",
    "private = True\n",
    "\n",
    "print(\"Uploading op_collection to HuggingFace Hub...\")\n",
    "print(f\"Current HF user: {current_user}\")\n",
    "print(f\"Repository: {repo_name}\")\n",
    "print(f\"Private: {private}\")\n",
    "print(f\"Source folder: {tmp_op_collection}\")\n",
    "\n",
    "# Ensure the user is authenticated\n",
    "repo_id = f'{current_user}/{repo_name}'\n",
    "try:\n",
    "    # Upload operations to hub\n",
    "    # 1. This will create the specified repository if it doesn't exist\n",
    "    # 2. If the repo exists, it will clean existing operations and upload the new ones in a single commit\n",
    "    #    - If no files have changed, it will skip the commit and leave the repository unchanged\n",
    "\n",
    "    upload_result = hub_manager.upload_ops(\n",
    "        local_dir=tmp_op_collection,\n",
    "        repo_id=repo_id,\n",
    "        private=private,\n",
    "        clean_existing=True,\n",
    "    )\n",
    "\n",
    "    print(f\"✓ Successfully uploaded operations (if necessary) to {repo_name}\")\n",
    "    print(f\"Upload result (new or latest op repo commit sha): {upload_result}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error uploading operations: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e3bafb",
   "metadata": {},
   "source": [
    "## Step 4: Download operations to default hub cache\n",
    "\n",
    "Download the uploaded operations collection to the default `IT_ANALYSIS_HUB_CACHE` location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ffd7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Downloading operations from {repo_id} to default cache...\")\n",
    "print(f\"Cache location: {IT_ANALYSIS_HUB_CACHE}\")\n",
    "\n",
    "# Initialize download_result to None so we can safely check it in cleanup step\n",
    "download_result = None\n",
    "\n",
    "try:\n",
    "    # Download operations from hub to default cache\n",
    "    download_result = hub_manager.download_ops(\n",
    "        repo_id=repo_id,\n",
    "        # cache_dir not specified, will use default IT_ANALYSIS_HUB_CACHE\n",
    "    )\n",
    "\n",
    "    print(\"✓ Successfully downloaded operations to cache\")\n",
    "    print(f\"Download result: {download_result}\")\n",
    "\n",
    "    # Check what was downloaded\n",
    "    cache_path = Path(IT_ANALYSIS_HUB_CACHE)\n",
    "    if cache_path.exists():\n",
    "        print(\"\\nContents of hub cache:\")\n",
    "        for item in cache_path.rglob(\"*\"):\n",
    "            if item.is_file():\n",
    "                rel_path = item.relative_to(cache_path)\n",
    "                print(f\"  - {rel_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error downloading operations: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f31c25",
   "metadata": {},
   "source": [
    "## Step 5: Re-import interpretune and verify hub and local operations\n",
    "\n",
    "Re-import interpretune to pick up both hub and local operations and verify they are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1754e16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Re-importing interpretune to pick up hub and local operations...\")\n",
    "# Remove interpretune modules from sys.modules to force reimport\n",
    "modules_to_remove = [name for name in sys.modules.keys() if name.startswith('interpretune')]\n",
    "for module_name in modules_to_remove:\n",
    "    del sys.modules[module_name]\n",
    "\n",
    "# ruff: noqa: E402\n",
    "\n",
    "# Re-import interpretune\n",
    "import interpretune as it\n",
    "from interpretune import DISPATCHER\n",
    "from interpretune.analysis.ops.base import OpWrapper\n",
    "\n",
    "print(\"✓ Interpretune re-imported\")\n",
    "\n",
    "# Get operation definitions and count hub vs local operations\n",
    "operation_definitions = DISPATCHER.registered_ops\n",
    "total_ops = len(operation_definitions)\n",
    "\n",
    "# Get canonical names (resolve aliases to their target operations)\n",
    "canonical_ops = {}\n",
    "alias_map = {}\n",
    "for op_name, op_def in operation_definitions.items():\n",
    "    canonical_name = op_def.name\n",
    "    if canonical_name not in canonical_ops:\n",
    "        canonical_ops[canonical_name] = op_def\n",
    "        alias_map[canonical_name] = []\n",
    "    if op_name != canonical_name:\n",
    "        alias_map[canonical_name].append(op_name)\n",
    "\n",
    "# Count hub operations (those with dots in their canonical names indicating namespacing)\n",
    "hub_ops = {name: op_def for name, op_def in canonical_ops.items()\n",
    "           if '.' in name}\n",
    "\n",
    "# Count local operations (those without dots but not in the built-in list)\n",
    "builtin_ops = {'labels_to_ids', 'get_answer_indices', 'get_alive_latents', 'model_forward',\n",
    "               'model_cache_forward', 'model_ablation', 'model_gradient', 'logit_diffs',\n",
    "               'logit_diffs_cache', 'sae_correct_acts', 'gradient_attribution', 'ablation_attribution'}\n",
    "\n",
    "local_ops = {name: op_def for name, op_def in canonical_ops.items()\n",
    "             if '.' not in name and name not in builtin_ops and op_def.composition is None}\n",
    "\n",
    "# Count composed operations\n",
    "composed_ops = {name: op_def for name, op_def in canonical_ops.items()\n",
    "                if op_def.composition is not None}\n",
    "\n",
    "print(\"\\n📊 Operation Summary:\")\n",
    "print(f\"  Total registered names: {total_ops}\")\n",
    "print(f\"  Unique operations: {len(canonical_ops)}\")\n",
    "print(f\"  Hub operations: {len(hub_ops)}\")\n",
    "print(f\"  Local operations: {len(local_ops)}\")\n",
    "print(f\"  Composed operations: {len(composed_ops)}\")\n",
    "print(f\"  Built-in operations: {len(builtin_ops)}\")\n",
    "\n",
    "print(\"\\n🌐 Hub operations found:\")\n",
    "for op_name, op_def in hub_ops.items():\n",
    "    aliases = alias_map.get(op_name, [])\n",
    "    all_names = [op_name] + aliases\n",
    "    print(f\"  - {op_name} (accessible as: {', '.join(all_names)})\")\n",
    "\n",
    "print(\"\\n🏠 Local operations found:\")\n",
    "for op_name, op_def in local_ops.items():\n",
    "    aliases = alias_map.get(op_name, [])\n",
    "    all_names = [op_name] + aliases\n",
    "    print(f\"  - {op_name} (accessible as: {', '.join(all_names)}) - {op_def.description}\")\n",
    "\n",
    "# Test operation instantiation\n",
    "print(\"\\n🔧 Testing operation instantiation:\")\n",
    "print(f\"labels_to_ids op reference type: {type(it.labels_to_ids)}\")\n",
    "\n",
    "print(f\"get_answer_indices op reference type: {type(it.get_answer_indices)}\")\n",
    "print(f\"trivial_test_op op reference type: {type(it.trivial_test_op)}\")\n",
    "\n",
    "print(f\"Get non-direct access attribute of labels_to_ids (description of the underlying AnalysisOp): \"\n",
    "      f\"{it.labels_to_ids.description}\")\n",
    "print(f\"Type of labels_to_ids now: {type(it.labels_to_ids)}\")\n",
    "print(f\"Type of get_answer_indices is still: {type(it.get_answer_indices)} and its instantiated status is \"\n",
    "      f\"{it.get_answer_indices._is_instantiated}\")\n",
    "print(f\"Non-direct access attribute of get_answer_indices (name of the underlying AnalysisOp): \"\n",
    "      f\"{it.get_answer_indices.name}\")\n",
    "print(f\"Type of get_answer_indices is now: {type(it.get_answer_indices)}\")\n",
    "print(f\"Type of trivial_test_op is: {type(it.trivial_test_op)} and its instantiated status is \"\n",
    "      f\"{it.trivial_test_op._is_instantiated}\")\n",
    "try:\n",
    "    print(f\"Non-direct access attribute of trivial_test_op (name of the underlying AnalysisOp): \"\n",
    "          f\"{it.trivial_test_op.name}\")\n",
    "    print(f\"Type of trivial_test_op is now: {type(it.trivial_test_op)} as it has been successfully instantiated\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error instantiating trivial_test_op: {e}\")\n",
    "    if isinstance(it.trivial_test_op, OpWrapper):\n",
    "        print(f\"Type of trivial_test_op is still: {type(it.trivial_test_op)} and its instantiated status is \"\n",
    "              f\"{it.trivial_test_op._is_instantiated} likely because of a dynamic import failure.\")\n",
    "\n",
    "try:\n",
    "    print(f\"Non-direct access attribute of trivial_local_test_op (name of the underlying AnalysisOp): \"\n",
    "          f\"{it.trivial_local_test_op.name}\")\n",
    "    print(f\"Type of trivial_local_test_op is now: {type(it.trivial_local_test_op)} as it has been successfully \"\n",
    "          f\"instantiated\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error instantiating trivial_local_test_op: {e}\")\n",
    "    if isinstance(it.trivial_local_test_op, OpWrapper):\n",
    "        print(f\"Type of trivial_local_test_op is still: {type(it.trivial_local_test_op)} and its instantiated status \"\n",
    "              f\"is {it.trivial_local_test_op._is_instantiated} likely because of a dynamic import failure.\")\n",
    "\n",
    "if hub_ops:\n",
    "    hub_op_name = next(iter(hub_ops.keys()))\n",
    "    print(f\"{hub_op_name} op reference type: {type(getattr(it, hub_op_name, 'Not found'))}\")\n",
    "if local_ops:\n",
    "    local_op_name = next(iter(local_ops.keys()))\n",
    "    print(f\"{local_op_name} op reference type: {type(getattr(it, local_op_name, 'Not found'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61dc24e",
   "metadata": {},
   "source": [
    "## Step 6: Test the loaded operations\n",
    "\n",
    "Test simple hub and local operations both individually executed and as part of a composite operation to ensure loading and execution works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cb901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n🧪 Testing loaded operations with demo data...\")\n",
    "\n",
    "# Import required components\n",
    "import torch\n",
    "from interpretune.analysis.ops.base import AnalysisBatch\n",
    "from interpretune import trivial_test_op, trivial_local_test_op, composite_trivial_test_op\n",
    "\n",
    "NUM_BATCHES = 2  # Number of test batches to generate\n",
    "\n",
    "# Simple generator that creates test analysis_batch objects\n",
    "def generate_test_batches(num_batches=NUM_BATCHES):\n",
    "    \"\"\"Generator that yields test analysis_batch objects with random orig_labels.\"\"\"\n",
    "    for i in range(num_batches):\n",
    "        separate_op_input_batch, pipeline_op_input_batch = AnalysisBatch(), AnalysisBatch()\n",
    "        orig_labels = torch.randint(0, 5, (4,))\n",
    "        for batch in (separate_op_input_batch, pipeline_op_input_batch):\n",
    "            batch.update(orig_labels=orig_labels.clone())\n",
    "        yield f\"Batch {i+1} (random orig_labels)\", separate_op_input_batch, pipeline_op_input_batch\n",
    "\n",
    "VERBOSE_OP_OUTPUTS = False  # Set to True to log operation outputs\n",
    "def maybe_print(output):\n",
    "    \"\"\"Log operation output if logging is enabled.\"\"\"\n",
    "    if VERBOSE_OP_OUTPUTS:\n",
    "        print(output)\n",
    "\n",
    "# Test the operations\n",
    "print(f\"\\n📋 Testing operation pipeline parity of composite vs individual component ops (over {NUM_BATCHES} batches):\")\n",
    "individual_op_output_batches = []\n",
    "composite_op_output_batches = []\n",
    "for batch_name, individual_test_batch, composite_test_batch in generate_test_batches():\n",
    "    print(\"\\nComposite op execution...\")\n",
    "    if VERBOSE_OP_OUTPUTS:\n",
    "        print(f\"\\n--- {batch_name} ---\")\n",
    "        print(f\"Input batch: {individual_test_batch}\")\n",
    "    composite_output_batch = composite_trivial_test_op(analysis_batch=composite_test_batch)\n",
    "    maybe_print(f\"Composite op output batch: {composite_output_batch}\")\n",
    "    composite_op_output_batches.append(composite_output_batch)\n",
    "    print(\"\\nRe-running with individual component ops...\")\n",
    "    local_batch_output = trivial_local_test_op(analysis_batch=individual_test_batch)\n",
    "    maybe_print(f\"Local op batch output: {local_batch_output}\")\n",
    "    individual_output_batch = trivial_test_op(analysis_batch=local_batch_output)\n",
    "    maybe_print(f\"Hub output batch: {individual_output_batch}\")\n",
    "    individual_op_output_batches.append(individual_output_batch)\n",
    "\n",
    "# Compare outputs from individual component op and composite op processing\n",
    "print(\"\\n🔍 Validating that composite and individual component op outputs are identical...\")\n",
    "all_match = True\n",
    "for idx, (sep_batch, composite_batch) in enumerate(zip(individual_op_output_batches, composite_op_output_batches)):\n",
    "    if sep_batch == composite_batch:\n",
    "        print(f\"  ✓ Batch {idx+1}: Outputs match.\")\n",
    "    else:\n",
    "        print(f\"  ❌ Batch {idx+1}: Outputs do NOT match!\")\n",
    "        print(f\"    Individual op output: {sep_batch}\")\n",
    "        print(f\"    Composite op output: {composite_batch}\")\n",
    "        all_match = False\n",
    "\n",
    "if all_match:\n",
    "    print(\"\\n🎉 All batches match: individual and composite operation outputs are identical!\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Some batches did not match: please check the operation implementations.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e04c8a4",
   "metadata": {},
   "source": [
    "## Step 7: Clean up hub operations and re-import\n",
    "\n",
    "Delete the downloaded hub operations folder and re-import interpretune to verify only local operations remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b99c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cleaning up downloaded hub operations...\")\n",
    "\n",
    "# Remove only the specific repository we downloaded, not the entire hub cache\n",
    "if download_result is not None and hasattr(download_result, 'local_path'):\n",
    "    repo_cache_path = download_result.local_path\n",
    "    if repo_cache_path.exists():\n",
    "        # Remove only the specific repository cache\n",
    "        # The path structure is typically: cache/models--username--repo-name/\n",
    "        # We want to remove the entire models--username--repo-name directory\n",
    "        repo_root = repo_cache_path\n",
    "        # Navigate up to find the repo root (models--username--repo-name)\n",
    "        while repo_root.parent != repo_root and not repo_root.name.startswith('models--'):\n",
    "            repo_root = repo_root.parent\n",
    "\n",
    "        if repo_root.name.startswith('models--'):\n",
    "            shutil.rmtree(repo_root)\n",
    "            print(f\"✓ Removed specific hub repository cache: {repo_root}\")\n",
    "        else:\n",
    "            print(f\"⚠️ Could not determine repo root from path: {repo_cache_path}\")\n",
    "    else:\n",
    "        print(f\"Hub repository cache path not found: {repo_cache_path}\")\n",
    "else:\n",
    "    print(\"⚠️ No download_result available - cannot determine what to clean up\")\n",
    "\n",
    "# Re-import interpretune again\n",
    "print(\"\\nRe-importing interpretune after cleanup...\")\n",
    "\n",
    "import io\n",
    "import contextlib\n",
    "\n",
    "# Capture stdout and stderr during import to check for the expected warning\n",
    "f_stdout = io.StringIO()\n",
    "f_stderr = io.StringIO()\n",
    "with contextlib.redirect_stdout(f_stdout), contextlib.redirect_stderr(f_stderr):\n",
    "    # Remove interpretune modules again to force reimport and warning emission\n",
    "    modules_to_remove = [name for name in sys.modules.keys() if name.startswith('interpretune')]\n",
    "    for module_name in modules_to_remove:\n",
    "        del sys.modules[module_name]\n",
    "\n",
    "    # Re-import interpretune\n",
    "    import interpretune\n",
    "    from interpretune import DISPATCHER\n",
    "\n",
    "stdout_output = f_stdout.getvalue()\n",
    "stderr_output = f_stderr.getvalue()\n",
    "\n",
    "if stderr_output and \"Failed to compile operation 'composite_trivial_test_op'\" in stderr_output:\n",
    "    # note that this warning won't be issued if we've already executed this cell since the latest cache will be used\n",
    "    print(stderr_output)\n",
    "    print(\n",
    "        \"Note the above \\\"Failed to compile operation 'composite_trivial_test_op'\\\" error on re-import of \"\n",
    "        \"interpretune after our cleanup.\\n\"\n",
    "        \"This is expected: we have removed our hub op definitions (trivial_test_op), but not our local op \"\n",
    "        \"definitions (trivial_local_test_op, composite_trivial_test_op).\\n\"\n",
    "        \"As a result, the locally defined composite operation 'composite_trivial_test_op' could not be \"\n",
    "        \"constructed since it depended on the now-missing hub op.\\nAll other available operations (local and \"\n",
    "        \"built-in) should still be present as we will see.\"\n",
    "    )\n",
    "elif stderr_output:\n",
    "    print(\"Unexpected stderr output during import:\")\n",
    "    print(stderr_output)\n",
    "\n",
    "\n",
    "print(\"\\n ✓ Interpretune re-imported after cleanup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d16772",
   "metadata": {},
   "source": [
    "## Step 8: Verify only local operations remain\n",
    "\n",
    "Verify that only the local and built-in operations are available after hub cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f0636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Verifying operations after cleanup...\")\n",
    "\n",
    "# Get operation definitions after cleanup\n",
    "operation_definitions_after = DISPATCHER.registered_ops\n",
    "total_ops_after = len(operation_definitions_after)\n",
    "\n",
    "# Get canonical names (resolve aliases to their target operations)\n",
    "canonical_ops_after = {}\n",
    "alias_map_after = {}\n",
    "for op_name, op_def in operation_definitions_after.items():\n",
    "    canonical_name = op_def.name\n",
    "    if canonical_name not in canonical_ops_after:\n",
    "        canonical_ops_after[canonical_name] = op_def\n",
    "        alias_map_after[canonical_name] = []\n",
    "    if op_name != canonical_name:\n",
    "        alias_map_after[canonical_name].append(op_name)\n",
    "\n",
    "# Count hub operations (those with dots in their canonical names)\n",
    "hub_ops_after = {name: op_def for name, op_def in canonical_ops_after.items()\n",
    "                 if '.' in name}\n",
    "\n",
    "# Count local operations (those without dots but not in the built-in list)\n",
    "builtin_ops = {'labels_to_ids', 'get_answer_indices', 'get_alive_latents', 'model_forward',\n",
    "               'model_cache_forward', 'model_ablation', 'model_gradient', 'logit_diffs',\n",
    "               'logit_diffs_cache', 'sae_correct_acts', 'gradient_attribution', 'ablation_attribution'}\n",
    "\n",
    "local_ops_after = {name: op_def for name, op_def in canonical_ops_after.items()\n",
    "                   if '.' not in name and name not in builtin_ops and op_def.composition is None}\n",
    "\n",
    "# Count composed operations\n",
    "composed_ops_after = {name: op_def for name, op_def in canonical_ops_after.items()\n",
    "                      if op_def.composition is not None}\n",
    "\n",
    "print(\"\\n📊 Operation Summary After Cleanup:\")\n",
    "print(f\"  Total registered names: {total_ops_after}\")\n",
    "print(f\"  Unique operations: {len(canonical_ops_after)}\")\n",
    "print(f\"  Hub operations: {len(hub_ops_after)}\")\n",
    "print(f\"  Local operations: {len(local_ops_after)}\")\n",
    "print(f\"  Composed operations: {len(composed_ops_after)}\")\n",
    "print(f\"  Built-in operations: {len(builtin_ops)}\")\n",
    "\n",
    "if len(hub_ops_after) == 0:\n",
    "    print(\"\\n✅ Success: No hub operations found - cleanup successful!\")\n",
    "else:\n",
    "    print(f\"\\n❌ Warning: {len(hub_ops_after)} hub operations still present:\")\n",
    "    for op_name, op_def in hub_ops_after.items():\n",
    "        aliases = alias_map_after.get(op_name, [])\n",
    "        all_names = [op_name] + aliases\n",
    "        print(f\"  - {op_name} (accessible as: {', '.join(all_names)})\")\n",
    "\n",
    "if len(local_ops_after) > 0:\n",
    "    print(\"\\n🏠 Local operations still available:\")\n",
    "    for op_name, op_def in local_ops_after.items():\n",
    "        aliases = alias_map_after.get(op_name, [])\n",
    "        all_names = [op_name] + aliases\n",
    "        print(f\"  - {op_name} (accessible as: {', '.join(all_names)}) - {op_def.description}\")\n",
    "else:\n",
    "    print(\"\\n⚠️ No local operations found\")\n",
    "\n",
    "# Show remaining operations by category\n",
    "print(\"\\n📋 Detailed breakdown:\")\n",
    "print(f\"\\n  Built-in operations ({len(builtin_ops)}):\")\n",
    "for name in sorted(builtin_ops):\n",
    "    if name in canonical_ops_after:\n",
    "        aliases = alias_map_after.get(name, [])\n",
    "        all_names = [name] + aliases\n",
    "        print(f\"    - {name} (accessible as: {', '.join(all_names)})\")\n",
    "\n",
    "if composed_ops_after:\n",
    "    print(f\"\\n  Composed operations ({len(composed_ops_after)}):\")\n",
    "    for name in sorted(composed_ops_after.keys()):\n",
    "        aliases = alias_map_after.get(name, [])\n",
    "        all_names = [name] + aliases\n",
    "        print(f\"    - {name} (accessible as: {', '.join(all_names)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb0b647",
   "metadata": {},
   "source": [
    "## Cleanup temporary files\n",
    "\n",
    "Clean up the temporary files created during this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a37ad14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cleaning up temporary files...\")\n",
    "\n",
    "# Remove temporary hub op_collection\n",
    "if tmp_op_collection.exists():\n",
    "    shutil.rmtree(tmp_op_collection)\n",
    "    print(f\"✓ Removed temporary hub op_collection: {tmp_op_collection}\")\n",
    "\n",
    "# Remove temporary local op_collection\n",
    "if tmp_local_op_collection.exists():\n",
    "    shutil.rmtree(tmp_local_op_collection)\n",
    "    print(f\"✓ Removed temporary local op_collection: {tmp_local_op_collection}\")\n",
    "\n",
    "# Restore the original IT_ANALYSIS_OP_PATHS environment variable\n",
    "if 'original_op_paths_env' in locals():\n",
    "    if original_op_paths_env:\n",
    "        os.environ['IT_ANALYSIS_OP_PATHS'] = original_op_paths_env\n",
    "        print(f\"✓ Restored IT_ANALYSIS_OP_PATHS environment variable to: '{original_op_paths_env}'\")\n",
    "    else:\n",
    "        if 'IT_ANALYSIS_OP_PATHS' in os.environ:\n",
    "            del os.environ['IT_ANALYSIS_OP_PATHS']\n",
    "            print(\"✓ Unset IT_ANALYSIS_OP_PATHS environment variable\")\n",
    "else:\n",
    "    print(\"⚠️ Original IT_ANALYSIS_OP_PATHS value not found, cannot restore\")\n",
    "\n",
    "# Remove from the imported IT_ANALYSIS_OP_PATHS list\n",
    "if str(tmp_local_op_collection) in IT_ANALYSIS_OP_PATHS:\n",
    "    IT_ANALYSIS_OP_PATHS.remove(str(tmp_local_op_collection))\n",
    "    print(f\"✓ Removed {tmp_local_op_collection} from imported IT_ANALYSIS_OP_PATHS list\")\n",
    "\n",
    "print(f\"\\nFinal IT_ANALYSIS_OP_PATHS list: {IT_ANALYSIS_OP_PATHS}\")\n",
    "print(f\"Final IT_ANALYSIS_OP_PATHS env var: '{os.environ.get('IT_ANALYSIS_OP_PATHS', '')}'\")\n",
    "\n",
    "print(\"\\n🎉 Hub and Local operations workflow example completed successfully!\")\n",
    "print(\"\\nSummary of what was demonstrated:\")\n",
    "print(\"1. ✓ Setup local op collection path via IT_ANALYSIS_OP_PATHS environment variable\")\n",
    "print(\"2. ✓ Copied hub op_collection to /tmp/ with overwrite warning\")\n",
    "print(\"3. ✓ Uploaded operations to HuggingFace Hub as private repo\")\n",
    "print(\"4. ✓ Downloaded operations to default hub cache\")\n",
    "print(\"5. ✓ Re-imported interpretune and verified both hub and local operations\")\n",
    "print(\"6. ✓ Tested operation instantiation and execution with demo data\")\n",
    "print(\"7. ✓ Cleaned up hub operations and re-imported\")\n",
    "print(\"8. ✓ Verified only local and built-in operations remain available\")\n",
    "print(\"9. ✓ Restored original IT_ANALYSIS_OP_PATHS environment variable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150f93a1",
   "metadata": {},
   "source": [
    "## Step 9: Final verification after environment cleanup\n",
    "\n",
    "Re-import interpretune one final time to verify that local operations are no longer available after unsetting IT_ANALYSIS_OP_PATHS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b881ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final verification: Re-importing interpretune after environment cleanup...\")\n",
    "\n",
    "# Remove interpretune modules from sys.modules to force reimport\n",
    "modules_to_remove = [name for name in sys.modules.keys() if name.startswith('interpretune')]\n",
    "for module_name in modules_to_remove:\n",
    "    del sys.modules[module_name]\n",
    "\n",
    "# Re-import interpretune one final time\n",
    "import interpretune\n",
    "from interpretune import DISPATCHER\n",
    "\n",
    "print(\"✓ Interpretune re-imported after environment cleanup\")\n",
    "\n",
    "# Get operation definitions after complete cleanup\n",
    "operation_definitions_final = DISPATCHER.registered_ops\n",
    "total_ops_final = len(operation_definitions_final)\n",
    "\n",
    "# Get canonical names (resolve aliases to their target operations)\n",
    "canonical_ops_final = {}\n",
    "alias_map_final = {}\n",
    "for op_name, op_def in operation_definitions_final.items():\n",
    "    canonical_name = op_def.name\n",
    "    if canonical_name not in canonical_ops_final:\n",
    "        canonical_ops_final[canonical_name] = op_def\n",
    "        alias_map_final[canonical_name] = []\n",
    "    if op_name != canonical_name:\n",
    "        alias_map_final[canonical_name].append(op_name)\n",
    "\n",
    "# Count operations after complete cleanup\n",
    "hub_ops_final = {name: op_def for name, op_def in canonical_ops_final.items()\n",
    "                 if '.' in name}\n",
    "\n",
    "builtin_ops = {'labels_to_ids', 'get_answer_indices', 'get_alive_latents', 'model_forward',\n",
    "               'model_cache_forward', 'model_ablation', 'model_gradient', 'logit_diffs',\n",
    "               'logit_diffs_cache', 'sae_correct_acts', 'gradient_attribution', 'ablation_attribution'}\n",
    "\n",
    "local_ops_final = {name: op_def for name, op_def in canonical_ops_final.items()\n",
    "                   if '.' not in name and name not in builtin_ops and op_def.composition is None}\n",
    "\n",
    "composed_ops_final = {name: op_def for name, op_def in canonical_ops_final.items()\n",
    "                      if op_def.composition is not None}\n",
    "\n",
    "print(\"\\n📊 Final Operation Summary (after complete cleanup):\")\n",
    "print(f\"  Total registered names: {total_ops_final}\")\n",
    "print(f\"  Unique operations: {len(canonical_ops_final)}\")\n",
    "print(f\"  Hub operations: {len(hub_ops_final)}\")\n",
    "print(f\"  Local operations: {len(local_ops_final)}\")\n",
    "print(f\"  Composed operations: {len(composed_ops_final)}\")\n",
    "print(f\"  Built-in operations: {len(builtin_ops)}\")\n",
    "\n",
    "# Verify complete cleanup\n",
    "if len(hub_ops_final) == 0 and len(local_ops_final) == 0:\n",
    "    print(\"\\n🎯 Perfect! Complete cleanup successful - only built-in and composed operations remain!\")\n",
    "elif len(hub_ops_final) == 0:\n",
    "    print(f\"\\n⚠️ Hub operations cleaned up, but {len(local_ops_final)} local operations still present:\")\n",
    "    for op_name, op_def in local_ops_final.items():\n",
    "        aliases = alias_map_final.get(op_name, [])\n",
    "        all_names = [op_name] + aliases\n",
    "        print(f\"    - {op_name} (accessible as: {', '.join(all_names)})\")\n",
    "elif len(local_ops_final) == 0:\n",
    "    print(f\"\\n⚠️ Local operations cleaned up, but {len(hub_ops_final)} hub operations still present:\")\n",
    "    for op_name, op_def in hub_ops_final.items():\n",
    "        aliases = alias_map_final.get(op_name, [])\n",
    "        all_names = [op_name] + aliases\n",
    "        print(f\"    - {op_name} (accessible as: {', '.join(all_names)})\")\n",
    "else:\n",
    "    print(f\"\\n❌ Cleanup incomplete: {len(hub_ops_final)} hub ops and {len(local_ops_final)} local ops still present\")\n",
    "\n",
    "print(\"\\nEnvironment verification:\")\n",
    "print(f\"  Current IT_ANALYSIS_OP_PATHS env var: '{os.environ.get('IT_ANALYSIS_OP_PATHS', 'Not set')}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "it_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
