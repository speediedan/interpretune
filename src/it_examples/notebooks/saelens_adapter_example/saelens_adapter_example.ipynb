{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretune SAELens Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fine-Tuning Scheduler logo](logo_fts.png){height=\"55px\" width=\"401px\"}\n",
    "\n",
    "### Intro\n",
    "\n",
    "[Interpretune](https://github.com/speediedan/interpretune) is a flexible framework for exploring, analyzing and tuning llm world models. In this tutorial, we'll walk through a simple example of using Interpretune to pursue interpretability research with SAELens. As we'll see, Interpretune handles the required execution context composition, allowing us to use the same code in a variety of contexts, depending upon the level of abstraction required.\n",
    "\n",
    "As a long-time PyTorch and PyTorch Lightning contributor, I've found the PyTorch Lightning framework is the right level of abstraction for a large variety of ML research contexts, but some contexts benefit from using core PyTorch directly. Additionally, some users may prefer to use the core PyTorch framework directly for a wide variety of reasons including maximizing portability. As will be demonstrated here, Interpretune maximizes flexibility and portability by adhering to a well-defined protocol that allows auto-composition of our research module with the adapters required for execution in a wide variety of contexts. In this example, we'll be executing the same module with core PyTorch and PyTorch Lightning, demonstrating the use of `SAELens` w/ Interpretune for interpretability research.\n",
    "\n",
    "> Note - **this is a WIP**, but this is the core idea. If you have any feedback, please let me know!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note on memory usage\n",
    "\n",
    "In these exercises, we'll be loading some pretty large models into memory (e.g. Gemma 2-2B and its SAEs, as well as a host of other models in later sections of the material). It's useful to have functions which can help profile memory usage for you, so that if you encounter OOM errors you can try and clear out unnecessary models. For example, we've found that with the right memory handling (i.e. deleting models and objects when you're not using them any more) it should be possible to run all the exercises in this material on a Colab Pro notebook, and all the exercises minus the handful involving Gemma on a free Colab notebook.\n",
    "\n",
    "<details>\n",
    "<summary>See this dropdown for some functions which you might find helpful, and how to use them.</summary>\n",
    "\n",
    "First, we can run some code to inspect our current memory usage. Here's me running this code during the exercise set on SAE circuits, after having already loaded in the Gemma models from the previous section. This was on a Colab Pro notebook.\n",
    "\n",
    "```python\n",
    "# Profile memory usage, and delete gemma models if we've loaded them in\n",
    "namespace = globals().copy() | locals()\n",
    "part32_utils.profile_pytorch_memory(namespace=namespace, filter_device=\"cuda:0\")\n",
    "```\n",
    "\n",
    "<pre style=\"font-family: Consolas; font-size: 14px\">Allocated = 35.88 GB\n",
    "Total = 39.56 GB\n",
    "Free = 3.68 GB\n",
    "┌──────────────────────┬────────────────────────┬──────────┬─────────────┐\n",
    "│ Name                 │ Object                 │ Device   │   Size (GB) │\n",
    "├──────────────────────┼────────────────────────┼──────────┼─────────────┤\n",
    "│ gemma_2_2b           │ HookedSAETransformer   │ cuda:0   │       11.94 │\n",
    "│ gpt2                 │ HookedSAETransformer   │ cuda:0   │        0.61 │\n",
    "│ gemma_2_2b_sae       │ SAE                    │ cuda:0   │        0.28 │\n",
    "│ sae_resid_dirs       │ Tensor (4, 24576, 768) │ cuda:0   │        0.28 │\n",
    "│ gpt2_sae             │ SAE                    │ cuda:0   │        0.14 │\n",
    "│ logits               │ Tensor (4, 15, 50257)  │ cuda:0   │        0.01 │\n",
    "│ logits_with_ablation │ Tensor (4, 15, 50257)  │ cuda:0   │        0.01 │\n",
    "│ clean_logits         │ Tensor (4, 15, 50257)  │ cuda:0   │        0.01 │\n",
    "│ _                    │ Tensor (16, 128, 768)  │ cuda:0   │        0.01 │\n",
    "│ clean_sae_acts_post  │ Tensor (4, 15, 24576)  │ cuda:0   │        0.01 │\n",
    "└──────────────────────┴────────────────────────┴──────────┴─────────────┘</pre>\n",
    "\n",
    "From this, we see that we've allocated a lot of memory for the the Gemma model, so let's delete it. We'll also run some code to move any remaining objects on the GPU which are larger than 100MB to the CPU, and print the memory status again.\n",
    "\n",
    "```python\n",
    "del gemma_2_2b\n",
    "del gemma_2_2b_sae\n",
    "\n",
    "THRESHOLD = 0.1  # GB\n",
    "for obj in gc.get_objects():\n",
    "    try:\n",
    "        if isinstance(obj, torch.nn.Module) and part32_utils.get_tensors_size(obj) / 1024**3 > THRESHOLD:\n",
    "            if hasattr(obj, \"cuda\"):\n",
    "                obj.cpu()\n",
    "            if hasattr(obj, \"reset\"):\n",
    "                obj.reset()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Move our gpt2 model & SAEs back to GPU (we'll need them for the exercises we're about to do)\n",
    "gpt2.to(device)\n",
    "gpt2_saes = {layer: sae.to(device) for layer, sae in gpt2_saes.items()}\n",
    "\n",
    "part32_utils.print_memory_status()\n",
    "```\n",
    "\n",
    "<pre style=\"font-family: Consolas; font-size: 14px\">Allocated = 14.90 GB\n",
    "Reserved = 39.56 GB\n",
    "Free = 24.66</pre>\n",
    "\n",
    "Mission success! We've managed to free up a lot of memory. Note that the code which moves all objects collected by the garbage collector to the CPU is often necessary to free up the memory. We can't just delete the objects directly because PyTorch can still sometimes keep references to them (i.e. their tensors) in memory. In fact, if you add code to the for loop above to print out `obj.shape` when `obj` is a tensor, you'll see that a lot of those tensors are actually Gemma model weights, even once you've deleted `gemma_2_2b`.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from typing import Any, Dict, Optional, Tuple, List, Callable, Set\n",
    "from dataclasses import dataclass, field\n",
    "from functools import partial\n",
    "\n",
    "import evaluate\n",
    "import datasets\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import PreTrainedTokenizerBase\n",
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "from datasets.arrow_dataset import LazyDict\n",
    "from tabulate import tabulate\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import ActivationCache  # noqa: F401\n",
    "\n",
    "from interpretune.adapters.core import ITModule\n",
    "from interpretune.base.call import it_init\n",
    "from interpretune.base.config.datamodule import PromptConfig, ITDataModuleConfig\n",
    "from interpretune.base.config.module import ITConfig\n",
    "from interpretune.base.config.mixins import GenerativeClassificationConfig\n",
    "from interpretune.base.components.mixins import ProfilerHooksMixin\n",
    "from interpretune.base.datamodules import ITDataModule\n",
    "from interpretune.utils.logging import rank_zero_warn\n",
    "from interpretune.utils.types import STEP_OUTPUT\n",
    "from interpretune.utils.tokenization import _sanitize_input_name\n",
    "from interpretune.adapters.sae_lens import SAELensFromPretrainedConfig\n",
    "from interpretune.base.config.shared import Adapter\n",
    "from interpretune.base.contract.session import ITSessionConfig, ITSession\n",
    "from it_examples import _ACTIVE_PATCHES # noqa: F401  # TODO: add note about this unless patched in SL before release\n",
    "from interpretune.utils.analysis import (SaveCfg, AnalysisCache,  AnalysisBatch, AnalysisCfg, run_with_ctx,\n",
    "                                         construct_names_filter, loss_and_logit_diffs, create_attribution_tables,\n",
    "                                         calculate_latent_metrics, display_latent_dashboards, calc_activation_summary,\n",
    "                                         plot_latent_effects, display_ref_vs_sae_logit_diffs, latent_metrics_scatter,\n",
    "                                         compute_correct, AnalysisMode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our Tutorial Configuration\n",
    "# By default, we will run all analysis modes available with LatentAttributionMixin:\n",
    "@dataclass\n",
    "class TutorialConfig:\n",
    "    analysis_mode_demos: Set[AnalysisMode] = field(default_factory=lambda: {\n",
    "        AnalysisMode.clean_no_sae, AnalysisMode.clean_w_sae, AnalysisMode.attr_patching, AnalysisMode.ablation})\n",
    "    limit_test_batches: int = 3\n",
    "    target_layers: List[int] = field(default_factory=lambda: [9, 10])\n",
    "    latent_effects_graphs: bool = True\n",
    "    latent_effects_graphs_per_batch: bool = False  # can be overwhelming with many batches\n",
    "    latents_table_per_sae: bool = True\n",
    "    top_k_latents_table: int = 2\n",
    "    top_k_latent_dashboards: int = 1  # (don't set too high, num dashboards = top_k_latent_dashboards * num_hooks * 2)\n",
    "    top_k_clean_logit_diffs: int = 10\n",
    "    sae_release: str = \"gpt2-small-hook-z-kk\"\n",
    "    sae_hook_point: str = \"hook_sae_acts_post\"\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.latent_effects_graphs_per_batch and not self.latent_effects_graphs:\n",
    "            print(\"Note: Setting latent_effects_graphs to True since latent_effects_graphs_per_batch is True\")\n",
    "            self.latent_effects_graphs = True\n",
    "\n",
    "# Change config here if you want to run a different set of analysis modes\n",
    "# e.g. TutorialConfig(analysis_mode_demos={AnalysisMode.attr_patching})\n",
    "#tutorial_config = TutorialConfig()\n",
    "# various test configs\n",
    "# tutorial_config = TutorialConfig(analysis_mode_demos={AnalysisMode.ablation})\n",
    "# tutorial_config = TutorialConfig(analysis_mode_demos={AnalysisMode.attr_patching}, limit_test_batches=-1)\n",
    "# tutorial_config = TutorialConfig(analysis_mode_demos={AnalysisMode.clean_no_sae, AnalysisMode.clean_w_sae},\n",
    "#                                  limit_test_batches=-1)\n",
    "# tutorial_config = TutorialConfig(analysis_mode_demos={AnalysisMode.clean_no_sae, AnalysisMode.clean_w_sae, \n",
    "#                                                       AnalysisMode.ablation, AnalysisMode.attr_patching}, \n",
    "#                                                       limit_test_batches=-1)\n",
    "tutorial_config = TutorialConfig(analysis_mode_demos={AnalysisMode.clean_no_sae, AnalysisMode.clean_w_sae,\n",
    "                                                      AnalysisMode.ablation, AnalysisMode.attr_patching},\n",
    "                                                      limit_test_batches=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Our IT Data Module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "TASK_TEXT_FIELD_MAP = {\"rte\": (\"premise\", \"hypothesis\"), \"boolq\": (\"passage\", \"question\")}\n",
    "TASK_NUM_LABELS = {\"boolq\": 2, \"rte\": 2}\n",
    "DEFAULT_TASK = \"rte\"\n",
    "INVALID_TASK_MSG = f\" is an invalid task_name. Proceeding with the default task: {DEFAULT_TASK!r}\"\n",
    "\n",
    "class RTEBoolqDataModule(ITDataModule):\n",
    "    def __init__(self, itdm_cfg: ITDataModuleConfig) -> None:\n",
    "        if itdm_cfg.task_name not in TASK_NUM_LABELS.keys():\n",
    "            rank_zero_warn(itdm_cfg.task_name + INVALID_TASK_MSG)\n",
    "            itdm_cfg.task_name = DEFAULT_TASK\n",
    "        itdm_cfg.text_fields = TASK_TEXT_FIELD_MAP[itdm_cfg.task_name]\n",
    "        super().__init__(itdm_cfg=itdm_cfg)\n",
    "\n",
    "    def prepare_data(self, target_model: Optional[torch.nn.Module] = None) -> None:\n",
    "        \"\"\"Load the SuperGLUE dataset.\"\"\"\n",
    "        # N.B. prepare_data is called in a single process (rank 0, either per node or globally) so do not use it to\n",
    "        # assign state (e.g. self.x=y)\n",
    "        # note for raw pytorch we require a target_model\n",
    "        # NOTE [HF Datasets Transformation Caching]:\n",
    "        # HF Datasets' transformation cache fingerprinting algo necessitates construction of these partials as the hash\n",
    "        # is generated using function args, dataset file, mapping args: https://bit.ly/HF_Datasets_fingerprint_algo)\n",
    "        tokenization_func = partial(\n",
    "            self.encode_for_rteboolq,\n",
    "            tokenizer=self.tokenizer,\n",
    "            text_fields=self.itdm_cfg.text_fields,\n",
    "            prompt_cfg=self.itdm_cfg.prompt_cfg,\n",
    "            template_fn=self.itdm_cfg.prompt_cfg.model_chat_template_fn,\n",
    "            tokenization_pattern=self.itdm_cfg.cust_tokenization_pattern,\n",
    "        )\n",
    "        dataset = datasets.load_dataset(\"super_glue\", self.itdm_cfg.task_name, trust_remote_code=True)\n",
    "        for split in dataset.keys():\n",
    "            dataset[split] = dataset[split].map(tokenization_func, **self.itdm_cfg.prepare_data_map_cfg)\n",
    "            dataset[split] = self._remove_unused_columns(dataset[split], target_model)\n",
    "        dataset.save_to_disk(self.itdm_cfg.dataset_path)\n",
    "\n",
    "    def dataloader_factory(self, split: str, use_train_batch_size: bool = False) -> DataLoader:\n",
    "        dataloader_kwargs = {\"dataset\": self.dataset[split], \"collate_fn\":self.data_collator,\n",
    "                             **self.itdm_cfg.dataloader_kwargs}\n",
    "        dataloader_kwargs['batch_size'] = self.itdm_cfg.train_batch_size if use_train_batch_size else \\\n",
    "            self.itdm_cfg.eval_batch_size\n",
    "        return DataLoader(**dataloader_kwargs)\n",
    "\n",
    "    # TODO: change to partialmethod's?\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return self.dataloader_factory(split='train', use_train_batch_size=True)\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return self.dataloader_factory(split='validation')\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return self.dataloader_factory(split='validation')\n",
    "\n",
    "    def predict_dataloader(self) -> DataLoader:\n",
    "        return self.dataloader_factory(split='validation')\n",
    "\n",
    "    #TODO: relax PreTrainedTokenizerBase to the protocol that is actually required\n",
    "    @staticmethod\n",
    "    def encode_for_rteboolq(example_batch: LazyDict, tokenizer: PreTrainedTokenizerBase, text_fields: List[str],\n",
    "                            prompt_cfg: PromptConfig, template_fn: Callable,\n",
    "                            tokenization_pattern: Optional[str] = None) -> BatchEncoding:\n",
    "        example_batch['sequences'] = []\n",
    "        # TODO: use promptsource instead of this manual approach after tinkering\n",
    "        for field1, field2 in zip(example_batch[text_fields[0]],\n",
    "                                  example_batch[text_fields[1]]):\n",
    "            if prompt_cfg.cust_task_prompt:\n",
    "                task_prompt = (prompt_cfg.cust_task_prompt['context'] + \"\\n\" +\n",
    "                               field1 + \"\\n\" +\n",
    "                               prompt_cfg.cust_task_prompt['question'] + \"\\n\" +\n",
    "                               field2)\n",
    "            else:\n",
    "                field2 = field2.rstrip('.')\n",
    "                task_prompt = (field1 + prompt_cfg.ctx_question_join + field2 \\\n",
    "                               + prompt_cfg.question_suffix)\n",
    "            sequence = template_fn(task_prompt=task_prompt, tokenization_pattern=tokenization_pattern)\n",
    "            example_batch['sequences'].append(sequence)\n",
    "        features = tokenizer.batch_encode_plus(example_batch[\"sequences\"], padding=\"longest\",\n",
    "                                               padding_side=tokenizer.padding_side)\n",
    "        features[\"labels\"] = example_batch[\"label\"]  # Rename label to labels, easier to pass to model forward\n",
    "        features = _sanitize_input_name(tokenizer.model_input_names, features)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Our IT Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(kw_only=True)\n",
    "class RTEBoolqEntailmentMapping:\n",
    "    entailment_mapping: Tuple = (\"Yes\", \"No\")  # RTE style, invert mapping for BoolQ\n",
    "    entailment_mapping_indices: Optional[torch.Tensor] = None\n",
    "\n",
    "\n",
    "@dataclass(kw_only=True)\n",
    "class RTEBoolqPromptConfig(PromptConfig):\n",
    "    ctx_question_join: str = \" Does the previous passage imply that \"\n",
    "    question_suffix: str = \"? Answer with only one word, either Yes or No.\"\n",
    "    cust_task_prompt: Optional[Dict[str, Any]] = None\n",
    "\n",
    "\n",
    "class RTEBoolqModule(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # when using TransformerLens, we need to manually calculate our loss from logit output\n",
    "        self.loss_fn = CrossEntropyLoss()\n",
    "\n",
    "    def setup(self, *args, **kwargs) -> None:\n",
    "        super().setup(*args, **kwargs)\n",
    "        self._init_entailment_mapping()\n",
    "\n",
    "    def _before_it_cfg_init(self, it_cfg: ITConfig) -> ITConfig:\n",
    "        if it_cfg.task_name not in TASK_NUM_LABELS.keys():\n",
    "            rank_zero_warn(it_cfg.task_name + INVALID_TASK_MSG)\n",
    "            it_cfg.task_name = DEFAULT_TASK\n",
    "        it_cfg.num_labels = 0 if it_cfg.generative_step_cfg.enabled else TASK_NUM_LABELS[it_cfg.task_name]\n",
    "        return it_cfg\n",
    "\n",
    "    def load_metric(self) -> None:\n",
    "        self.metric = evaluate.load(\n",
    "            \"super_glue\", self.it_cfg.task_name, experiment_id=self._it_state._init_hparams[\"experiment_id\"]\n",
    "        )\n",
    "\n",
    "    def _init_entailment_mapping(self) -> None:\n",
    "        ent_cfg, tokenizer = self.it_cfg, self.datamodule.tokenizer\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(ent_cfg.entailment_mapping)\n",
    "        device = self.device if isinstance(self.device, torch.device) else self.output_device\n",
    "        ent_cfg.entailment_mapping_indices = torch.tensor(token_ids).to(device)\n",
    "\n",
    "    def labels_to_ids(self, labels: List[str]) -> List[int]:\n",
    "        return torch.take(self.it_cfg.entailment_mapping_indices, labels), labels\n",
    "\n",
    "    @ProfilerHooksMixin.memprofilable\n",
    "    def training_step(self, batch: BatchEncoding, batch_idx: int) -> STEP_OUTPUT:\n",
    "        # TODO: need to be explicit about the compatibility constraints/contract\n",
    "        # TODO: note that this example uses generative_step_cfg and lm_head except for the test_step where we demo how\n",
    "        # to use the GenerativeStepMixin to run inference with or without a generative_step_cfg enabled as well as with\n",
    "        # different heads (e.g., seqclassification or LM head in this case)\n",
    "        answer_logits, labels, *_ = self.logits_and_labels(batch, batch_idx)\n",
    "        loss = self.loss_fn(answer_logits, labels)\n",
    "        self.log(\"train_loss\", loss, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    @ProfilerHooksMixin.memprofilable\n",
    "    def validation_step(self, batch: BatchEncoding, batch_idx: int, dataloader_idx: int = 0) -> Optional[STEP_OUTPUT]:\n",
    "        answer_logits, labels, orig_labels, cache = self.logits_and_labels(batch, batch_idx)\n",
    "        val_loss = self.loss_fn(answer_logits, labels)\n",
    "        self.log(\"val_loss\", val_loss, prog_bar=True, sync_dist=True)\n",
    "        self.collect_answers(answer_logits, orig_labels)\n",
    "\n",
    "    @ProfilerHooksMixin.memprofilable\n",
    "    def test_step(self, batch: BatchEncoding, batch_idx: int, dataloader_idx: int = 0) -> Optional[STEP_OUTPUT]:\n",
    "        if self.it_cfg.generative_step_cfg.enabled:\n",
    "            self.generative_classification_test_step(batch, batch_idx, dataloader_idx=dataloader_idx)\n",
    "        else:\n",
    "            self.default_test_step(batch, batch_idx, dataloader_idx=dataloader_idx)\n",
    "\n",
    "    def generative_classification_test_step(\n",
    "        self, batch: BatchEncoding, batch_idx: int, dataloader_idx: int = 0\n",
    "    ) -> Optional[STEP_OUTPUT]:\n",
    "        labels = batch.pop(\"labels\")\n",
    "        outputs = self.it_generate(batch, **self.it_cfg.generative_step_cfg.lm_generation_cfg.generate_kwargs)\n",
    "        self.collect_answers(outputs.logits, labels)\n",
    "\n",
    "    def default_test_step(self, batch: BatchEncoding, batch_idx: int, dataloader_idx: int = 0) -> Optional[STEP_OUTPUT]:\n",
    "        labels = batch.pop(\"labels\")\n",
    "        outputs = self(**batch)\n",
    "        self.collect_answers(outputs.logits, labels)\n",
    "\n",
    "    def predict_step(self, batch: BatchEncoding, batch_idx: int, dataloader_idx: int = 0) -> Optional[STEP_OUTPUT]:\n",
    "        labels = batch.pop(\"labels\")\n",
    "        outputs = self(**batch)\n",
    "        return self.collect_answers(outputs, labels, mode=\"return\")\n",
    "\n",
    "    def collect_answers(self, logits: torch.Tensor | tuple, labels: torch.Tensor, mode: str = \"log\") -> Optional[Dict]:\n",
    "        logits = self.standardize_logits(logits)\n",
    "        per_example_answers, _ = torch.max(logits, dim=-2)\n",
    "        preds = torch.argmax(per_example_answers, axis=-1)  # type: ignore[call-arg]\n",
    "        metric_dict = self.metric.compute(predictions=preds, references=labels)\n",
    "        # TODO: check if this type casting is still required for lightning torchmetrics, bug should be fixed now...\n",
    "        metric_dict = dict(\n",
    "            map(lambda x: (x[0], torch.tensor(x[1], device=self.device).to(torch.float32)), metric_dict.items())\n",
    "        )\n",
    "        if mode == \"log\":\n",
    "            self.log_dict(metric_dict, prog_bar=True, sync_dist=True)\n",
    "        else:\n",
    "            return metric_dict\n",
    "\n",
    "    def standardize_logits(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        # to support generative classification/non-generative classification configs and LM/SeqClassification heads we\n",
    "        # adhere to the following logits logical shape invariant:\n",
    "        # [batch size, positions to consider, answers to consider]\n",
    "        if isinstance(logits, tuple):\n",
    "            logits = torch.stack([out for out in logits], dim=1)\n",
    "        logits = logits.to(device=self.device)\n",
    "        if logits.ndim == 2:  # if answer logits have already been squeezed\n",
    "            logits = logits.unsqueeze(1)\n",
    "        if logits.shape[-1] != self.it_cfg.num_labels:\n",
    "            logits = torch.index_select(logits, -1, self.it_cfg.entailment_mapping_indices)\n",
    "            if not self.it_cfg.generative_step_cfg.enabled:\n",
    "                logits = logits[:, -1:, :]\n",
    "        return logits\n",
    "\n",
    "    def logits_and_labels(\n",
    "        self, batch: BatchEncoding, batch_idx: int, run_ctx: str, hook_names: Optional[str] = None\n",
    "    ) -> torch.Tensor:\n",
    "        label_ids, labels = self.labels_to_ids(batch.pop(\"labels\"))\n",
    "        cache = None\n",
    "        logits, cache = self.run_with_ctx(run_ctx, batch, batch_idx, hook_names=hook_names)\n",
    "        # TODO: add another layer of abstraction here to handle different model output types? Tradeoffs to consider...\n",
    "        if not isinstance(logits, torch.Tensor):\n",
    "            logits = logits.logits\n",
    "            assert isinstance(logits, torch.Tensor), f\"Expected logits to be a torch.Tensor but got {type(logits)}\"\n",
    "        return torch.squeeze(logits[:, -1, :], dim=1), label_ids, labels, cache\n",
    "\n",
    "    def analysis_test_step(\n",
    "        self, batch: BatchEncoding, batch_idx: int, analysis_cfg: AnalysisCfg, dataloader_idx: int = 0\n",
    "    ) -> Optional[STEP_OUTPUT]:\n",
    "        label_ids, orig_labels = self.labels_to_ids(batch.pop(\"labels\"))\n",
    "        analysis_batch = AnalysisBatch(labels=label_ids, orig_labels=orig_labels)\n",
    "        run_with_ctx(self, analysis_batch, analysis_cfg, batch, batch_idx)\n",
    "        loss_and_logit_diffs(self, analysis_batch, analysis_cfg, batch, batch_idx)\n",
    "        analysis_cfg.analysis_cache.save(analysis_batch, batch, tokenizer=self.datamodule.tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure our IT Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpretune.adapters.transformer_lens import TLensGenerationConfig\n",
    "from interpretune.base.config.mixins import HFFromPretrainedConfig\n",
    "from interpretune.adapters.transformer_lens import ITLensFromPretrainedNoProcessingConfig\n",
    "from interpretune.base.config.shared import ITSharedConfig, AutoCompConfig\n",
    "\n",
    "shared_cfg = ITSharedConfig(model_name_or_path='gpt2', task_name='rte', tokenizer_id_overrides={'pad_token_id': 50256},\n",
    "                  tokenizer_kwargs={'model_input_names': ['input'], 'padding_side': 'left', 'add_bos_token': True})\n",
    "datamodule_cfg = ITDataModuleConfig(prompt_cfg=RTEBoolqPromptConfig(), train_batch_size=2, eval_batch_size=2,\n",
    "                                    signature_columns=['input', 'labels'], prepare_data_map_cfg={\"batched\": True})\n",
    "genclassif_cfg = GenerativeClassificationConfig(enabled=True, lm_generation_cfg=TLensGenerationConfig(max_new_tokens=1))\n",
    "hf_cfg = HFFromPretrainedConfig(pretrained_kwargs={'torch_dtype': 'float32'}, model_head='transformers.GPT2LMHeadModel')\n",
    "tl_cfg = ITLensFromPretrainedNoProcessingConfig(model_name=\"gpt2-small\", default_padding_side='left')\n",
    "sae_cfgs = [SAELensFromPretrainedConfig(release=\"gpt2-small-hook-z-kk\", sae_id=f\"blocks.{layer}.hook_z\")\n",
    "            for layer in tutorial_config.target_layers]\n",
    "\n",
    "auto_comp_cfg = AutoCompConfig(module_cfg_name='RTEBoolqConfig', module_cfg_mixin=RTEBoolqEntailmentMapping)\n",
    "module_cfg = ITConfig(auto_comp_cfg=auto_comp_cfg, generative_step_cfg=genclassif_cfg, hf_from_pretrained_cfg=hf_cfg,\n",
    "                      tl_cfg=tl_cfg, sae_cfgs=sae_cfgs)\n",
    "\n",
    "session_cfg = ITSessionConfig(adapter_ctx=(Adapter.core, Adapter.sae_lens),\n",
    "                              datamodule_cls=RTEBoolqDataModule, module_cls=RTEBoolqModule,\n",
    "                              shared_cfg=shared_cfg, datamodule_cfg=datamodule_cfg, module_cfg=module_cfg)\n",
    "it_session = ITSession(session_cfg)\n",
    "# TODO: maybe open a PR for the below\n",
    "# https://github.com/jbloomAus/SAELens/blob/aa8f42bf06d9c68bb890f4881af0aac916ecd17c/sae_lens/sae.py#L144-L151 warning\n",
    "# that inspects whether the loaded model has a default config override specified in ``pretrained_saes.yaml`` (e.g.\n",
    "# 'gpt2-small-res-jb', config_overrides: model_from_pretrained_kwargs: center_writing_weights: true) and if so, avoids\n",
    "# giving an arguably spurious warning to the user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Demo Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: consider instantiating a trainer here once initial exploration is done\n",
    "# manually init IT components for now\n",
    "it_init(**it_session)\n",
    "sl_test_module = it_session.module\n",
    "assert sl_test_module.it_cfg.entailment_mapping_indices is not None\n",
    "\n",
    "# TODO: move to module\n",
    "def run_test_split_fn(\n",
    "    module: ITModule,\n",
    "    datamodule: ITDataModule,\n",
    "    limit_test_batches: int,\n",
    "    analysis_cfg: AnalysisCfg = field(default_factory=AnalysisCfg),\n",
    "    step_fn: str = \"analysis_test_step\",\n",
    "    *args,\n",
    "    **kwargs,\n",
    "):\n",
    "    dataloader = datamodule.test_dataloader()\n",
    "    module._it_state._current_epoch = 0  # TODO: test removing this, prob not needed in this context\n",
    "    step_func = getattr(module, step_fn)\n",
    "    for batch_idx, batch in tqdm(enumerate(dataloader)):\n",
    "        if batch_idx >= limit_test_batches >= 0:\n",
    "            break\n",
    "        batch = module.batch_to_device(batch)\n",
    "        step_func(batch, batch_idx, analysis_cfg=analysis_cfg, *args, **kwargs)\n",
    "        module.global_step += 1\n",
    "    return analysis_cfg.analysis_cache  # return cache handle for further analysis\n",
    "\n",
    "logit_diff_summs = {}\n",
    "test_cache_base_args = dict(limit_test_batches=tutorial_config.limit_test_batches, **it_session)\n",
    "torch.set_grad_enabled(False)\n",
    "names_filter = construct_names_filter(sl_test_module, target_hooks=(tutorial_config.target_layers,\n",
    "                                                                    [tutorial_config.sae_hook_point]))\n",
    "\n",
    "def run_clean_w_sae():\n",
    "    logit_diff_summs[AnalysisMode.clean_w_sae] = AnalysisCache(save_cfg=SaveCfg(prompts=True, tokens=True))\n",
    "    analysis_cfg = AnalysisCfg(mode=AnalysisMode.clean_w_sae, analysis_cache=logit_diff_summs[AnalysisMode.clean_w_sae],\n",
    "                                names_filter=names_filter)\n",
    "    run_test_split_fn(analysis_cfg=analysis_cfg, **test_cache_base_args)\n",
    "\n",
    "if AnalysisMode.clean_w_sae in tutorial_config.analysis_mode_demos:\n",
    "    run_clean_w_sae()\n",
    "\n",
    "if AnalysisMode.clean_no_sae in tutorial_config.analysis_mode_demos:\n",
    "    logit_diff_summs[AnalysisMode.clean_no_sae] = (\n",
    "        AnalysisCache() if AnalysisMode.clean_w_sae in tutorial_config.analysis_mode_demos\n",
    "        else AnalysisCache(save_cfg=SaveCfg(prompts=True, tokens=True))\n",
    "    )\n",
    "    analysis_cfg = AnalysisCfg(analysis_cache=logit_diff_summs[AnalysisMode.clean_no_sae])\n",
    "    run_test_split_fn(analysis_cfg=analysis_cfg, **test_cache_base_args)\n",
    "\n",
    "if AnalysisMode.ablation in tutorial_config.analysis_mode_demos:\n",
    "    # Run clean_w_sae mode as well since ablation mode requires the base logit diffs\n",
    "    if AnalysisMode.clean_w_sae not in logit_diff_summs:\n",
    "        run_clean_w_sae()\n",
    "        print(\"Note, running clean SAE cache first to get the base logit diffs and other cache values\")\n",
    "    logit_diff_summs[AnalysisMode.ablation] = AnalysisCache()\n",
    "    analysis_cfg = AnalysisCfg(mode=AnalysisMode.ablation, analysis_cache=logit_diff_summs[AnalysisMode.ablation],\n",
    "                               names_filter=names_filter,\n",
    "                               base_logit_diffs=logit_diff_summs[AnalysisMode.clean_w_sae].logit_diffs,\n",
    "                               answer_indices=logit_diff_summs[AnalysisMode.clean_w_sae].answer_indices,\n",
    "                               alive_latents=logit_diff_summs[AnalysisMode.clean_w_sae].alive_latents)\n",
    "    run_test_split_fn(analysis_cfg=analysis_cfg, **test_cache_base_args)\n",
    "\n",
    "if AnalysisMode.attr_patching in tutorial_config.analysis_mode_demos:\n",
    "    torch.set_grad_enabled(True)\n",
    "    prompts_tokens_flags = {\"prompts\": True, \"tokens\": True} if not run_clean_w_sae else {}\n",
    "    logit_diff_summs[AnalysisMode.attr_patching] = AnalysisCache(save_cfg=SaveCfg(**prompts_tokens_flags))\n",
    "    analysis_cfg = AnalysisCfg(\n",
    "        mode=AnalysisMode.attr_patching,\n",
    "        analysis_cache=logit_diff_summs[AnalysisMode.attr_patching],\n",
    "        names_filter=names_filter\n",
    "    )\n",
    "    run_test_split_fn(analysis_cfg=analysis_cfg, **test_cache_base_args)\n",
    "    torch.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Demo Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean vs SAE Sample-wise Logit Diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if {AnalysisMode.clean_no_sae, AnalysisMode.clean_w_sae}.issubset(tutorial_config.analysis_mode_demos):\n",
    "    display_ref_vs_sae_logit_diffs(sae=logit_diff_summs[AnalysisMode.clean_w_sae],\n",
    "                                   no_sae_ref=logit_diff_summs[AnalysisMode.clean_no_sae],\n",
    "                                   top_k=tutorial_config.top_k_clean_logit_diffs,\n",
    "                                   tokenizer=sl_test_module.datamodule.tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proportion Correct Answers on Dataset By Analysis Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_summaries = {mode: compute_correct(summ, mode) for mode, summ in logit_diff_summs.items()}\n",
    "\n",
    "table_rows = []\n",
    "for mode, (total_correct, percentage_correct, _) in pred_summaries.items():\n",
    "    table_rows.append([mode, total_correct, f\"{percentage_correct:.2f}%\"])\n",
    "\n",
    "print(tabulate(table_rows, headers=[\"Mode\", \"Total Correct\", \"Percentage Correct\"], tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Per Batch Ablation Effect Graphs [Optional]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tutorial_config.latent_effects_graphs and AnalysisMode.ablation in tutorial_config.analysis_mode_demos:\n",
    "    # TODO: add note that only latent effects associated with correct answers currently displayed\n",
    "    # TODO: allow toggling correct filtering during runs\n",
    "    plot_latent_effects(logit_diff_summs[AnalysisMode.ablation],\n",
    "                        per_batch=tutorial_config.latent_effects_graphs_per_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Per-SAE Ablation Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if AnalysisMode.ablation in tutorial_config.analysis_mode_demos:\n",
    "    ablation_batch_preds = pred_summaries[AnalysisMode.ablation].batch_predictions\n",
    "    activation_summary = calc_activation_summary(logit_diff_summs[AnalysisMode.clean_w_sae])\n",
    "    ablation_metrics = calculate_latent_metrics(\n",
    "        analysis_cache=logit_diff_summs[AnalysisMode.ablation],\n",
    "        pred_summ=pred_summaries[AnalysisMode.ablation],\n",
    "        activation_summary=activation_summary,\n",
    "        # filter_by_correct=True,\n",
    "        run_name=\"ablation\"\n",
    "    )\n",
    "\n",
    "    tables = create_attribution_tables(metrics=ablation_metrics,\n",
    "                                       top_k=tutorial_config.top_k_latents_table, filter_type='both',\n",
    "                                       per_sae=tutorial_config.latents_table_per_sae)\n",
    "\n",
    "    for title, table in tables.items():\n",
    "        print(f\"\\n{title}\\n{table}\\n\")\n",
    "\n",
    "    display_latent_dashboards(ablation_metrics, title=\"Ablation-Mediated Latent Analysis\",\n",
    "                              sae_release=tutorial_config.sae_release, top_k=tutorial_config.top_k_latent_dashboards)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Per-SAE Attribution Patching Effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if AnalysisMode.attr_patching in tutorial_config.analysis_mode_demos:\n",
    "    # per-SAE activation summaries are calculated using our AnalysisCache since the relevant keys are present,\n",
    "    # no need to provide a separate activation summary from another comparison cache in this case as with ablation\n",
    "    activation_summary = calc_activation_summary(logit_diff_summs[AnalysisMode.attr_patching])\n",
    "    attribution_patching_metrics = calculate_latent_metrics(\n",
    "        analysis_cache=logit_diff_summs[AnalysisMode.attr_patching],\n",
    "        pred_summ=pred_summaries[AnalysisMode.attr_patching],\n",
    "        run_name=\"attr_patching\"\n",
    "    )\n",
    "\n",
    "    tables = create_attribution_tables(metrics=attribution_patching_metrics,\n",
    "                                       top_k=tutorial_config.top_k_latents_table, filter_type='both',\n",
    "                                       per_sae=tutorial_config.latents_table_per_sae)\n",
    "\n",
    "    for title, table in tables.items():\n",
    "        print(f\"\\n{title}\\n{table}\\n\")\n",
    "\n",
    "    display_latent_dashboards(attribution_patching_metrics, title=\"Attribution Patching-Mediated Latent Analysis\",\n",
    "                            sae_release=tutorial_config.sae_release, top_k=tutorial_config.top_k_latent_dashboards)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Per-SAE Ablation vs Attribution-Patching Effect Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if {AnalysisMode.attr_patching, AnalysisMode.ablation}.issubset(tutorial_config.analysis_mode_demos):\n",
    "    # Visualize results for each hook\n",
    "    # Call the function with our metrics\n",
    "    latent_metrics_scatter(ablation_metrics, attribution_patching_metrics, label1=\"Ablation\", label2=\"Attribution Patching\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: continue implementation here\n",
    "#   - finish moving/testing LatentMetrics calculation for attr_patching cell to analysis.py module\n",
    "#   - move analysis_test_step components into LatentAttributionMixin in saelens adapter\n",
    "#   - re-test padding right, verify/adjust if necessary for multiple answer positions\n",
    "#   - may need to refactor per-hook data structures to be more efficient for summarization\n",
    "#    (once desired summary stats stabilize)\n",
    "#   - apply created pipeline to gemma2 and other models!\n",
    "#   - demo framework flexibility by running Lightning-supported steps with lightning and non-lightning supported custom\n",
    "#     steps (analysis step and sae training steps) with IT basictrainer and/or raw pytorch training loop\n",
    "#   - explore using a threshold positive logit diff for tuning\n",
    "#   - though relative logits remain comparable, investigate impact of logit magnitude being substantially smaller with\n",
    "#     padding_side=left (should be able to re-run with padding_side=right)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "it_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
