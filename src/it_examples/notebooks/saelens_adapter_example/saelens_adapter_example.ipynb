{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretune SAELens Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fine-Tuning Scheduler logo](logo_fts.png){height=\"55px\" width=\"401px\"}\n",
    "\n",
    "### Intro\n",
    "\n",
    "[Interpretune](https://github.com/speediedan/interpretune) is a flexible framework for exploring, analyzing and tuning llm world models. In this tutorial, we'll walk through a simple example of using Interpretune to pursue interpretability research with SAELens. As we'll see, Interpretune handles the required execution context composition, allowing us to use the same code in a variety of contexts, depending upon the level of abstraction required.\n",
    "\n",
    "As a long-time PyTorch and PyTorch Lightning contributor, I've found the PyTorch Lightning framework is the right level of abstraction for a large variety of ML research contexts, but some contexts benefit from using core PyTorch directly. Additionally, some users may prefer to use the core PyTorch framework directly for a wide variety of reasons including maximizing portability. As will be demonstrated here, Interpretune maximizes flexibility and portability by adhering to a well-defined protocol that allows auto-composition of our research module with the adapters required for execution in a wide variety of contexts. In this example, we'll be executing the same module with core PyTorch and PyTorch Lightning, demonstrating the use of `SAELens` w/ Interpretune for interpretability research.\n",
    "\n",
    "> Note - **this is a WIP**, but this is the core idea. If you have any feedback, please let me know!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note on memory usage\n",
    "\n",
    "In these exercises, we'll be loading some pretty large opls into memory (e.g. Gemma 2-2B and its SAEs, as well as a host of other models in later sections of the material). It's useful to have functions which can help profile memory usage for you, so that if you encounter OOM errors you can try and clear out unnecessary models. For example, we've found that with the right memory handling (i.e. deleting models and objects when you're not using them any more) it should be possible to run all the exercises in this material on a Colab Pro notebook, and all the exercises minus the handful involving Gemma on a free Colab notebook.\n",
    "\n",
    "<details>\n",
    "<summary>See this dropdown for some functions which you might find helpful, and how to use them.</summary>\n",
    "\n",
    "First, we can run some code to inspect our current memory usage. Here's me running this code during the exercise set on SAE circuits, after having already loaded in the Gemma models from the previous section. This was on a Colab Pro notebook.\n",
    "\n",
    "```python\n",
    "# Profile memory usage, and delete gemma models if we've loaded them in\n",
    "namespace = globals().copy() | locals()\n",
    "part32_utils.profile_pytorch_memory(namespace=namespace, filter_device=\"cuda:0\")\n",
    "```\n",
    "\n",
    "<pre style=\"font-family: Consolas; font-size: 14px\">Allocated = 35.88 GB\n",
    "Total = 39.56 GB\n",
    "Free = 3.68 GB\n",
    "┌──────────────────────┬────────────────────────┬──────────┬─────────────┐\n",
    "│ Name                 │ Object                 │ Device   │   Size (GB) │\n",
    "├──────────────────────┼────────────────────────┼──────────┼─────────────┤\n",
    "│ gemma_2_2b           │ HookedSAETransformer   │ cuda:0   │       11.94 │\n",
    "│ gpt2                 │ HookedSAETransformer   │ cuda:0   │        0.61 │\n",
    "│ gemma_2_2b_sae       │ SAE                    │ cuda:0   │        0.28 │\n",
    "│ sae_resid_dirs       │ Tensor (4, 24576, 768) │ cuda:0   │        0.28 │\n",
    "│ gpt2_sae             │ SAE                    │ cuda:0   │        0.14 │\n",
    "│ logits               │ Tensor (4, 15, 50257)  │ cuda:0   │        0.01 │\n",
    "│ logits_with_ablation │ Tensor (4, 15, 50257)  │ cuda:0   │        0.01 │\n",
    "│ clean_logits         │ Tensor (4, 15, 50257)  │ cuda:0   │        0.01 │\n",
    "│ _                    │ Tensor (16, 128, 768)  │ cuda:0   │        0.01 │\n",
    "│ clean_sae_acts_post  │ Tensor (4, 15, 24576)  │ cuda:0   │        0.01 │\n",
    "└──────────────────────┴────────────────────────┴──────────┴─────────────┘</pre>\n",
    "\n",
    "From this, we see that we've allocated a lot of memory for the the Gemma model, so let's delete it. We'll also run some code to move any remaining objects on the GPU which are larger than 100MB to the CPU, and print the memory status again.\n",
    "\n",
    "```python\n",
    "del gemma_2_2b\n",
    "del gemma_2_2b_sae\n",
    "\n",
    "THRESHOLD = 0.1  # GB\n",
    "for obj in gc.get_objects():\n",
    "    try:\n",
    "        if isinstance(obj, torch.nn.Module) and part32_utils.get_tensors_size(obj) / 1024**3 > THRESHOLD:\n",
    "            if hasattr(obj, \"cuda\"):\n",
    "                obj.cpu()\n",
    "            if hasattr(obj, \"reset\"):\n",
    "                obj.reset()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Move our gpt2 model & SAEs back to GPU (we'll need them for the exercises we're about to do)\n",
    "gpt2.to(device)\n",
    "gpt2_saes = {layer: sae.to(device) for layer, sae in gpt2_saes.items()}\n",
    "\n",
    "part32_utils.print_memory_status()\n",
    "```\n",
    "\n",
    "<pre style=\"font-family: Consolas; font-size: 14px\">Allocated = 14.90 GB\n",
    "Reserved = 39.56 GB\n",
    "Free = 24.66</pre>\n",
    "\n",
    "Mission success! We've managed to free up a lot of memory. Note that the code which moves all objects collected by the garbage collector to the CPU is often necessary to free up the memory. We can't just delete the objects directly because PyTorch can still sometimes keep references to them (i.e. their tensors) in memory. In fact, if you add code to the for loop above to print out `obj.shape` when `obj` is a tensor, you'll see that a lot of those tensors are actually Gemma model weights, even once you've deleted `gemma_2_2b`.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from typing import Any, Dict, Optional, Tuple, List, Callable, Generator\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "\n",
    "import evaluate\n",
    "import datasets\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import PreTrainedTokenizerBase\n",
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "from datasets.arrow_dataset import LazyDict\n",
    "from tabulate import tabulate\n",
    "from transformer_lens import ActivationCache  # noqa: F401\n",
    "\n",
    "from interpretune.base.config.datamodule import PromptConfig, ITDataModuleConfig\n",
    "from interpretune.base.config.module import ITConfig\n",
    "from interpretune.base.config.mixins import GenerativeClassificationConfig\n",
    "from interpretune.base.components.mixins import ProfilerHooksMixin\n",
    "from interpretune.base.datamodules import ITDataModule\n",
    "from interpretune.utils.logging import rank_zero_warn\n",
    "from interpretune.utils.session_runner import AnalysisRunnerCfg, AnalysisRunner, AnalysisSetCfg\n",
    "from interpretune.utils.types import STEP_OUTPUT\n",
    "from interpretune.utils.tokenization import _sanitize_input_name\n",
    "from interpretune.adapters.sae_lens import SAELensFromPretrainedConfig\n",
    "from interpretune.base.config.shared import Adapter\n",
    "from interpretune.base.contract.session import ITSessionConfig, ITSession\n",
    "from it_examples import _ACTIVE_PATCHES # noqa: F401  # TODO: add note about this unless patched in SL before release\n",
    "from interpretune.base.analysis import (AnalysisBatch, base_vs_sae_logit_diffs, latent_metrics_scatter, compute_correct,\n",
    "                                        SAEAnalysisTargets)\n",
    "from interpretune.base.ops import ANALYSIS_OPS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our Tutorial Configuration\n",
    "# By default, we will run all analysis ops available with SAEAnalysisMixin:\n",
    "# Change config here if you want to run a different set of analysis ops, use different sae targets, etc.\n",
    "\n",
    "sae_targets = SAEAnalysisTargets(sae_release=\"gpt2-small-hook-z-kk\", target_layers=[9, 10])\n",
    "# TODO: move non-sae analysis target cfg to a separate ArtifactCfg class\n",
    "# tutorial_config = AnalysisSetCfg(sae_analysis_targets=sae_targets)  # default 1 full epoch and all analysis ops\n",
    "\n",
    "########################################################################################################################\n",
    "# Various example configs\n",
    "#-----------------------------------------------------------------------------------------------------------------------\n",
    "# tutorial_config = AnalysisSetCfg(analysis_ops=(ANALYSIS_OPS['logit_diffs.base'], ANALYSIS_OPS['logit_diffs.sae']))\n",
    "w_batch_limit_kwargs = dict(limit_analysis_batches=3, sae_analysis_targets=sae_targets)\n",
    "# tutorial_config = AnalysisSetCfg(analysis_ops=(ANALYSIS_OPS['logit_diffs.attribution.grad_based'],), **w_batch_limit_kwargs) # patching-only\n",
    "# tutorial_config = AnalysisSetCfg(analysis_ops=(ANALYSIS_OPS['logit_diffs.attribution.ablation'],), **w_batch_limit_kwargs)  # ablation-only\n",
    "# tutorial_config = AnalysisSetCfg(analysis_ops=(ANALYSIS_OPS['logit_diffs.base'],), **w_batch_limit_kwargs)  # no-sae only\n",
    "# tutorial_config = AnalysisSetCfg(analysis_ops=(ANALYSIS_OPS['logit_diffs.sae'],), **w_batch_limit_kwargs)  # w-sae only\n",
    "# tutorial_config = AnalysisSetCfg(analysis_ops=(ANALYSIS_OPS['logit_diffs.base'], ANALYSIS_OPS['logit_diffs.sae']), # clean-only\n",
    "#                                  **w_batch_limit_kwargs)\n",
    "tutorial_config = AnalysisSetCfg(**w_batch_limit_kwargs)  # all analysis ops, but limit to 3 batches\n",
    "########################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Our IT Data Module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "TASK_TEXT_FIELD_MAP = {\"rte\": (\"premise\", \"hypothesis\"), \"boolq\": (\"passage\", \"question\")}\n",
    "TASK_NUM_LABELS = {\"boolq\": 2, \"rte\": 2}\n",
    "DEFAULT_TASK = \"rte\"\n",
    "INVALID_TASK_MSG = f\" is an invalid task_name. Proceeding with the default task: {DEFAULT_TASK!r}\"\n",
    "\n",
    "class RTEBoolqDataModule(ITDataModule):\n",
    "    def __init__(self, itdm_cfg: ITDataModuleConfig) -> None:\n",
    "        if itdm_cfg.task_name not in TASK_NUM_LABELS.keys():\n",
    "            rank_zero_warn(itdm_cfg.task_name + INVALID_TASK_MSG)\n",
    "            itdm_cfg.task_name = DEFAULT_TASK\n",
    "        itdm_cfg.text_fields = TASK_TEXT_FIELD_MAP[itdm_cfg.task_name]\n",
    "        super().__init__(itdm_cfg=itdm_cfg)\n",
    "\n",
    "    def prepare_data(self, target_model: Optional[torch.nn.Module] = None) -> None:\n",
    "        \"\"\"Load the SuperGLUE dataset.\"\"\"\n",
    "        # N.B. prepare_data is called in a single process (rank 0, either per node or globally) so do not use it to\n",
    "        # assign state (e.g. self.x=y)\n",
    "        # note for raw pytorch we require a target_model\n",
    "        # NOTE [HF Datasets Transformation Caching]:\n",
    "        # HF Datasets' transformation cache fingerprinting algo necessitates construction of these partials as the hash\n",
    "        # is generated using function args, dataset file, mapping args: https://bit.ly/HF_Datasets_fingerprint_algo)\n",
    "        tokenization_func = partial(\n",
    "            self.encode_for_rteboolq,\n",
    "            tokenizer=self.tokenizer,\n",
    "            text_fields=self.itdm_cfg.text_fields,\n",
    "            prompt_cfg=self.itdm_cfg.prompt_cfg,\n",
    "            template_fn=self.itdm_cfg.prompt_cfg.model_chat_template_fn,\n",
    "            tokenization_pattern=self.itdm_cfg.cust_tokenization_pattern,\n",
    "        )\n",
    "        dataset = datasets.load_dataset(\"super_glue\", self.itdm_cfg.task_name, trust_remote_code=True)\n",
    "        for split in dataset.keys():\n",
    "            dataset[split] = dataset[split].map(tokenization_func, **self.itdm_cfg.prepare_data_map_cfg)\n",
    "            dataset[split] = self._remove_unused_columns(dataset[split], target_model)\n",
    "        dataset.save_to_disk(self.itdm_cfg.dataset_path)\n",
    "\n",
    "    def dataloader_factory(self, split: str, use_train_batch_size: bool = False) -> DataLoader:\n",
    "        dataloader_kwargs = {\"dataset\": self.dataset[split], \"collate_fn\":self.data_collator,\n",
    "                             **self.itdm_cfg.dataloader_kwargs}\n",
    "        dataloader_kwargs['batch_size'] = self.itdm_cfg.train_batch_size if use_train_batch_size else \\\n",
    "            self.itdm_cfg.eval_batch_size\n",
    "        return DataLoader(**dataloader_kwargs)\n",
    "\n",
    "    # TODO: change to partialmethod's?\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return self.dataloader_factory(split='train', use_train_batch_size=True)\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return self.dataloader_factory(split='validation')\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return self.dataloader_factory(split='validation')\n",
    "\n",
    "    def predict_dataloader(self) -> DataLoader:\n",
    "        return self.dataloader_factory(split='validation')\n",
    "\n",
    "    #TODO: relax PreTrainedTokenizerBase to the protocol that is actually required\n",
    "    @staticmethod\n",
    "    def encode_for_rteboolq(example_batch: LazyDict, tokenizer: PreTrainedTokenizerBase, text_fields: List[str],\n",
    "                            prompt_cfg: PromptConfig, template_fn: Callable,\n",
    "                            tokenization_pattern: Optional[str] = None) -> BatchEncoding:\n",
    "        example_batch['sequences'] = []\n",
    "        # TODO: use promptsource instead of this manual approach after tinkering\n",
    "        for field1, field2 in zip(example_batch[text_fields[0]],\n",
    "                                  example_batch[text_fields[1]]):\n",
    "            if prompt_cfg.cust_task_prompt:\n",
    "                task_prompt = (prompt_cfg.cust_task_prompt['context'] + \"\\n\" +\n",
    "                               field1 + \"\\n\" +\n",
    "                               prompt_cfg.cust_task_prompt['question'] + \"\\n\" +\n",
    "                               field2)\n",
    "            else:\n",
    "                field2 = field2.rstrip('.')\n",
    "                task_prompt = (field1 + prompt_cfg.ctx_question_join + field2 \\\n",
    "                               + prompt_cfg.question_suffix)\n",
    "            sequence = template_fn(task_prompt=task_prompt, tokenization_pattern=tokenization_pattern)\n",
    "            example_batch['sequences'].append(sequence)\n",
    "        features = tokenizer.batch_encode_plus(example_batch[\"sequences\"], padding=\"longest\",\n",
    "                                               padding_side=tokenizer.padding_side)\n",
    "        features[\"labels\"] = example_batch[\"label\"]  # Rename label to labels, easier to pass to model forward\n",
    "        features = _sanitize_input_name(tokenizer.model_input_names, features)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Our IT Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(kw_only=True)\n",
    "class RTEBoolqEntailmentMapping:\n",
    "    entailment_mapping: Tuple = (\"Yes\", \"No\")  # RTE style, invert mapping for BoolQ\n",
    "    entailment_mapping_indices: Optional[torch.Tensor] = None\n",
    "\n",
    "\n",
    "@dataclass(kw_only=True)\n",
    "class RTEBoolqPromptConfig(PromptConfig):\n",
    "    ctx_question_join: str = \" Does the previous passage imply that \"\n",
    "    question_suffix: str = \"? Answer with only one word, either Yes or No.\"\n",
    "    cust_task_prompt: Optional[Dict[str, Any]] = None\n",
    "\n",
    "\n",
    "class RTEBoolqModule(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # when using TransformerLens, we need to manually calculate our loss from logit output\n",
    "        self.loss_fn = CrossEntropyLoss()\n",
    "\n",
    "    def setup(self, *args, **kwargs) -> None:\n",
    "        super().setup(*args, **kwargs)\n",
    "        self._init_entailment_mapping()\n",
    "\n",
    "    def _before_it_cfg_init(self, it_cfg: ITConfig) -> ITConfig:\n",
    "        if it_cfg.task_name not in TASK_NUM_LABELS.keys():\n",
    "            rank_zero_warn(it_cfg.task_name + INVALID_TASK_MSG)\n",
    "            it_cfg.task_name = DEFAULT_TASK\n",
    "        it_cfg.num_labels = 0 if it_cfg.generative_step_cfg.enabled else TASK_NUM_LABELS[it_cfg.task_name]\n",
    "        return it_cfg\n",
    "\n",
    "    def load_metric(self) -> None:\n",
    "        self.metric = evaluate.load(\n",
    "            \"super_glue\", self.it_cfg.task_name, experiment_id=self._it_state._init_hparams[\"experiment_id\"]\n",
    "        )\n",
    "\n",
    "    def _init_entailment_mapping(self) -> None:\n",
    "        ent_cfg, tokenizer = self.it_cfg, self.datamodule.tokenizer\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(ent_cfg.entailment_mapping)\n",
    "        device = self.device if isinstance(self.device, torch.device) else self.output_device\n",
    "        ent_cfg.entailment_mapping_indices = torch.tensor(token_ids).to(device)\n",
    "\n",
    "    def labels_to_ids(self, labels: List[str]) -> List[int]:\n",
    "        return torch.take(self.it_cfg.entailment_mapping_indices, labels), labels\n",
    "\n",
    "    @ProfilerHooksMixin.memprofilable\n",
    "    def training_step(self, batch: BatchEncoding, batch_idx: int) -> STEP_OUTPUT:\n",
    "        # TODO: need to be explicit about the compatibility constraints/contract\n",
    "        # TODO: note that this example uses generative_step_cfg and lm_head except for the test_step where we demo how\n",
    "        # to use the GenerativeStepMixin to run inference with or without a generative_step_cfg enabled as well as with\n",
    "        # different heads (e.g., seqclassification or LM head in this case)\n",
    "        answer_logits, labels, *_ = self.logits_and_labels(batch, batch_idx)\n",
    "        loss = self.loss_fn(answer_logits, labels)\n",
    "        self.log(\"train_loss\", loss, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    @ProfilerHooksMixin.memprofilable\n",
    "    def validation_step(self, batch: BatchEncoding, batch_idx: int, dataloader_idx: int = 0) -> Optional[STEP_OUTPUT]:\n",
    "        answer_logits, labels, orig_labels, *_ = self.logits_and_labels(batch, batch_idx)\n",
    "        val_loss = self.loss_fn(answer_logits, labels)\n",
    "        self.log(\"val_loss\", val_loss, prog_bar=True, sync_dist=True)\n",
    "        self.collect_answers(answer_logits, orig_labels)\n",
    "\n",
    "    @ProfilerHooksMixin.memprofilable\n",
    "    def test_step(self, batch: BatchEncoding, batch_idx: int, dataloader_idx: int = 0) -> Optional[STEP_OUTPUT]:\n",
    "        if self.it_cfg.generative_step_cfg.enabled:\n",
    "            self.generative_classification_test_step(batch, batch_idx, dataloader_idx=dataloader_idx)\n",
    "        else:\n",
    "            self.default_test_step(batch, batch_idx, dataloader_idx=dataloader_idx)\n",
    "\n",
    "    def generative_classification_test_step(\n",
    "        self, batch: BatchEncoding, batch_idx: int, dataloader_idx: int = 0\n",
    "    ) -> Optional[STEP_OUTPUT]:\n",
    "        labels = batch.pop(\"labels\")\n",
    "        outputs = self.it_generate(batch, **self.it_cfg.generative_step_cfg.lm_generation_cfg.generate_kwargs)\n",
    "        self.collect_answers(outputs.logits, labels)\n",
    "\n",
    "    def default_test_step(self, batch: BatchEncoding, batch_idx: int, dataloader_idx: int = 0) -> Optional[STEP_OUTPUT]:\n",
    "        labels = batch.pop(\"labels\")\n",
    "        outputs = self(**batch)\n",
    "        self.collect_answers(outputs.logits, labels)\n",
    "\n",
    "    def predict_step(self, batch: BatchEncoding, batch_idx: int, dataloader_idx: int = 0) -> Optional[STEP_OUTPUT]:\n",
    "        labels = batch.pop(\"labels\")\n",
    "        outputs = self(**batch)\n",
    "        return self.collect_answers(outputs, labels, mode=\"return\")\n",
    "\n",
    "    # def analysis_step(self, batch: BatchEncoding, batch_idx: int, dataloader_idx: int = 0) -> Optional[STEP_OUTPUT]:\n",
    "    #     label_ids, orig_labels = self.labels_to_ids(batch.pop(\"labels\"))\n",
    "    #     analysis_batch = AnalysisBatch(labels=label_ids, orig_labels=orig_labels)\n",
    "    #     self.run_with_ctx(analysis_batch, batch, batch_idx)\n",
    "    #     self.loss_and_logit_diffs(analysis_batch, batch, batch_idx)\n",
    "    #     self.analysis_cfg.analysis_store.save(analysis_batch, batch, tokenizer=self.datamodule.tokenizer)\n",
    "\n",
    "    def analysis_step(self, batch: BatchEncoding, batch_idx: int, dataloader_idx: int = 0) -> Generator[STEP_OUTPUT,\n",
    "                                                                                                        None, None]:\n",
    "        label_ids, orig_labels = self.labels_to_ids(batch.pop(\"labels\"))\n",
    "        analysis_batch = AnalysisBatch(labels=label_ids, orig_labels=orig_labels)\n",
    "        self.run_with_ctx(analysis_batch, batch, batch_idx)\n",
    "        self.loss_and_logit_diffs(analysis_batch, batch, batch_idx)\n",
    "        yield from self.analysis_cfg.save_batch(analysis_batch, batch, tokenizer=self.datamodule.tokenizer)\n",
    "\n",
    "    def collect_answers(self, logits: torch.Tensor | tuple, labels: torch.Tensor, mode: str = \"log\") -> Optional[Dict]:\n",
    "        logits = self.standardize_logits(logits)\n",
    "        per_example_answers, _ = torch.max(logits, dim=-2)\n",
    "        preds = torch.argmax(per_example_answers, axis=-1)  # type: ignore[call-arg]\n",
    "        metric_dict = self.metric.compute(predictions=preds, references=labels)\n",
    "        # TODO: check if this type casting is still required for lightning torchmetrics, bug should be fixed now...\n",
    "        metric_dict = dict(\n",
    "            map(lambda x: (x[0], torch.tensor(x[1], device=self.device).to(torch.float32)), metric_dict.items())\n",
    "        )\n",
    "        if mode == \"log\":\n",
    "            self.log_dict(metric_dict, prog_bar=True, sync_dist=True)\n",
    "        else:\n",
    "            return metric_dict\n",
    "\n",
    "    def standardize_logits(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        # to support generative classification/non-generative classification configs and LM/SeqClassification heads we\n",
    "        # adhere to the following logits logical shape invariant:\n",
    "        # [batch size, positions to consider, answers to consider]\n",
    "        if isinstance(logits, tuple):\n",
    "            logits = torch.stack([out for out in logits], dim=1)\n",
    "        logits = logits.to(device=self.device)\n",
    "        if logits.ndim == 2:  # if answer logits have already been squeezed\n",
    "            logits = logits.unsqueeze(1)\n",
    "        if logits.shape[-1] != self.it_cfg.num_labels:\n",
    "            logits = torch.index_select(logits, -1, self.it_cfg.entailment_mapping_indices)\n",
    "            if not self.it_cfg.generative_step_cfg.enabled:\n",
    "                logits = logits[:, -1:, :]\n",
    "        return logits\n",
    "\n",
    "    def logits_and_labels(\n",
    "        self, batch: BatchEncoding, batch_idx: int, run_ctx: str, hook_names: Optional[str] = None\n",
    "    ) -> torch.Tensor:\n",
    "        label_ids, labels = self.labels_to_ids(batch.pop(\"labels\"))\n",
    "        cache = None\n",
    "        logits, cache = self.run_with_ctx(run_ctx, batch, batch_idx, hook_names=hook_names)\n",
    "        # TODO: add another layer of abstraction here to handle different model output types? Tradeoffs to consider...\n",
    "        if not isinstance(logits, torch.Tensor):\n",
    "            logits = logits.logits\n",
    "            assert isinstance(logits, torch.Tensor), f\"Expected logits to be a torch.Tensor but got {type(logits)}\"\n",
    "        return torch.squeeze(logits[:, -1, :], dim=1), label_ids, labels, cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure our IT Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpretune.adapters.transformer_lens import TLensGenerationConfig\n",
    "from interpretune.base.config.mixins import HFFromPretrainedConfig\n",
    "from interpretune.adapters.transformer_lens import ITLensFromPretrainedNoProcessingConfig\n",
    "from interpretune.base.config.shared import ITSharedConfig, AutoCompConfig\n",
    "\n",
    "shared_cfg = ITSharedConfig(model_name_or_path='gpt2', task_name='rte', tokenizer_id_overrides={'pad_token_id': 50256},\n",
    "                  tokenizer_kwargs={'model_input_names': ['input'], 'padding_side': 'left', 'add_bos_token': True})\n",
    "datamodule_cfg = ITDataModuleConfig(prompt_cfg=RTEBoolqPromptConfig(), train_batch_size=2, eval_batch_size=2,\n",
    "                                    signature_columns=['input', 'labels'], prepare_data_map_cfg={\"batched\": True})\n",
    "genclassif_cfg = GenerativeClassificationConfig(enabled=True, lm_generation_cfg=TLensGenerationConfig(max_new_tokens=1))\n",
    "hf_cfg = HFFromPretrainedConfig(pretrained_kwargs={'torch_dtype': 'float32'}, model_head='transformers.GPT2LMHeadModel')\n",
    "tl_cfg = ITLensFromPretrainedNoProcessingConfig(model_name=\"gpt2-small\", default_padding_side='left')\n",
    "sae_cfgs = [SAELensFromPretrainedConfig(release=sae_fqn.release, sae_id=sae_fqn.sae_id) for sae_fqn\n",
    "            in tutorial_config.sae_analysis_targets.sae_fqns]\n",
    "auto_comp_cfg = AutoCompConfig(module_cfg_name='RTEBoolqConfig', module_cfg_mixin=RTEBoolqEntailmentMapping)\n",
    "module_cfg = ITConfig(auto_comp_cfg=auto_comp_cfg, generative_step_cfg=genclassif_cfg, hf_from_pretrained_cfg=hf_cfg,\n",
    "                      tl_cfg=tl_cfg, sae_cfgs=sae_cfgs)\n",
    "session_cfg = ITSessionConfig(adapter_ctx=(Adapter.core, Adapter.sae_lens),\n",
    "                              datamodule_cls=RTEBoolqDataModule, module_cls=RTEBoolqModule,\n",
    "                              shared_cfg=shared_cfg, datamodule_cfg=datamodule_cfg, module_cfg=module_cfg)\n",
    "it_session = ITSession(session_cfg)\n",
    "# TODO: maybe open a PR for the below\n",
    "# https://github.com/jbloomAus/SAELens/blob/aa8f42bf06d9c68bb890f4881af0aac916ecd17c/sae_lens/sae.py#L144-L151 warning\n",
    "# that inspects whether the loaded model has a default config override specified in ``pretrained_saes.yaml`` (e.g.\n",
    "# 'gpt2-small-res-jb', config_overrides: model_from_pretrained_kwargs: center_writing_weights: true) and if so, avoids\n",
    "# giving an arguably spurious warning to the user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Demo Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We of course could manually init IT components here and run the exploratory analysis ops one by one \n",
    "# (or our own custom analysis)\n",
    "# it_init(**it_session)\n",
    "# TODO: add more demo manual commands and likely a separate demo tutorial for manual analysis (AI model generated)\n",
    "#       after abstractions stabilize and are better refined/integrated\n",
    "\n",
    "\n",
    "run_config = AnalysisRunnerCfg(it_session=it_session, max_epochs=1, analysis_set_cfg=tutorial_config)\n",
    "runner = AnalysisRunner(run_cfg=run_config)\n",
    "sl_test_module = run_config.module\n",
    "analysis_results = runner.run_analysis_set()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Demo Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean vs SAE Sample-wise Logit Diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if {ANALYSIS_OPS['logit_diffs.base'], ANALYSIS_OPS['logit_diffs.sae']}.issubset(set(tutorial_config.analysis_ops)):\n",
    "    base_vs_sae_logit_diffs(sae=analysis_results[ANALYSIS_OPS['logit_diffs.sae']],\n",
    "                            base_ref=analysis_results[ANALYSIS_OPS['logit_diffs.base']],\n",
    "                            top_k=tutorial_config.top_k_clean_logit_diffs,\n",
    "                            tokenizer=sl_test_module.datamodule.tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proportion Correct Answers on Dataset By Analysis Op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: NEXT: debug reconstruction here.\n",
    "pred_summaries = {op: compute_correct(summ, op) for op, summ in analysis_results.items()}\n",
    "\n",
    "table_rows = []\n",
    "for op, (total_correct, percentage_correct, _) in pred_summaries.items():\n",
    "    table_rows.append([op, total_correct, f\"{percentage_correct:.2f}%\"])\n",
    "\n",
    "print(tabulate(table_rows, headers=[\"Op\", \"Total Correct\", \"Percentage Correct\"], tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Per Batch Ablation Effect Graphs [Optional]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tutorial_config.latent_effects_graphs and ANALYSIS_OPS['logit_diffs.attribution.ablation'] in tutorial_config.analysis_ops:\n",
    "    # TODO: add note that only latent effects associated with correct answers currently displayed\n",
    "    # TODO: allow toggling correct filtering during runs\n",
    "    analysis_results[ANALYSIS_OPS['logit_diffs.attribution.ablation']].plot_latent_effects(per_batch=tutorial_config.latent_effects_graphs_per_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Per-SAE Ablation Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if ANALYSIS_OPS['logit_diffs.attribution.ablation'] in tutorial_config.analysis_ops:\n",
    "    ablation_batch_preds = pred_summaries[ANALYSIS_OPS['logit_diffs.attribution.ablation']].batch_predictions\n",
    "    activation_summary = analysis_results[ANALYSIS_OPS['logit_diffs.sae']].calc_activation_summary()\n",
    "    ablation_metrics = analysis_results[ANALYSIS_OPS['logit_diffs.attribution.ablation']].calculate_latent_metrics(\n",
    "        pred_summ=pred_summaries[ANALYSIS_OPS['logit_diffs.attribution.ablation']],\n",
    "        activation_summary=activation_summary,\n",
    "        # filter_by_correct=True,\n",
    "        run_name=\"logit_diffs.attribution.ablation\"\n",
    "    )\n",
    "\n",
    "    tables = ablation_metrics.create_attribution_tables(top_k=tutorial_config.top_k_latents_table, filter_type='both',\n",
    "                                                        per_sae=tutorial_config.latents_table_per_sae)\n",
    "\n",
    "    for title, table in tables.items():\n",
    "        print(f\"\\n{title}\\n{table}\\n\")\n",
    "\n",
    "    sl_test_module.display_latent_dashboards(ablation_metrics, title=\"Ablation-Mediated Latent Analysis\",\n",
    "                              sae_release=tutorial_config.sae_analysis_targets.sae_release, \n",
    "                              top_k=tutorial_config.top_k_latent_dashboards)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Per-SAE Attribution Patching Effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ANALYSIS_OPS['logit_diffs.attribution.grad_based'] in tutorial_config.analysis_ops:\n",
    "    # per-SAE activation summaries are calculated using our AnalysisStore since the relevant keys are present,\n",
    "    # no need to provide a separate activation summary from another comparison cache in this case as with ablation\n",
    "    activation_summary = analysis_results[ANALYSIS_OPS['logit_diffs.attribution.grad_based']].calc_activation_summary()\n",
    "    attribution_patching_metrics = analysis_results[ANALYSIS_OPS['logit_diffs.attribution.grad_based']].calculate_latent_metrics(\n",
    "        pred_summ=pred_summaries[ANALYSIS_OPS['logit_diffs.attribution.grad_based']],\n",
    "        run_name=\"logit_diffs.attribution.grad_based\"\n",
    "    )\n",
    "\n",
    "    tables = attribution_patching_metrics.create_attribution_tables(top_k=tutorial_config.top_k_latents_table,\n",
    "                                                                    filter_type='both',\n",
    "                                                                    per_sae=tutorial_config.latents_table_per_sae)\n",
    "\n",
    "    for title, table in tables.items():\n",
    "        print(f\"\\n{title}\\n{table}\\n\")\n",
    "\n",
    "    sl_test_module.display_latent_dashboards(attribution_patching_metrics, \n",
    "                                             title=\"Attribution Patching-Mediated Latent Analysis\",\n",
    "                                             sae_release=tutorial_config.sae_analysis_targets.sae_release, \n",
    "                                             top_k=tutorial_config.top_k_latent_dashboards)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Per-SAE Ablation vs Attribution-Patching Effect Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if {ANALYSIS_OPS['logit_diffs.attribution.grad_based'], ANALYSIS_OPS['logit_diffs.attribution.ablation']}.issubset(tutorial_config.analysis_ops):\n",
    "    # Visualize results for each hook\n",
    "    # Call the function with our metrics\n",
    "    latent_metrics_scatter(ablation_metrics, attribution_patching_metrics, label1=\"Ablation\", label2=\"Attribution Patching\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: continue implementation here\n",
    "# NEXT: implement DFA using activationstore on rte val dataset and plot for top latents \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "it_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
