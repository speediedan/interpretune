{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretune SAELens Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fine-Tuning Scheduler logo](logo_fts.png){height=\"55px\" width=\"401px\"}\n",
    "\n",
    "### Intro\n",
    "\n",
    "[Interpretune](https://github.com/speediedan/interpretune) is a flexible framework for exploring, analyzing and tuning llm world models. In this tutorial, we'll walk through a simple example of using Interpretune to pursue interpretability research with SAELens. As we'll see, Interpretune handles the required execution context composition, allowing us to use the same code in a variety of contexts, depending upon the level of abstraction required.\n",
    "\n",
    "As a long-time PyTorch and PyTorch Lightning contributor, I've found the PyTorch Lightning framework is the right level of abstraction for a large variety of ML research contexts, but some contexts benefit from using core PyTorch directly. Additionally, some users may prefer to use the core PyTorch framework directly for a wide variety of reasons including maximizing portability. As will be demonstrated here, Interpretune maximizes flexibility and portability by adhering to a well-defined protocol that allows auto-composition of our research module with the adapters required for execution in a wide variety of contexts. In this example, we'll be executing the same module with core PyTorch and PyTorch Lightning, demonstrating the use of `SAELens` w/ Interpretune for interpretability research.\n",
    "\n",
    "> Note - **this is a WIP**, but this is the core idea. If you have any feedback, please let me know!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note on memory usage\n",
    "\n",
    "In these exercises, we'll be loading some pretty large models into memory (e.g. Gemma 2-2B and its SAEs, as well as a host of other models in later sections of the material). It's useful to have functions which can help profile memory usage for you, so that if you encounter OOM errors you can try and clear out unnecessary models. For example, we've found that with the right memory handling (i.e. deleting models and objects when you're not using them any more) it should be possible to run all the exercises in this material on a Colab Pro notebook, and all the exercises minus the handful involving Gemma on a free Colab notebook.\n",
    "\n",
    "<details>\n",
    "<summary>See this dropdown for some functions which you might find helpful, and how to use them.</summary>\n",
    "\n",
    "First, we can run some code to inspect our current memory usage. Here's me running this code during the exercise set on SAE circuits, after having already loaded in the Gemma models from the previous section. This was on a Colab Pro notebook.\n",
    "\n",
    "```python\n",
    "# Profile memory usage, and delete gemma models if we've loaded them in\n",
    "namespace = globals().copy() | locals()\n",
    "part32_utils.profile_pytorch_memory(namespace=namespace, filter_device=\"cuda:0\")\n",
    "```\n",
    "\n",
    "<pre style=\"font-family: Consolas; font-size: 14px\">Allocated = 35.88 GB\n",
    "Total = 39.56 GB\n",
    "Free = 3.68 GB\n",
    "┌──────────────────────┬────────────────────────┬──────────┬─────────────┐\n",
    "│ Name                 │ Object                 │ Device   │   Size (GB) │\n",
    "├──────────────────────┼────────────────────────┼──────────┼─────────────┤\n",
    "│ gemma_2_2b           │ HookedSAETransformer   │ cuda:0   │       11.94 │\n",
    "│ gpt2                 │ HookedSAETransformer   │ cuda:0   │        0.61 │\n",
    "│ gemma_2_2b_sae       │ SAE                    │ cuda:0   │        0.28 │\n",
    "│ sae_resid_dirs       │ Tensor (4, 24576, 768) │ cuda:0   │        0.28 │\n",
    "│ gpt2_sae             │ SAE                    │ cuda:0   │        0.14 │\n",
    "│ logits               │ Tensor (4, 15, 50257)  │ cuda:0   │        0.01 │\n",
    "│ logits_with_ablation │ Tensor (4, 15, 50257)  │ cuda:0   │        0.01 │\n",
    "│ clean_logits         │ Tensor (4, 15, 50257)  │ cuda:0   │        0.01 │\n",
    "│ _                    │ Tensor (16, 128, 768)  │ cuda:0   │        0.01 │\n",
    "│ clean_sae_acts_post  │ Tensor (4, 15, 24576)  │ cuda:0   │        0.01 │\n",
    "└──────────────────────┴────────────────────────┴──────────┴─────────────┘</pre>\n",
    "\n",
    "From this, we see that we've allocated a lot of memory for the the Gemma model, so let's delete it. We'll also run some code to move any remaining objects on the GPU which are larger than 100MB to the CPU, and print the memory status again.\n",
    "\n",
    "```python\n",
    "del gemma_2_2b\n",
    "del gemma_2_2b_sae\n",
    "\n",
    "THRESHOLD = 0.1  # GB\n",
    "for obj in gc.get_objects():\n",
    "    try:\n",
    "        if isinstance(obj, torch.nn.Module) and part32_utils.get_tensors_size(obj) / 1024**3 > THRESHOLD:\n",
    "            if hasattr(obj, \"cuda\"):\n",
    "                obj.cpu()\n",
    "            if hasattr(obj, \"reset\"):\n",
    "                obj.reset()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Move our gpt2 model & SAEs back to GPU (we'll need them for the exercises we're about to do)\n",
    "gpt2.to(device)\n",
    "gpt2_saes = {layer: sae.to(device) for layer, sae in gpt2_saes.items()}\n",
    "\n",
    "part32_utils.print_memory_status()\n",
    "```\n",
    "\n",
    "<pre style=\"font-family: Consolas; font-size: 14px\">Allocated = 14.90 GB\n",
    "Reserved = 39.56 GB\n",
    "Free = 24.66</pre>\n",
    "\n",
    "Mission success! We've managed to free up a lot of memory. Note that the code which moves all objects collected by the garbage collector to the CPU is often necessary to free up the memory. We can't just delete the objects directly because PyTorch can still sometimes keep references to them (i.e. their tensors) in memory. In fact, if you add code to the for loop above to print out `obj.shape` when `obj` is a tensor, you'll see that a lot of those tensors are actually Gemma model weights, even once you've deleted `gemma_2_2b`.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Any, Dict, Optional, Tuple, List, Callable, Literal\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "\n",
    "import evaluate\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import PreTrainedTokenizerBase\n",
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "from datasets.arrow_dataset import LazyDict\n",
    "from tabulate import tabulate\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import ActivationCache\n",
    "\n",
    "from interpretune.adapters.core import ITModule\n",
    "from interpretune.base.call import it_init\n",
    "from interpretune.base.config.datamodule import PromptConfig, ITDataModuleConfig\n",
    "from interpretune.base.config.module import ITConfig\n",
    "from interpretune.base.config.mixins import GenerativeClassificationConfig\n",
    "from interpretune.base.components.mixins import ProfilerHooksMixin\n",
    "from interpretune.base.datamodules import ITDataModule\n",
    "from interpretune.utils.logging import rank_zero_warn\n",
    "from interpretune.utils.types import STEP_OUTPUT\n",
    "from interpretune.utils.tokenization import _sanitize_input_name\n",
    "from interpretune.adapters.sae_lens import SAELensFromPretrainedConfig\n",
    "from interpretune.base.config.shared import Adapter\n",
    "from interpretune.base.contract.session import ITSessionConfig, ITSession\n",
    "from it_examples import _ACTIVE_PATCHES # noqa: F401  # TODO: add note about this unless patched in SL before release\n",
    "from interpretune.utils.analysis import (SaveCfg, AnalysisCache, boolean_logits_to_avg_logit_diff, batch_alive_latents,\n",
    "                                         ablate_sae_latent, resolve_answer_indices, DEFAULT_DECODE_KWARGS,\n",
    "                                         display_dashboard)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Our IT Data Module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = logging.getLogger(__name__)\n",
    "\n",
    "TASK_TEXT_FIELD_MAP = {\"rte\": (\"premise\", \"hypothesis\"), \"boolq\": (\"passage\", \"question\")}\n",
    "TASK_NUM_LABELS = {\"boolq\": 2, \"rte\": 2}\n",
    "DEFAULT_TASK = \"rte\"\n",
    "INVALID_TASK_MSG = f\" is an invalid task_name. Proceeding with the default task: {DEFAULT_TASK!r}\"\n",
    "\n",
    "class RTEBoolqDataModule(ITDataModule):\n",
    "    def __init__(self, itdm_cfg: ITDataModuleConfig) -> None:\n",
    "        if itdm_cfg.task_name not in TASK_NUM_LABELS.keys():\n",
    "            rank_zero_warn(itdm_cfg.task_name + INVALID_TASK_MSG)\n",
    "            itdm_cfg.task_name = DEFAULT_TASK\n",
    "        itdm_cfg.text_fields = TASK_TEXT_FIELD_MAP[itdm_cfg.task_name]\n",
    "        super().__init__(itdm_cfg=itdm_cfg)\n",
    "\n",
    "    def prepare_data(self, target_model: Optional[torch.nn.Module] = None) -> None:\n",
    "        \"\"\"Load the SuperGLUE dataset.\"\"\"\n",
    "        # N.B. prepare_data is called in a single process (rank 0, either per node or globally) so do not use it to\n",
    "        # assign state (e.g. self.x=y)\n",
    "        # note for raw pytorch we require a target_model\n",
    "        # NOTE [HF Datasets Transformation Caching]:\n",
    "        # HF Datasets' transformation cache fingerprinting algo necessitates construction of these partials as the hash\n",
    "        # is generated using function args, dataset file, mapping args: https://bit.ly/HF_Datasets_fingerprint_algo)\n",
    "        tokenization_func = partial(\n",
    "            self.encode_for_rteboolq,\n",
    "            tokenizer=self.tokenizer,\n",
    "            text_fields=self.itdm_cfg.text_fields,\n",
    "            prompt_cfg=self.itdm_cfg.prompt_cfg,\n",
    "            template_fn=self.itdm_cfg.prompt_cfg.model_chat_template_fn,\n",
    "            tokenization_pattern=self.itdm_cfg.cust_tokenization_pattern,\n",
    "        )\n",
    "        dataset = datasets.load_dataset(\"super_glue\", self.itdm_cfg.task_name, trust_remote_code=True)\n",
    "        for split in dataset.keys():\n",
    "            dataset[split] = dataset[split].map(tokenization_func, **self.itdm_cfg.prepare_data_map_cfg)\n",
    "            dataset[split] = self._remove_unused_columns(dataset[split], target_model)\n",
    "        dataset.save_to_disk(self.itdm_cfg.dataset_path)\n",
    "\n",
    "    def dataloader_factory(self, split: str, use_train_batch_size: bool = False) -> DataLoader:\n",
    "        dataloader_kwargs = {\"dataset\": self.dataset[split], \"collate_fn\":self.data_collator,\n",
    "                             **self.itdm_cfg.dataloader_kwargs}\n",
    "        dataloader_kwargs['batch_size'] = self.itdm_cfg.train_batch_size if use_train_batch_size else \\\n",
    "            self.itdm_cfg.eval_batch_size\n",
    "        return DataLoader(**dataloader_kwargs)\n",
    "\n",
    "    # TODO: change to partialmethod's?\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return self.dataloader_factory(split='train', use_train_batch_size=True)\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return self.dataloader_factory(split='validation')\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return self.dataloader_factory(split='validation')\n",
    "\n",
    "    def predict_dataloader(self) -> DataLoader:\n",
    "        return self.dataloader_factory(split='validation')\n",
    "\n",
    "    #TODO: relax PreTrainedTokenizerBase to the protocol that is actually required\n",
    "    @staticmethod\n",
    "    def encode_for_rteboolq(example_batch: LazyDict, tokenizer: PreTrainedTokenizerBase, text_fields: List[str],\n",
    "                            prompt_cfg: PromptConfig, template_fn: Callable,\n",
    "                            tokenization_pattern: Optional[str] = None) -> BatchEncoding:\n",
    "        example_batch['sequences'] = []\n",
    "        # TODO: use promptsource instead of this manual approach after tinkering\n",
    "        for field1, field2 in zip(example_batch[text_fields[0]],\n",
    "                                  example_batch[text_fields[1]]):\n",
    "            if prompt_cfg.cust_task_prompt:\n",
    "                task_prompt = (prompt_cfg.cust_task_prompt['context'] + \"\\n\" +\n",
    "                               field1 + \"\\n\" +\n",
    "                               prompt_cfg.cust_task_prompt['question'] + \"\\n\" +\n",
    "                               field2)\n",
    "            else:\n",
    "                task_prompt = (field1 + prompt_cfg.ctx_question_join + field2 \\\n",
    "                               + prompt_cfg.question_suffix)\n",
    "            sequence = template_fn(task_prompt=task_prompt, tokenization_pattern=tokenization_pattern)\n",
    "            example_batch['sequences'].append(sequence)\n",
    "        features = tokenizer.batch_encode_plus(example_batch[\"sequences\"], padding=\"longest\",\n",
    "                                               padding_side=tokenizer.padding_side)\n",
    "        features[\"labels\"] = example_batch[\"label\"]  # Rename label to labels, easier to pass to model forward\n",
    "        features = _sanitize_input_name(tokenizer.model_input_names, features)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Our IT Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass(kw_only=True)\n",
    "class RTEBoolqEntailmentMapping:\n",
    "    entailment_mapping: Tuple = (\"Yes\", \"No\")  # RTE style, invert mapping for BoolQ\n",
    "    entailment_mapping_indices: Optional[torch.Tensor] = None\n",
    "\n",
    "\n",
    "@dataclass(kw_only=True)\n",
    "class RTEBoolqPromptConfig(PromptConfig):\n",
    "    ctx_question_join: str = 'Does the previous passage imply that '\n",
    "    question_suffix: str = '? Answer with only one word, either Yes or No.'\n",
    "    cust_task_prompt: Optional[Dict[str, Any]] = None\n",
    "\n",
    "\n",
    "class RTEBoolqModule(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # when using TransformerLens, we need to manually calculate our loss from logit output\n",
    "        self.loss_fn = CrossEntropyLoss()\n",
    "\n",
    "    def setup(self, *args, **kwargs) -> None:\n",
    "        super().setup(*args, **kwargs)\n",
    "        self._init_entailment_mapping()\n",
    "\n",
    "    def _before_it_cfg_init(self, it_cfg: ITConfig) -> ITConfig:\n",
    "        if it_cfg.task_name not in TASK_NUM_LABELS.keys():\n",
    "            rank_zero_warn(it_cfg.task_name + INVALID_TASK_MSG)\n",
    "            it_cfg.task_name = DEFAULT_TASK\n",
    "        it_cfg.num_labels = 0 if it_cfg.generative_step_cfg.enabled else TASK_NUM_LABELS[it_cfg.task_name]\n",
    "        return it_cfg\n",
    "\n",
    "    def load_metric(self) -> None:\n",
    "        self.metric = evaluate.load(\"super_glue\", self.it_cfg.task_name,\n",
    "                                    experiment_id=self._it_state._init_hparams['experiment_id'])\n",
    "\n",
    "    def _init_entailment_mapping(self) -> None:\n",
    "        ent_cfg, tokenizer = self.it_cfg, self.datamodule.tokenizer\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(ent_cfg.entailment_mapping)\n",
    "        device = self.device if isinstance(self.device, torch.device) else self.output_device\n",
    "        ent_cfg.entailment_mapping_indices = torch.tensor(token_ids).to(device)\n",
    "\n",
    "    def labels_to_ids(self, labels: List[str]) -> List[int]:\n",
    "        return torch.take(self.it_cfg.entailment_mapping_indices, labels), labels\n",
    "\n",
    "    @ProfilerHooksMixin.memprofilable\n",
    "    def training_step(self, batch: BatchEncoding, batch_idx: int) -> STEP_OUTPUT:\n",
    "        # TODO: need to be explicit about the compatibility constraints/contract\n",
    "        # TODO: note that this example uses generative_step_cfg and lm_head except for the test_step where we demo how\n",
    "        # to use the GenerativeStepMixin to run inference with or without a generative_step_cfg enabled as well as with\n",
    "        # different heads (e.g., seqclassification or LM head in this case)\n",
    "        answer_logits, labels, *_ = self.logits_and_labels(batch, batch_idx)\n",
    "        loss = self.loss_fn(answer_logits, labels)\n",
    "        self.log(\"train_loss\", loss, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    @ProfilerHooksMixin.memprofilable\n",
    "    def validation_step(self, batch: BatchEncoding, batch_idx: int, dataloader_idx: int = 0) -> Optional[STEP_OUTPUT]:\n",
    "        answer_logits, labels, orig_labels, cache = self.logits_and_labels(batch, batch_idx)\n",
    "        val_loss = self.loss_fn(answer_logits, labels)\n",
    "        self.log(\"val_loss\", val_loss, prog_bar=True, sync_dist=True)\n",
    "        self.collect_answers(answer_logits, orig_labels)\n",
    "\n",
    "    @ProfilerHooksMixin.memprofilable\n",
    "    def test_step(self, batch: BatchEncoding, batch_idx: int, dataloader_idx: int = 0) -> Optional[STEP_OUTPUT]:\n",
    "        if self.it_cfg.generative_step_cfg.enabled:\n",
    "            self.generative_classification_test_step(batch, batch_idx, dataloader_idx=dataloader_idx)\n",
    "        else:\n",
    "            self.default_test_step(batch, batch_idx, dataloader_idx=dataloader_idx)\n",
    "\n",
    "    def generative_classification_test_step(self, batch: BatchEncoding, batch_idx: int, dataloader_idx: int = 0) -> \\\n",
    "        Optional[STEP_OUTPUT]:\n",
    "        labels = batch.pop(\"labels\")\n",
    "        outputs = self.it_generate(batch, **self.it_cfg.generative_step_cfg.lm_generation_cfg.generate_kwargs)\n",
    "        self.collect_answers(outputs.logits, labels)\n",
    "\n",
    "    def default_test_step(self, batch: BatchEncoding, batch_idx: int, dataloader_idx: int = 0) -> Optional[STEP_OUTPUT]:\n",
    "        labels = batch.pop(\"labels\")\n",
    "        outputs = self(**batch)\n",
    "        self.collect_answers(outputs.logits, labels)\n",
    "\n",
    "    def predict_step(self, batch: BatchEncoding, batch_idx: int, dataloader_idx: int = 0) -> Optional[STEP_OUTPUT]:\n",
    "        labels = batch.pop(\"labels\")\n",
    "        outputs = self(**batch)\n",
    "        return self.collect_answers(outputs, labels, mode='return')\n",
    "\n",
    "    def collect_answers(self, logits: torch.Tensor | tuple, labels: torch.Tensor, mode: str = 'log') -> Optional[Dict]:\n",
    "        logits = self.standardize_logits(logits)\n",
    "        per_example_answers, _ = torch.max(logits, dim=-2)\n",
    "        preds = torch.argmax(per_example_answers, axis=-1)  # type: ignore[call-arg]\n",
    "        metric_dict = self.metric.compute(predictions=preds, references=labels)\n",
    "        # TODO: check if this type casting is still required for lightning torchmetrics, bug should be fixed now...\n",
    "        metric_dict = dict(map(lambda x: (x[0], torch.tensor(x[1], device=self.device)\n",
    "                                          .to(torch.float32)),\n",
    "                               metric_dict.items()))\n",
    "        if mode == 'log':\n",
    "            self.log_dict(metric_dict, prog_bar=True, sync_dist=True)\n",
    "        else:\n",
    "            return metric_dict\n",
    "\n",
    "    def standardize_logits(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        # to support generative classification/non-generative classification configs and LM/SeqClassification heads we\n",
    "        # adhere to the following logits logical shape invariant:\n",
    "        # [batch size, positions to consider, answers to consider]\n",
    "        if isinstance(logits, tuple):\n",
    "            logits = torch.stack([out for out in logits], dim=1)\n",
    "        logits = logits.to(device=self.device)\n",
    "        if logits.ndim == 2:  # if answer logits have already been squeezed\n",
    "            logits = logits.unsqueeze(1)\n",
    "        if logits.shape[-1] != self.it_cfg.num_labels:\n",
    "            logits = torch.index_select(logits, -1, self.it_cfg.entailment_mapping_indices)\n",
    "            if not self.it_cfg.generative_step_cfg.enabled:\n",
    "                logits = logits[:, -1:, :]\n",
    "        return logits\n",
    "\n",
    "    def per_latent_answer_logits(self, batch: BatchEncoding, batch_idx: int, fwd_hooks_cfg: Dict) -> dict:\n",
    "        assert fwd_hooks_cfg is not None\n",
    "        per_latent_answer_logits = {}\n",
    "        for latent_idx in tqdm(fwd_hooks_cfg['alive_latents'][batch_idx]):\n",
    "            batch_answer_idxs = fwd_hooks_cfg['answer_indices'][batch_idx]\n",
    "            answer_logits = self.model.run_with_hooks_with_saes(\n",
    "                **batch,\n",
    "                saes=self.sae_handles,\n",
    "                clear_contexts=True,\n",
    "                fwd_hooks=[\n",
    "                    (fwd_hooks_cfg['hook_names'],\n",
    "                        partial(fwd_hooks_cfg['hook_fn'], latent_idx=latent_idx, seq_pos=batch_answer_idxs)\n",
    "                    )\n",
    "                ],\n",
    "            )\n",
    "            per_latent_answer_logits[latent_idx] = answer_logits\n",
    "        return per_latent_answer_logits, None\n",
    "\n",
    "    def attr_patch_logits(self, batch: BatchEncoding, batch_idx: int, logit_diff_fn: Callable, hooks_cfg: dict,\n",
    "                          label_ids: torch.Tensor, orig_labels: torch.Tensor, answer_indices: torch.Tensor) \\\n",
    "                            -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, Any]:\n",
    "        assert hooks_cfg is not None\n",
    "        with self.model.saes(saes=self.sae_handles):\n",
    "            # We add hooks to cache values from the forward and backward pass respectively\n",
    "            with self.model.hooks(\n",
    "                fwd_hooks=hooks_cfg['fwd_hooks'],\n",
    "                bwd_hooks=hooks_cfg['bwd_hooks'],\n",
    "            ):\n",
    "                # Forward pass fills the fwd cache, then backward pass fills the bwd cache\n",
    "                answer_logits = self.model(**batch)\n",
    "                answer_logits = answer_logits[torch.arange(batch['input'].size(0)), answer_indices]\n",
    "                answer_logits = self.standardize_logits(answer_logits)\n",
    "                per_example_answers, _ = torch.max(answer_logits, dim=-2)\n",
    "                preds = torch.argmax(per_example_answers, axis=-1)  # type: ignore[call-arg]\n",
    "                logit_diffs = logit_diff_fn(logits=answer_logits, target_indices=orig_labels)\n",
    "                #loss = self.loss_fn(answer_logits, labels)\n",
    "                #answer_logits = self.standardize_logits(answer_logits)\n",
    "                #per_example_answers, _ = torch.max(answer_logits, dim=-2)\n",
    "                #preds = torch.argmax(per_example_answers, axis=-1)  # type: ignore[call-arg]\n",
    "                logit_diffs.sum().backward()\n",
    "                if logit_diffs.dim() == 0:\n",
    "                    logit_diffs.unsqueeze_(0)\n",
    "                # TODO: return only answer_indices and alive_latent cache indices for each cache\n",
    "        return (answer_logits, logit_diffs, preds, label_ids, orig_labels,\n",
    "                ActivationCache(hooks_cfg[\"cache_dict\"][\"fwd\"], self.model),\n",
    "                ActivationCache(hooks_cfg[\"cache_dict\"][\"bwd\"], self.model),)\n",
    "\n",
    "    # TODO: add this helper function to SL adapter?\n",
    "    def run_with_ctx(self, mode: str, batch: BatchEncoding, batch_idx: int, hook_names: Optional[str]= None, **kwargs):\n",
    "        if mode == 'clean':\n",
    "            return self(**batch), None\n",
    "        elif mode == 'cache_with_saes':\n",
    "            return self.model.run_with_cache_with_saes(**batch, saes=self.sae_handles, names_filter=hook_names)\n",
    "        else:\n",
    "            return self.model.run_with_saes(**batch, saes=self.sae_handles)\n",
    "\n",
    "    def ablation_logits_with_labels(self, batch: BatchEncoding, batch_idx: int, fwd_hooks_cfg: Dict) \\\n",
    "            -> tuple[dict[Any, torch.Tensor], torch.Tensor, torch.Tensor, Any]:\n",
    "        label_ids, labels = self.labels_to_ids(batch.pop(\"labels\"))\n",
    "        batch_sz = batch['input'].size(0)\n",
    "        answer_indices = fwd_hooks_cfg['answer_indices'][batch_idx]\n",
    "        per_latent_logits, cache = self.per_latent_answer_logits(batch, batch_idx, fwd_hooks_cfg=fwd_hooks_cfg)\n",
    "        per_latent_logits = {k: v[torch.arange(batch_sz), answer_indices, :] for k, v in per_latent_logits.items()}\n",
    "        return per_latent_logits, label_ids, labels, cache\n",
    "\n",
    "    def logits_and_labels(self, batch: BatchEncoding, batch_idx: int, run_ctx: str,\n",
    "                          hook_names: Optional[str] = None) -> torch.Tensor:\n",
    "        label_ids, labels = self.labels_to_ids(batch.pop(\"labels\"))\n",
    "        cache = None\n",
    "        logits, cache = self.run_with_ctx(run_ctx, batch, batch_idx, hook_names=hook_names)\n",
    "        # TODO: add another layer of abstraction here to handle different model output types? Tradeoffs to consider...\n",
    "        if not isinstance(logits, torch.Tensor):\n",
    "            logits = logits.logits\n",
    "            assert isinstance(logits, torch.Tensor), f\"Expected logits to be a torch.Tensor but got {type(logits)}\"\n",
    "        return torch.squeeze(logits[:, -1, :], dim=1), label_ids, labels, cache\n",
    "\n",
    "    def ablation_test_step(self, batch: BatchEncoding, batch_idx: int, analysis_cache: AnalysisCache,\n",
    "                           logit_diff_fn: Callable, fwd_hooks_cfg: Optional[Dict] = None,\n",
    "                           dataloader_idx: int = 0, *args, **kwargs) -> \\\n",
    "        Optional[STEP_OUTPUT]:\n",
    "        per_latent_answer_logits, labels, orig_labels, cache = self.ablation_logits_with_labels(\n",
    "            batch, batch_idx, fwd_hooks_cfg=fwd_hooks_cfg)\n",
    "        per_latent_loss, per_latent_logit_diffs, per_latent_preds = {}, {}, {}\n",
    "        ablation_effects = torch.zeros(batch['input'].size(0), self.sae_handles[0].cfg.d_sae)\n",
    "        # TODO: return per-latent cache, preds, logit_diffs and loss or just ablation_effects?\n",
    "        for latent_idx, logits in per_latent_answer_logits.items():\n",
    "            per_latent_loss[latent_idx] = self.loss_fn(logits, labels)\n",
    "            logits = self.standardize_logits(logits)\n",
    "            per_example_answers, _ = torch.max(logits, dim=-2)\n",
    "            per_latent_preds[latent_idx] = torch.argmax(per_example_answers, axis=-1)\n",
    "            logit_diffs = logit_diff_fn(logits, target_indices=orig_labels, reduction=None, keep_as_tensor=True)\n",
    "            example_mask = (logit_diffs > 0).cpu()\n",
    "            per_latent_logit_diffs[latent_idx] = logit_diffs[example_mask].detach().cpu()\n",
    "            # for the edge case where the mask/saved batch tensors are scalars (usually due to a batch size of 1)\n",
    "            for t in [example_mask, fwd_hooks_cfg['base_logit_diffs'][batch_idx]]:\n",
    "                if t.dim() == 0:\n",
    "                    t.unsqueeze_(0)\n",
    "            ablation_effects[example_mask, latent_idx] = (\n",
    "                fwd_hooks_cfg['base_logit_diffs'][batch_idx][example_mask] - per_latent_logit_diffs[latent_idx]\n",
    "            )\n",
    "        step_summ = {\"loss\": per_latent_loss, \"logit_diffs\": per_latent_logit_diffs, \"labels\": labels,\n",
    "                    \"orig_labels\": orig_labels, \"preds\": per_latent_preds, \"ablation_effects\": ablation_effects}\n",
    "        analysis_cache.save(step_summ, batch, cache, tokenizer=self.datamodule.tokenizer)\n",
    "\n",
    "    def attribution_patch_test_step(self, batch: BatchEncoding, batch_idx: int, analysis_cache: AnalysisCache,\n",
    "                                    hooks_cfg: Dict, logit_diff_fn: Callable,\n",
    "                                    dataloader_idx: int = 0, *args, **kwargs) -> Optional[STEP_OUTPUT]:\n",
    "        label_ids, orig_labels = self.labels_to_ids(batch.pop(\"labels\"))\n",
    "        answer_indices = resolve_answer_indices(batch['input'].detach().cpu()) if \\\n",
    "            self.datamodule.tokenizer.padding_side == 'right' else torch.full((batch['input'].size(0),), -1)\n",
    "        answer_logits, logit_diffs, preds, labels, orig_labels, cache, grad_cache = (\n",
    "            self.attr_patch_logits(\n",
    "                batch, batch_idx, logit_diff_fn, hooks_cfg, label_ids, orig_labels, answer_indices\n",
    "            )\n",
    "        )\n",
    "        attribution_values = torch.zeros(batch['input'].size(0), self.sae_handles[0].cfg.d_sae)\n",
    "        # TODO: consider allowing passing these values from previous sae_cache run if provided to avoid recomputing\n",
    "        alive_latents = batch_alive_latents(answer_indices, cache, hooks_cfg['hook_names'])\n",
    "        batch_sae_acts_post = cache[hooks_cfg['hook_names']][torch.arange(batch['input'].size(0)), answer_indices]\n",
    "        batch_grad_sae_acts_post = grad_cache[hooks_cfg['hook_names']][torch.arange(batch['input'].size(0)),\n",
    "                                                                       answer_indices]\n",
    "        for t in [batch_sae_acts_post, batch_grad_sae_acts_post]:\n",
    "            if t.dim() == 2:\n",
    "                t.unsqueeze_(1)\n",
    "        correct_activations = torch.squeeze(batch_sae_acts_post[(logit_diffs > 0), :, :], dim=1)\n",
    "        # # Compute attribution values for all latents, then index to get live ones\n",
    "        attribution_values[:, alive_latents] = torch.squeeze(\n",
    "            (batch_grad_sae_acts_post[:, :, alive_latents] * batch_sae_acts_post[:, :, alive_latents]).cpu(), dim=1\n",
    "        )\n",
    "        step_summ = {\n",
    "            \"attribution_values\": attribution_values, \"labels\": labels, \"orig_labels\": orig_labels, #\"loss\": loss,\n",
    "            \"correct_activations\": correct_activations, \"alive_latents\": alive_latents,\n",
    "            \"answer_logits\": answer_logits, \"logit_diffs\": logit_diffs, \"preds\": preds}\n",
    "        analysis_cache.save(step_summ, batch, cache, grad_cache, tokenizer=self.datamodule.tokenizer)\n",
    "\n",
    "    def activation_cache_test_step(self, batch: BatchEncoding, batch_idx: int, analysis_cache: AnalysisCache,\n",
    "                                   logit_diff_fn: Callable,\n",
    "                                   hook_names: str, run_ctx: str = 'clean',\n",
    "                                   dataloader_idx: int = 0) -> \\\n",
    "        Optional[STEP_OUTPUT]:\n",
    "        answer_logits, labels, orig_labels, cache = self.logits_and_labels(batch, batch_idx, run_ctx=run_ctx,\n",
    "                                                                           hook_names=hook_names)\n",
    "        loss = self.loss_fn(answer_logits, labels)\n",
    "        answer_logits = self.standardize_logits(answer_logits)\n",
    "        per_example_answers, _ = torch.max(answer_logits, dim=-2)\n",
    "        preds = torch.argmax(per_example_answers, axis=-1)  # type: ignore[call-arg]\n",
    "        logit_diffs = logit_diff_fn(answer_logits, target_indices=orig_labels, reduction=None, keep_as_tensor=True)\n",
    "        for t in [logit_diffs]:\n",
    "            if t.dim() == 0:\n",
    "                t.unsqueeze_(0)\n",
    "        step_summ = {\"loss\": loss, \"logit_diffs\": logit_diffs, \"labels\": labels, \"orig_labels\": orig_labels,\n",
    "                    \"preds\": preds, \"answer_logits\": answer_logits}\n",
    "        if run_ctx == 'cache_with_saes':\n",
    "            answer_indices = resolve_answer_indices(batch['input'].detach().cpu()) if \\\n",
    "                self.datamodule.tokenizer.padding_side == 'right' else torch.full((batch['input'].size(0),), -1)\n",
    "            alive_latents = batch_alive_latents(answer_indices, cache, hook_names)\n",
    "            correct_activations = cache[hook_names][(logit_diffs > 0), -1, :]\n",
    "            step_summ.update({\"answer_indices\": answer_indices, \"alive_latents\": alive_latents,\n",
    "                             \"correct_activations\": correct_activations})\n",
    "        analysis_cache.save(step_summ, batch, cache, tokenizer=self.datamodule.tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure our IT Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpretune.adapters.transformer_lens import TLensGenerationConfig\n",
    "from interpretune.base.config.mixins import HFFromPretrainedConfig\n",
    "from interpretune.adapters.transformer_lens import ITLensFromPretrainedNoProcessingConfig\n",
    "from interpretune.base.config.shared import ITSharedConfig, AutoCompConfig\n",
    "\n",
    "\n",
    "shared_cfg = ITSharedConfig(model_name_or_path='gpt2', task_name='rte', tokenizer_id_overrides={'pad_token_id': 50256},\n",
    "                  tokenizer_kwargs={'model_input_names': ['input'], 'padding_side': 'left', 'add_bos_token': True})\n",
    "datamodule_cfg = ITDataModuleConfig(prompt_cfg=RTEBoolqPromptConfig(), train_batch_size=2, eval_batch_size=2,\n",
    "                                    signature_columns=['input', 'labels'], prepare_data_map_cfg={\"batched\": True})\n",
    "genclassif_cfg = GenerativeClassificationConfig(enabled=True, lm_generation_cfg=TLensGenerationConfig(max_new_tokens=1))\n",
    "hf_cfg = HFFromPretrainedConfig(pretrained_kwargs={'torch_dtype': 'float32'}, model_head='transformers.GPT2LMHeadModel')\n",
    "tl_cfg = ITLensFromPretrainedNoProcessingConfig(model_name=\"gpt2-small\", default_padding_side='left')\n",
    "# testing using attention SAEs release here instead of pre resid small res\n",
    "# sae_cfgs = [\n",
    "#     SAELensFromPretrainedConfig(release=\"gpt2-small-res-jb\", sae_id=\"blocks.9.hook_resid_pre\"),\n",
    "#     SAELensFromPretrainedConfig(release=\"gpt2-small-res-jb\", sae_id=\"blocks.10.hook_resid_pre\"),\n",
    "# ]\n",
    "sae_cfgs = [\n",
    "    SAELensFromPretrainedConfig(release=\"gpt2-small-hook-z-kk\", sae_id=\"blocks.9.hook_z\"),\n",
    "    SAELensFromPretrainedConfig(release=\"gpt2-small-hook-z-kk\", sae_id=\"blocks.10.hook_z\"),\n",
    "]\n",
    "auto_comp_cfg = AutoCompConfig(module_cfg_name='RTEBoolqConfig', module_cfg_mixin=RTEBoolqEntailmentMapping)\n",
    "module_cfg = ITConfig(auto_comp_cfg=auto_comp_cfg, generative_step_cfg=genclassif_cfg, hf_from_pretrained_cfg=hf_cfg,\n",
    "                      tl_cfg=tl_cfg, sae_cfgs=sae_cfgs)\n",
    "\n",
    "session_cfg = ITSessionConfig(adapter_ctx=(Adapter.core, Adapter.sae_lens),\n",
    "                              datamodule_cls=RTEBoolqDataModule, module_cls=RTEBoolqModule,\n",
    "                              shared_cfg=shared_cfg, datamodule_cfg=datamodule_cfg, module_cfg=module_cfg)\n",
    "it_session = ITSession(session_cfg)\n",
    "# TODO: maybe open a PR for the below\n",
    "# https://github.com/jbloomAus/SAELens/blob/aa8f42bf06d9c68bb890f4881af0aac916ecd17c/sae_lens/sae.py#L144-L151 warning\n",
    "# that inspects whether the loaded model has a default config override specified in ``pretrained_saes.yaml`` (e.g.\n",
    "# 'gpt2-small-res-jb', config_overrides: model_from_pretrained_kwargs: center_writing_weights: true) and if so, avoids\n",
    "# giving an arguably spurious warning to the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: consider instantiating a trainer here once initial exploration is done\n",
    "# manually init IT components for now\n",
    "it_init(**it_session)\n",
    "sl_test_module = it_session.module\n",
    "assert sl_test_module.it_cfg.entailment_mapping_indices is not None\n",
    "\n",
    "layer = 9\n",
    "\n",
    "# TODO: move to module\n",
    "def run_test_split_fn(\n",
    "    module: ITModule,\n",
    "    datamodule: ITDataModule,\n",
    "    limit_test_batches: int,\n",
    "    analysis_cache: Optional[AnalysisCache] = None,\n",
    "    step_fn: str = \"test_step\",\n",
    "    *args,\n",
    "    **kwargs,\n",
    "):\n",
    "    dataloader = datamodule.test_dataloader()\n",
    "    analysis_cache = analysis_cache or AnalysisCache()\n",
    "    module._it_state._current_epoch = 0  # TODO: test removing this, prob not needed in this context\n",
    "    step_func = getattr(module, step_fn)\n",
    "    for batch_idx, batch in tqdm(enumerate(dataloader)):\n",
    "        if batch_idx >= limit_test_batches >= 0:\n",
    "            break\n",
    "        batch = module.batch_to_device(batch)\n",
    "        step_func(batch, batch_idx, analysis_cache=analysis_cache, *args, **kwargs)\n",
    "        module.global_step += 1\n",
    "    return analysis_cache\n",
    "\n",
    "hook_sae_acts_post = f\"{sl_test_module.sae_handles[0].cfg.hook_name}.hook_sae_acts_post\"\n",
    "\n",
    "no_reduce_bool_logit_diff_fn = partial(boolean_logits_to_avg_logit_diff, reduction=None)\n",
    "sum_reduce_bool_logit_diff_fn = partial(boolean_logits_to_avg_logit_diff, reduction=\"sum\")\n",
    "\n",
    "\n",
    "#TODO: make hook_names a list of strings instead of a single string and support it in downstream functions\n",
    "test_cache_base_args = dict(\n",
    "    #limit_test_batches=-1, hook_names=hook_sae_acts_post, **it_session\n",
    "    limit_test_batches=2, hook_names=hook_sae_acts_post, **it_session\n",
    ")\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "logit_diff_summs = {}\n",
    "\n",
    "# TODO: maybe combine prompt and tokens collection into a single function, wrapping clean, sae and ablated modes\n",
    "\n",
    "run_clean_sae_cache= True\n",
    "if run_clean_sae_cache:\n",
    "    logit_diff_summs[\"sae_cache\"] = AnalysisCache(save_cfg=SaveCfg(prompts=True, tokens=True))\n",
    "    run_test_split_fn(\n",
    "        run_ctx=\"cache_with_saes\",\n",
    "        step_fn=\"activation_cache_test_step\",\n",
    "        analysis_cache=logit_diff_summs[\"sae_cache\"],\n",
    "        logit_diff_fn=no_reduce_bool_logit_diff_fn,\n",
    "        **test_cache_base_args,\n",
    "    )\n",
    "\n",
    "# TODO: after experimentation clean this toggles up\n",
    "run_clean_sanity_check = True\n",
    "if run_clean_sanity_check:\n",
    "    assert run_clean_sae_cache, \"Need to run clean SAE cache first to get the base logit diffs and other cache values\"\n",
    "    logit_diff_summs[\"clean\"] = AnalysisCache()\n",
    "    run_test_split_fn(\n",
    "        run_ctx=\"clean\",\n",
    "        step_fn=\"activation_cache_test_step\",\n",
    "        analysis_cache=logit_diff_summs[\"clean\"],\n",
    "        logit_diff_fn=no_reduce_bool_logit_diff_fn,\n",
    "        **test_cache_base_args,\n",
    "    )\n",
    "\n",
    "run_ablation_sanity_check = True\n",
    "if run_ablation_sanity_check:\n",
    "    assert run_clean_sae_cache, \"Need to run clean SAE cache first to get the base logit diffs and other cache values\"\n",
    "    ablation_fwd_hook_cfg = {\n",
    "        \"hook_names\": hook_sae_acts_post,\n",
    "        \"hook_fn\": ablate_sae_latent,\n",
    "        \"alive_latents\": logit_diff_summs[\"sae_cache\"].alive_latents,\n",
    "        \"answer_indices\": logit_diff_summs[\"sae_cache\"].answer_indices,\n",
    "        \"base_logit_diffs\": logit_diff_summs[\"sae_cache\"].logit_diffs,\n",
    "    }\n",
    "\n",
    "    logit_diff_summs[\"ablation\"] = AnalysisCache()\n",
    "    run_test_split_fn(\n",
    "        run_ctx=\"hooks_with_saes\",\n",
    "        step_fn=\"ablation_test_step\",\n",
    "        fwd_hooks_cfg=ablation_fwd_hook_cfg,\n",
    "        analysis_cache=logit_diff_summs[\"ablation\"],\n",
    "        logit_diff_fn=no_reduce_bool_logit_diff_fn,\n",
    "        **test_cache_base_args,\n",
    "    )\n",
    "\n",
    "run_attr_patching = True\n",
    "\n",
    "if run_attr_patching:\n",
    "    torch.set_grad_enabled(True)\n",
    "    # assert torch.is_grad_enabled()\n",
    "    def cache_hook(act, hook, dir: Literal[\"fwd\", \"bwd\"], cache_dict: dict):\n",
    "        cache_dict[dir][hook.name] = act.detach()\n",
    "\n",
    "    hooks_lambda = lambda name: \"hook_sae_acts_post\" in name\n",
    "\n",
    "    cache_dict = {\"fwd\": {}, \"bwd\": {}}\n",
    "    default_attr_patch_hook_cfg = {\"hook_names\": hook_sae_acts_post, \"cache_dict\": cache_dict}\n",
    "\n",
    "    prompts_tokens_flags = {\"prompts\": True, \"tokens\": True} if not run_clean_sae_cache else {}\n",
    "    # TODO: test multiple layer analysis via hooks_lambda instead of a specific hook name\n",
    "    hooks_cfg = {**default_attr_patch_hook_cfg}\n",
    "    hooks_cfg['fwd_hooks'] = [(hook_sae_acts_post, partial(cache_hook, dir=\"fwd\", cache_dict=hooks_cfg[\"cache_dict\"]))]\n",
    "    hooks_cfg['bwd_hooks'] = [(hook_sae_acts_post, partial(cache_hook, dir=\"bwd\", cache_dict=hooks_cfg[\"cache_dict\"]))]\n",
    "    logit_diff_summs[\"attr_patching\"] = AnalysisCache(save_cfg=SaveCfg(**prompts_tokens_flags))\n",
    "    run_test_split_fn(\n",
    "        step_fn=\"attribution_patch_test_step\",\n",
    "        hooks_cfg=hooks_cfg,\n",
    "        analysis_cache=logit_diff_summs[\"attr_patching\"],\n",
    "        logit_diff_fn=no_reduce_bool_logit_diff_fn,\n",
    "        **test_cache_base_args,\n",
    "    )\n",
    "    torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_clean_sanity_check:\n",
    "\n",
    "    translated_labels = [\n",
    "        sl_test_module.datamodule.tokenizer.batch_decode(labels, **DEFAULT_DECODE_KWARGS)\n",
    "        for labels in logit_diff_summs[\"clean\"].labels\n",
    "    ]\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"prompt\": logit_diff_summs[\"sae_cache\"].prompts,\n",
    "            \"correct_answer\": translated_labels,\n",
    "            \"clean_logit_diff\": logit_diff_summs[\"clean\"].logit_diffs,\n",
    "            \"sae_logit_diff\": logit_diff_summs[\"sae_cache\"].logit_diffs,\n",
    "        }\n",
    "    )\n",
    "    df = df.explode([\"prompt\", \"correct_answer\", \"clean_logit_diff\", \"sae_logit_diff\"])\n",
    "    df[\"sample_id\"] = range(len(df))\n",
    "    df = df[[\"sample_id\", \"prompt\", \"correct_answer\", \"clean_logit_diff\", \"sae_logit_diff\"]]\n",
    "    df = df[df.clean_logit_diff > 0].sort_values(by=\"clean_logit_diff\", ascending=False)\n",
    "\n",
    "    print(\n",
    "        tabulate(\n",
    "            df,\n",
    "            headers=[\"Sample ID\", \"Prompt\", \"Answer\", \"Clean Logit Diff\", \"SAE Logit Diff\"],\n",
    "            maxcolwidths=[None, 80, None, None, None],\n",
    "            tablefmt=\"grid\",\n",
    "            numalign=\"left\",\n",
    "            floatfmt=\"+.3f\",\n",
    "            showindex=\"never\",\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_correct = {}\n",
    "\n",
    "for mode, summ in logit_diff_summs.items():\n",
    "    if mode in [\"clean\", \"sae_cache\", \"attr_patching\"]:\n",
    "        correct_statuses = [(labels == preds) for labels, preds in zip(summ.orig_labels, summ.preds)]\n",
    "        positive_logit_diff_statuses = [(logit_diffs > 0) for logit_diffs in summ.logit_diffs]\n",
    "        assert all(torch.eq(torch.cat(correct_statuses), torch.cat(positive_logit_diff_statuses)))\n",
    "        total_correct = sum(torch.cat(correct_statuses)).item()\n",
    "        percentage_correct = total_correct / len(torch.cat(correct_statuses)) * 100\n",
    "        mode_correct[mode] = (total_correct, percentage_correct)\n",
    "    elif mode == \"ablation\":\n",
    "        ablation_per_batch_preds = [torch.stack([p for p in pl.values()]).mode(dim=0).values.cpu() for pl in summ.preds]\n",
    "        num_correct = [(labels == preds).nonzero().unique().size(0) for labels, preds in zip(summ.orig_labels,\n",
    "                                                                                             ablation_per_batch_preds)]\n",
    "        total_correct = sum(num_correct)\n",
    "        batch_size = summ.labels[0].size(0)\n",
    "        percentage_correct = total_correct / (len(torch.cat(summ.orig_labels))) * 100\n",
    "        mode_correct[mode] = (total_correct, percentage_correct)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown mode: {mode}\")\n",
    "\n",
    "table_rows = []\n",
    "for mode, (total_correct, percentage_correct) in mode_correct.items():\n",
    "    table_rows.append([mode, total_correct, f\"{percentage_correct:.2f}%\"])\n",
    "\n",
    "print(tabulate(table_rows, headers=[\"Mode\", \"Total Correct\", \"Percentage Correct\"], tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_per_batch_effects_graphs = False\n",
    "if generate_per_batch_effects_graphs and run_ablation_sanity_check:\n",
    "    for i, ablation_effects in enumerate(logit_diff_summs[\"ablation\"].ablation_effects):\n",
    "        len_alive = len(logit_diff_summs['sae_cache'].alive_latents[i])\n",
    "        px.line(\n",
    "            ablation_effects.mean(dim=0).cpu().numpy(),\n",
    "            title=f\"Causal effects of latent ablation on logit diff of batch {i} ({len_alive} alive)\",\n",
    "            labels={\"index\": \"Latent\", \"value\": \"Causal effect on logit diff\"},\n",
    "            template=\"ggplot2\",\n",
    "            width=1000,\n",
    "        ).update_layout(showlegend=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if run_ablation_sanity_check:\n",
    "    k = 10\n",
    "\n",
    "    correct_acts = torch.cat(logit_diff_summs[\"sae_cache\"].correct_activations)\n",
    "    avg_correct_act, num_correct_act = correct_acts.mean(dim=0), correct_acts.count_nonzero(dim=0)\n",
    "    proportion_correct_active = num_correct_act / len(correct_acts)\n",
    "    correct_mask = torch.cat([(labels == preds) for labels, preds in zip(logit_diff_summs[\"ablation\"].orig_labels,\n",
    "                                                                         ablation_per_batch_preds)])\n",
    "    per_example_ablation_effects = torch.cat(logit_diff_summs[\"ablation\"].ablation_effects)[correct_mask]\n",
    "    total_ablation_effects = per_example_ablation_effects.sum(dim=0)\n",
    "    avg_ablation_effects = per_example_ablation_effects.mean(dim=0)\n",
    "\n",
    "    topk_entries = {\n",
    "        \"positive\": total_ablation_effects.topk(k),\n",
    "        \"negative\": total_ablation_effects.topk(k, largest=False),\n",
    "    }\n",
    "\n",
    "    for label, total_summ in topk_entries.items():\n",
    "        table_rows = []\n",
    "        for value, ind in zip(*total_summ):\n",
    "            table_rows.append([\n",
    "                ind.item(),\n",
    "                value.item(),\n",
    "                avg_ablation_effects[ind].item(),\n",
    "                avg_correct_act[ind].item(),\n",
    "                num_correct_act[ind].item(),\n",
    "                proportion_correct_active[ind].item(),\n",
    "            ])\n",
    "        print(tabulate(\n",
    "            table_rows,\n",
    "            headers=[\n",
    "                \"Latent Index\",\n",
    "                f\"Total {label.capitalize()} Effect\",\n",
    "                \"Mean Effect\",\n",
    "                \"Mean Activation\",\n",
    "                \"Num Examples Active\",\n",
    "                \"Proportion Correct Examples Active\",\n",
    "            ],\n",
    "            tablefmt=\"grid\",\n",
    "        ))\n",
    "\n",
    "    # Print the top 3 positive and negative dashboards\n",
    "    dashboard_k = 3\n",
    "    topk_ablation_latents = {\n",
    "        \"ablation_positive\": total_ablation_effects.topk(dashboard_k),\n",
    "        \"ablation_negative\": total_ablation_effects.topk(dashboard_k, largest=False),\n",
    "    }\n",
    "\n",
    "    for label, topk_latents in topk_ablation_latents.items():\n",
    "        for value, ind in zip(*topk_latents):\n",
    "            print(f\"#{ind} had total causal effect {value:.2f} and was active in {num_correct_act[ind]} examples\")\n",
    "            display_dashboard(\n",
    "                sae_release=\"gpt2-small-hook-z-kk\",\n",
    "                sae_id=f\"blocks.{layer}.hook_z\",\n",
    "                latent_idx=int(ind),\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if run_attr_patching:\n",
    "    k = 10\n",
    "\n",
    "    correct_acts = torch.cat(logit_diff_summs[\"attr_patching\"].correct_activations)\n",
    "    avg_correct_act, num_correct_act = correct_acts.mean(dim=0), correct_acts.count_nonzero(dim=0)\n",
    "    proportion_correct_active = num_correct_act / len(correct_acts)\n",
    "    correct_mask = torch.cat([(logit_diffs > 0) for logit_diffs in summ.logit_diffs])\n",
    "    per_example_attribution_values = torch.cat(logit_diff_summs[\"attr_patching\"].attribution_values)[correct_mask]\n",
    "    total_attribution_values = per_example_attribution_values.sum(dim=0)\n",
    "    avg_attribution_values = per_example_attribution_values.mean(dim=0)\n",
    "\n",
    "    topk_entries = {\n",
    "        \"positive\": total_attribution_values.topk(k),\n",
    "        \"negative\": total_attribution_values.topk(k, largest=False),\n",
    "    }\n",
    "\n",
    "    for label, total_summ in topk_entries.items():\n",
    "        table_rows = []\n",
    "        for value, ind in zip(*total_summ):\n",
    "            table_rows.append([\n",
    "                ind.item(),\n",
    "                value.item(),\n",
    "                avg_attribution_values[ind].item(),\n",
    "                avg_correct_act[ind].item(),\n",
    "                num_correct_act[ind].item(),\n",
    "                proportion_correct_active[ind].item(),\n",
    "            ])\n",
    "        print(tabulate(\n",
    "            table_rows,\n",
    "            headers=[\n",
    "                \"Latent Index\",\n",
    "                f\"Total {label.capitalize()} Effect\",\n",
    "                \"Mean Effect\",\n",
    "                \"Mean Activation\",\n",
    "                \"Num Examples Active\",\n",
    "                \"Proportion Correct Examples Active\",\n",
    "            ],\n",
    "            tablefmt=\"grid\",\n",
    "        ))\n",
    "\n",
    "    # Print the top 3 positive and negative dashboards\n",
    "    dashboard_k = 3\n",
    "    topk_ablation_latents = {\n",
    "        \"ablation_positive\": total_attribution_values.topk(dashboard_k),\n",
    "        \"ablation_negative\": total_attribution_values.topk(dashboard_k, largest=False),\n",
    "    }\n",
    "\n",
    "    for label, topk_latents in topk_ablation_latents.items():\n",
    "        for value, ind in zip(*topk_latents):\n",
    "            print(f\"#{ind} had total causal effect {value:.2f} and was active in {num_correct_act[ind]} examples\")\n",
    "            display_dashboard(\n",
    "                sae_release=\"gpt2-small-hook-z-kk\",\n",
    "                sae_id=f\"blocks.{layer}.hook_z\",\n",
    "                latent_idx=int(ind),\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_attr_patching and run_ablation_sanity_check:\n",
    "    # Visualize results\n",
    "    px.scatter(\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"Ablation\": total_ablation_effects.numpy(),\n",
    "                \"Attribution Patching\": total_attribution_values.numpy(),\n",
    "                \"Latent\": torch.arange(total_attribution_values.size(0)).numpy(),\n",
    "            }\n",
    "        ),\n",
    "        x=\"Ablation\",\n",
    "        y=\"Attribution Patching\",\n",
    "        hover_data=[\"Latent\"],\n",
    "        title=\"Attribution Patching vs Ablation\",\n",
    "        template=\"ggplot2\",\n",
    "        width=800,\n",
    "        height=600,\n",
    "    ).add_shape(\n",
    "        type=\"line\",\n",
    "        x0=total_attribution_values.min(),\n",
    "        x1=total_attribution_values.max(),\n",
    "        y0=total_attribution_values.min(),\n",
    "        y1=total_attribution_values.max(),\n",
    "        line=dict(color=\"red\", width=2, dash=\"dash\"),\n",
    "    ).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: continue implementation here\n",
    "#   - unify hooks_cfg and fwd_hooks_cfg (replacing fwd_hooks_cfg with hooks_cfg fwd only config)\n",
    "#   - abstract sae_cache_test_step to handle both ablation and attribution patching (and clean/sae_cache runs) via saelens adapter\n",
    "#   - cleanup existing functions and toggle flags etc.\n",
    "#   - extend attribution patching to collect latent effects at two different layers and replot top latents (look into\n",
    "#     3898 latent ablation/attribution patching divergence if others emerge)\n",
    "#   - re-test padding right, verify/adjust if necessary for multiple answer positions\n",
    "#   - apply created pipeline to gemma2 and other models!\n",
    "#   - demo framework flexibility by running Lightning-supported steps with lightning and non-lightning supported custom \n",
    "#     steps (analysis step and sae training steps) with IT basictrainer and/or raw pytorch training loop\n",
    "#   - explore using a threshold positive logit diff for tuning\n",
    "#   - though relative logits remain comparable, investigate impact of logit magnitude being substantially smaller with\n",
    "#     padding_side=left (should be able to re-run with padding_side=right)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "it_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
