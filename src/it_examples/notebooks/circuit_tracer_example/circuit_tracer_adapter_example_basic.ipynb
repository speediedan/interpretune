{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretune Circuit Tracer Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fine-Tuning Scheduler logo](logo_fts.png){height=\"55px\" width=\"401px\"}\n",
    "\n",
    "### Intro\n",
    "\n",
    "[Interpretune](https://github.com/speediedan/interpretune) is a flexible framework for exploring, analyzing and tuning \n",
    "llm world models. In this tutorial, we'll walk through a simple example of using Interpretune to pursue interpretability \n",
    "research with Circuit Tracer. As we'll see, Interpretune handles the required execution context composition, allowing us to \n",
    "use the same code in a variety of contexts, depending upon the level of abstraction required.\n",
    "\n",
    "As a long-time PyTorch and PyTorch Lightning contributor, I've found the PyTorch Lightning framework is the right level \n",
    "of abstraction for a large variety of ML research contexts, but some contexts benefit from using core PyTorch directly. \n",
    "Additionally, some users may prefer to use the core PyTorch framework directly for a wide variety of reasons including \n",
    "maximizing portability. As will be demonstrated here, Interpretune maximizes flexibility and portability by adhering to \n",
    "a well-defined protocol that allows auto-composition of our research module with the adapters required for execution in \n",
    "a wide variety of contexts. In this example, we'll be executing the same module with core PyTorch and PyTorch Lightning, \n",
    "demonstrating the use of `Circuit Tracer` w/ Interpretune for circuit discovery and interpretability research.\n",
    "\n",
    "> Note - **this is a WIP**, but this is the core idea. If you have any feedback, please let me know!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note on memory usage\n",
    "\n",
    "In these exercises, we'll be loading language models into memory for circuit analysis. It's useful to have functions which can help profile memory usage for you, so that if you encounter OOM errors you can try and clear out unnecessary models. For example, we've found that with the right memory handling (i.e. deleting models and objects when you're not using them any more) it should be possible to run all the exercises in this material on a Colab Pro notebook.\n",
    "\n",
    "<details>\n",
    "<summary>See this dropdown for some functions which you might find helpful, and how to use them.</summary>\n",
    "\n",
    "First, we can run some code to inspect our current memory usage. Here's an example of running this code during circuit analysis exercises.\n",
    "\n",
    "```python\n",
    "# Profile memory usage\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU Memory Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU Memory Free: {(torch.cuda.memory_reserved() - torch.cuda.memory_allocated()) / 1024**3:.2f} GB\")\n",
    "```\n",
    "\n",
    "If you need to free up memory, you can delete large objects and run garbage collection:\n",
    "\n",
    "```python\n",
    "# Delete large objects if needed\n",
    "# del model\n",
    "# del circuit_tracer_session\n",
    "\n",
    "# Move objects to CPU if needed\n",
    "THRESHOLD = 0.1  # GB\n",
    "for obj in gc.get_objects():\n",
    "    try:\n",
    "        if isinstance(obj, torch.nn.Module):\n",
    "            # Calculate approximate size\n",
    "            total_params = sum(p.numel() for p in obj.parameters())\n",
    "            if total_params * 4 / 1024**3 > THRESHOLD:  # Assuming float32\n",
    "                if hasattr(obj, \"cpu\"):\n",
    "                    obj.cpu()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Force garbage collection\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "```\n",
    "\n",
    "This approach helps manage memory when working with large language models during circuit analysis.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import circuit tracer and required modules\n",
    "from transformer_lens import ActivationCache  # noqa: F401\n",
    "from pprint import pformat\n",
    "from datetime import datetime\n",
    "\n",
    "import interpretune as it  # registered analysis ops will be available as it.<op> when analysis is imported\n",
    "from it_examples import _ACTIVE_PATCHES  # noqa: F401  # TODO: add note about this unless patched in SL before release\n",
    "from it_examples.example_module_registry import MODULE_EXAMPLE_REGISTRY  # TODO: move to hub once implemented\n",
    "from interpretune import ITSessionConfig, ITSession\n",
    "from interpretune.base.call import it_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure our IT Session\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define or customize our session configuration, which includes:\n",
    "1. Experiment/task module and datamodule (in this case, 'rte' for the RTE task) \n",
    "    * We can customize any module, datamodule, or adapter-specific configuration options we want to use. In this case, we set target `circuit_tracer_cfg` that we want to use for our analysis. We also could customize generation parameters, tokenization, the pretrained/config-based model we want to use (in this case, GPT2) etc.\n",
    "2. The adapter context we want to use. In this case, `core` PyTorch (vs e.g. Lightning) and `circuit_tracer` (vs e.g. `transformer_lens` or `sae_lens`). \n",
    "\n",
    "When an `ITSession` is created, the selected adapter context will trigger composition of the relevant adapters with our experiment/task module and datamodule. The intention of this abstraction is to enable the same experiment/task logic to be used unchanged across a broad variety of PyTorch framework and analytical package contexts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our demo config (this will be done from the hub once that is available)\n",
    "base_itdm_cfg, base_it_cfg, dm_cls, m_cls = MODULE_EXAMPLE_REGISTRY.get(\"gemma2.rte_demo.circuit_tracer\")\n",
    "\n",
    "print(pformat(base_it_cfg.circuit_tracer_cfg))\n",
    "\n",
    "# configure our session with our desired adapter composition, core and circuit_tracer in this case\n",
    "session_cfg = ITSessionConfig(\n",
    "    adapter_ctx=(it.Adapter.core, it.Adapter.circuit_tracer),\n",
    "    datamodule_cfg=base_itdm_cfg,\n",
    "    module_cfg=base_it_cfg,\n",
    "    datamodule_cls=dm_cls,\n",
    "    module_cls=m_cls,\n",
    ")\n",
    "\n",
    "# start our session\n",
    "it_session = ITSession(session_cfg)\n",
    "print(\"\\nIT Session created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual init for now\n",
    "it_init(**it_session)\n",
    "print(\"\\nIT Session initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Attribution Graph\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "limit_analysis_batches = 1\n",
    "test_token_limit = -1\n",
    "force_manual_debug_prompts = True  # Set to True to use manual debug prompts instead of random samples\n",
    "# specific tokens to analyze, will use tokens associated with top `max_n_logits` if `None`\n",
    "# analysis_target_tokens: Optional[torch.Tensor] = None\n",
    "\n",
    "example_prompts = []\n",
    "ct_module = it_session.module\n",
    "if not force_manual_debug_prompts:\n",
    "    dataloader = it_session.datamodule.test_dataloader()\n",
    "    for epoch_idx in range(1):  # Run for a single epoch for simplicity\n",
    "        ct_module.current_epoch = epoch_idx\n",
    "        for batch_idx, batch in tqdm(enumerate(dataloader)):\n",
    "            if batch_idx >= limit_analysis_batches >= 0:\n",
    "                break\n",
    "            # fetch the first test_token_limit from the first example in the batch\n",
    "            first_ex_in_batch = batch[:1]\n",
    "            first_ex_in_batch = first_ex_in_batch[\"input\"]\n",
    "            first_ex_in_batch.squeeze_()\n",
    "            if test_token_limit > 0:\n",
    "                first_ex_in_batch = first_ex_in_batch[-test_token_limit:]\n",
    "            first_ex_in_batch = first_ex_in_batch[first_ex_in_batch != 0]\n",
    "            example_prompts.append(first_ex_in_batch)\n",
    "else:\n",
    "    # Generate attribution graphs for a few example prompts\n",
    "    example_prompts = [\n",
    "        # \"The capital of France is\",\n",
    "        \"The capital of the state containing Dallas is\",\n",
    "        # \"When I look at the sky, I see\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating attribution graphs for example prompts...\")\n",
    "slug_base = \"it_circuit_tracer_compute_specific_logits_demo\"\n",
    "results = []\n",
    "\n",
    "for i, prompt in enumerate(example_prompts):\n",
    "    print(f\"\\nProcessing prompt {i + 1}: '{prompt}'\")\n",
    "    slug = f\"{slug_base}_{i + 1}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    # Process the batch using the session, the adapter will handle tokenization and graph generation\n",
    "    try:\n",
    "        graph, local_graph_path, _ = ct_module.generate_graph(prompt=prompt, slug=slug)\n",
    "        results.append(local_graph_path)\n",
    "    except Exception as e:\n",
    "        print(f\"  - Error processing prompt: {e}\")\n",
    "\n",
    "print(f\"\\nProcessed {len(results)} prompts successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Visualizing Attribution Graphs\n",
    "\n",
    "In this section, we'll demonstrate how to save the generated attribution graphs and prepare them for visualization. The CircuitTracerAdapter integrates with Interpretune's AnalysisStore to persistently store graph data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from circuit_tracer.frontend.local_server import serve\n",
    "\n",
    "\n",
    "enable_iframe = False  # whether to enable the IFrame display or not\n",
    "\n",
    "port = 8046\n",
    "server = serve(data_dir=ct_module.circuit_tracer_cfg.graph_output_dir, port=port)\n",
    "# TODO: make this configurable at the top or request the user setup port forwarding and use localhost\n",
    "port_forwarding = False  # whether to use port forwarding or not\n",
    "# hostname = 'localhost'  # the hostname of the server where the graph files are hosted\n",
    "hostname = \"speediedl\"\n",
    "\n",
    "if port_forwarding:\n",
    "    hostname = \"localhost\"  # use localhost for port forwarding\n",
    "    print(\n",
    "        f\"Using port forwarding (ensure it is configured) and localhost. Open your graph here at http://{hostname}:{port}/index.html\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        f\"Not using port forwarding. Use the IFrame below, or open your graph here directly at http://{hostname}:{port}/index.html\"\n",
    "    )\n",
    "\n",
    "if enable_iframe:\n",
    "    from IPython.display import IFrame\n",
    "\n",
    "    # Display the IFrame with the graph visualization\n",
    "    print(f\"Displaying graph visualization in IFrame at http://{hostname}:{port}/index.html\")\n",
    "    display(IFrame(src=f\"http://{hostname}:{port}/index.html\", width=\"100%\", height=\"800px\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps and Future Extensions\n",
    "\n",
    "This notebook demonstrates the basic scaffolding for the CircuitTracerAdapter. The current implementation provides:\n",
    "\n",
    "1. **Basic Integration**: CircuitTracerAdapter integrates with Interpretune's session management\n",
    "2. **Configuration**: CircuitTracerConfig allows customization of attribution parameters\n",
    "3. **Protocol Support**: CircuitAnalysisBatchProtocol defines the interface for batch processing\n",
    "4. **Adapter Composition**: Seamless integration with other Interpretune adapters\n",
    "\n",
    "#### Future Extensions:\n",
    "\n",
    "1. **Full Implementation**: Complete the adapter methods to actually generate attribution graphs\n",
    "2. **Batch Processing**: Support for efficient batch attribution analysis\n",
    "3. **Advanced Analysis**: Integration with AnalysisOp for complex circuit analysis workflows\n",
    "4. **Visualization**: Built-in support for graph visualization and exploration\n",
    "5. **Caching**: Intelligent caching of attribution results for faster iteration\n",
    "6. **Model Support**: Extended support for different model architectures beyond GPT-2\n",
    "\n",
    "#### Resources:\n",
    "\n",
    "- [Circuit Tracer Documentation](https://github.com/jacobdunefsky/circuit-tracer)\n",
    "- [Interpretune Documentation](https://github.com/speediedan/interpretune)\n",
    "- [Attribution Methods Paper](https://arxiv.org/abs/2310.10348)\n",
    "\n",
    "This scaffold provides a solid foundation for building sophisticated circuit analysis workflows with Interpretune and Circuit Tracer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "it_latest (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
