{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/speediedan/interpretune/blob/main/src/it_examples/notebooks/publish/saelens_adapter_example/saelens_adapter_example_registry.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Uncomment to run installation steps if you do not have a development\n",
    "# editable install and want to run this notebook in a fresh environment.\n",
    "# %pip install uv\n",
    "# %uv pip install --upgrade pip setuptools wheel && \\\n",
    "# %uv pip install 'git+https://github.com/speediedan/interpretune.git@main[examples]'\n",
    "# %uv pip install --group git-deps\n",
    "#\n",
    "# NOTE: This cell is intentionally commented out. We will uncomment these\n",
    "# install commands once we no longer need to preserve editable installs\n",
    "# for active developer venvs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretune SAELens Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fine-Tuning Scheduler logo](logo_fts.png){height=\"55px\" width=\"401px\"}\n",
    "\n",
    "### Intro\n",
    "\n",
    "[Interpretune](https://github.com/speediedan/interpretune) is a flexible framework for exploring, analyzing and tuning \n",
    "llm world models. In this tutorial, we'll walk through a simple example of using Interpretune to pursue interpretability \n",
    "research with SAELens. As we'll see, Interpretune handles the required execution context composition, allowing us to \n",
    "use the same code in a variety of contexts, depending upon the level of abstraction required.\n",
    "\n",
    "As a long-time PyTorch and PyTorch Lightning contributor, I've found the PyTorch Lightning framework is the right level \n",
    "of abstraction for a large variety of ML research contexts, but some contexts benefit from using core PyTorch directly. \n",
    "Additionally, some users may prefer to use the core PyTorch framework directly for a wide variety of reasons including \n",
    "maximizing portability. As will be demonstrated here, Interpretune maximizes flexibility and portability by adhering to \n",
    "a well-defined protocol that allows auto-composition of our research module with the adapters required for execution in \n",
    "a wide variety of contexts. In this example, we'll be executing the same module with core PyTorch and PyTorch Lightning, \n",
    "demonstrating the use of `SAELens` w/ Interpretune for interpretability research.\n",
    "\n",
    "> Note - **this is a WIP**, but this is the core idea. If you have any feedback, please let me know!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note on memory usage\n",
    "\n",
    "In these exercises, we'll be loading some pretty large opls into memory (e.g. Gemma 2-2B and its SAEs, as well as a host of other models in later sections of the material). It's useful to have functions which can help profile memory usage for you, so that if you encounter OOM errors you can try and clear out unnecessary models. For example, we've found that with the right memory handling (i.e. deleting models and objects when you're not using them any more) it should be possible to run all the exercises in this material on a Colab Pro notebook, and all the exercises minus the handful involving Gemma on a free Colab notebook.\n",
    "\n",
    "<details>\n",
    "<summary>See this dropdown for some functions which you might find helpful, and how to use them.</summary>\n",
    "\n",
    "First, we can run some code to inspect our current memory usage. Here's me running this code during the exercise set on SAE circuits, after having already loaded in the Gemma models from the previous section. This was on a Colab Pro notebook.\n",
    "\n",
    "```python\n",
    "# Profile memory usage, and delete gemma models if we've loaded them in\n",
    "namespace = globals().copy() | locals()\n",
    "part32_utils.profile_pytorch_memory(namespace=namespace, filter_device=\"cuda:0\")\n",
    "```\n",
    "\n",
    "<pre style=\"font-family: Consolas; font-size: 14px\">Allocated = 35.88 GB\n",
    "Total = 39.56 GB\n",
    "Free = 3.68 GB\n",
    "┌──────────────────────┬────────────────────────┬──────────┬─────────────┐\n",
    "│ Name                 │ Object                 │ Device   │   Size (GB) │\n",
    "├──────────────────────┼────────────────────────┼──────────┼─────────────┤\n",
    "│ gemma_2_2b           │ HookedSAETransformer   │ cuda:0   │       11.94 │\n",
    "│ gpt2                 │ HookedSAETransformer   │ cuda:0   │        0.61 │\n",
    "│ gemma_2_2b_sae       │ SAE                    │ cuda:0   │        0.28 │\n",
    "│ sae_resid_dirs       │ Tensor (4, 24576, 768) │ cuda:0   │        0.28 │\n",
    "│ gpt2_sae             │ SAE                    │ cuda:0   │        0.14 │\n",
    "│ logits               │ Tensor (4, 15, 50257)  │ cuda:0   │        0.01 │\n",
    "│ logits_with_ablation │ Tensor (4, 15, 50257)  │ cuda:0   │        0.01 │\n",
    "│ clean_logits         │ Tensor (4, 15, 50257)  │ cuda:0   │        0.01 │\n",
    "│ _                    │ Tensor (16, 128, 768)  │ cuda:0   │        0.01 │\n",
    "│ clean_sae_acts_post  │ Tensor (4, 15, 24576)  │ cuda:0   │        0.01 │\n",
    "└──────────────────────┴────────────────────────┴──────────┴─────────────┘</pre>\n",
    "\n",
    "From this, we see that we've allocated a lot of memory for the the Gemma model, so let's delete it. We'll also run some code to move any remaining objects on the GPU which are larger than 100MB to the CPU, and print the memory status again.\n",
    "\n",
    "```python\n",
    "del gemma_2_2b\n",
    "del gemma_2_2b_sae\n",
    "\n",
    "THRESHOLD = 0.1  # GB\n",
    "for obj in gc.get_objects():\n",
    "    try:\n",
    "        if isinstance(obj, torch.nn.Module) and part32_utils.get_tensors_size(obj) / 1024**3 > THRESHOLD:\n",
    "            if hasattr(obj, \"cuda\"):\n",
    "                obj.cpu()\n",
    "            if hasattr(obj, \"reset\"):\n",
    "                obj.reset()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Move our gpt2 model & SAEs back to GPU (we'll need them for the exercises we're about to do)\n",
    "gpt2.to(device)\n",
    "gpt2_saes = {layer: sae.to(device) for layer, sae in gpt2_saes.items()}\n",
    "\n",
    "part32_utils.print_memory_status()\n",
    "```\n",
    "\n",
    "<pre style=\"font-family: Consolas; font-size: 14px\">Allocated = 14.90 GB\n",
    "Reserved = 39.56 GB\n",
    "Free = 24.66</pre>\n",
    "\n",
    "Mission success! We've managed to free up a lot of memory. Note that the code which moves all objects collected by the garbage collector to the CPU is often necessary to free up the memory. We can't just delete the objects directly because PyTorch can still sometimes keep references to them (i.e. their tensors) in memory. In fact, if you add code to the for loop above to print out `obj.shape` when `obj` is a tensor, you'll see that a lot of those tensors are actually Gemma model weights, even once you've deleted `gemma_2_2b`.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters - These will be injected by papermill during parameterized test runs\n",
    "core_log_dir = None  # Directory to save analysis logs (if None, a temp directory will be created)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import interpretune as it  # registered analysis ops will be available as it.<op> when analysis is imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import ActivationCache  # noqa: F401\n",
    "from tabulate import tabulate\n",
    "\n",
    "from it_examples import _ACTIVE_PATCHES  # noqa: F401  # TODO: add note about this unless patched in SL before release\n",
    "from it_examples.example_module_registry import MODULE_EXAMPLE_REGISTRY  # TODO: move to hub once implemented\n",
    "from interpretune import ITSessionConfig, ITSession, SAELensFromPretrainedConfig, SAEAnalysisTargets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure our IT Session\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define or customize our session configuration, which includes:\n",
    "1. Experiment/task module and datamodule (in this case, 'rte' for the RTE task) \n",
    "    * We can customize any module, datamodule, or adapter-specific configuration options we want to use. In this case, we set target `sae_cfgs` that we want to use for our analysis. We also could customize generation parameters, tokenization, the pretrained/config-based model we want to use (in this case, GPT2) etc.\n",
    "2. The adapter context we want to use. In this case, `core` PyTorch (vs e.g. Lightning) and `sae_lens` (vs e.g. `transformer_lens`). \n",
    "\n",
    "When an `ITSession` is created, the selected adapter context will trigger composition of the relevant adapters with our experiment/task module and datamodule. The intention of this abstraction is to enable the same experiment/task logic to be used unchanged across a broad variety of PyTorch framework and analytical package contexts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our demo config (this will be done from the hub once that is available)\n",
    "base_itdm_cfg, base_it_cfg, dm_cls, m_cls = MODULE_EXAMPLE_REGISTRY.get(\"gpt2.rte_demo.sae_lens\")\n",
    "# Optionally override base_it_cfg.core_log_dir with the notebook parameter if provided\n",
    "if core_log_dir:\n",
    "    base_it_cfg.core_log_dir = core_log_dir\n",
    "# update our config with our desired SAE analysis targets\n",
    "sae_targets = SAEAnalysisTargets(sae_release=\"gpt2-small-hook-z-kk\", target_layers=[9, 10])\n",
    "sae_cfgs = [\n",
    "    SAELensFromPretrainedConfig(release=sae_fqn.release, sae_id=sae_fqn.sae_id) for sae_fqn in sae_targets.sae_fqns\n",
    "]\n",
    "base_it_cfg.sae_cfgs = sae_cfgs\n",
    "\n",
    "# configure our session with our desired adapter composition, core and sae_lens in this case\n",
    "session_cfg = ITSessionConfig(\n",
    "    adapter_ctx=(it.Adapter.core, it.Adapter.sae_lens),\n",
    "    datamodule_cfg=base_itdm_cfg,\n",
    "    module_cfg=base_it_cfg,\n",
    "    datamodule_cls=dm_cls,\n",
    "    module_cls=m_cls,\n",
    ")\n",
    "\n",
    "# start our session\n",
    "it_session = ITSession(session_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Demo Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Our Analysis Run\n",
    "\n",
    "We define what analysis we want to run. This includes defining:\n",
    "\n",
    "1. our latent space targets (sae_analysis_targets in this case)\n",
    "2. one or more analysis configurations (which can use manual or generated analysis steps)\n",
    "3. the analysis runner\n",
    "\n",
    "The `AnalysisRunner` is the core component of Interpretune that handles the execution of our analysis. It takes care of running the analysis operations defined in our analysis set, managing the execution context, and storing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpretune import AnalysisRunner, AnalysisCfg, AnalysisStore\n",
    "\n",
    "# Define our `AnalysisRunner`. We set:\n",
    "# 1. our analysis targets across all analysis configurations we want to run in the next analysis run\n",
    "# 2. batch and epoch limits\n",
    "# 3. ignore any manual `analysis_step` in our provided module because we want to generate analysis steps based on\n",
    "#    provided operations\n",
    "run_kwargs = dict(sae_analysis_targets=sae_targets, it_session=it_session, max_epochs=1)\n",
    "run_config = dict(limit_analysis_batches=3, ignore_manual=True, **run_kwargs)\n",
    "runner = AnalysisRunner(run_cfg=run_config)\n",
    "\n",
    "# Define our Analysis Configurations\n",
    "# here we demo a few different op compositions involving logit differences\n",
    "auto_logit_diffs_base_cfg = AnalysisCfg(target_op=it.logit_diffs_base, save_prompts=False, save_tokens=False)\n",
    "auto_logit_diffs_sae_cfg = AnalysisCfg(target_op=it.logit_diffs_sae, save_prompts=True, save_tokens=True)\n",
    "auto_logit_diffs_attr_grad_cfg = AnalysisCfg(target_op=it.logit_diffs_attr_grad, save_prompts=True, save_tokens=True)\n",
    "auto_logit_diffs_attr_ablation_cfg = AnalysisCfg(\n",
    "    target_op=it.logit_diffs_attr_ablation, save_prompts=False, save_tokens=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_results = runner.run_analysis(\n",
    "    analysis_cfgs=(\n",
    "        auto_logit_diffs_base_cfg,\n",
    "        auto_logit_diffs_sae_cfg,\n",
    "        auto_logit_diffs_attr_grad_cfg,\n",
    "        auto_logit_diffs_attr_ablation_cfg,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set convenience variables for exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_cfg = runner.run_cfg\n",
    "sl_test_module = run_cfg.module  # convenience handle to the module used in the analysis\n",
    "artifact_cfg = run_cfg.artifact_cfg\n",
    "# Set tutorial_active_ops based on the keys in analysis_results\n",
    "if isinstance(analysis_results, dict):\n",
    "    tutorial_active_ops = set(analysis_results.keys())\n",
    "elif hasattr(run_cfg, \"analysis_cfg\") and run_cfg.analysis_cfg:\n",
    "    # Single analysis configuration\n",
    "    tutorial_active_ops = {run_cfg.analysis_cfg.name}\n",
    "else:\n",
    "    tutorial_active_ops = set()\n",
    "\n",
    "# If analysis_results is an AnalysisStore, convert to a dict with a single entry\n",
    "if isinstance(analysis_results, AnalysisStore):\n",
    "    analysis_results = {run_cfg.analysis_cfg.name: analysis_results}\n",
    "\n",
    "print(f\"Analysis completed for {len(analysis_results) if isinstance(analysis_results, dict) else 1} operations:\")\n",
    "for cfg_name in analysis_results.keys() if isinstance(analysis_results, dict) else [run_cfg.analysis_cfg.name]:\n",
    "    print(f\"- {cfg_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Demo Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean vs SAE Sample-wise Logit Diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if {it.logit_diffs_base.name, it.logit_diffs_sae.name}.issubset(tutorial_active_ops):\n",
    "    from interpretune.analysis import base_vs_sae_logit_diffs\n",
    "\n",
    "    base_vs_sae_logit_diffs(\n",
    "        sae=analysis_results[it.logit_diffs_sae.name],\n",
    "        base_ref=analysis_results[it.logit_diffs_base.name],\n",
    "        top_k=artifact_cfg.top_k_clean_logit_diffs,\n",
    "        tokenizer=sl_test_module.datamodule.tokenizer,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proportion Correct Answers on Dataset By Analysis Op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpretune.analysis import compute_correct\n",
    "\n",
    "pred_summaries = {op: compute_correct(summ, op) for op, summ in analysis_results.items()}\n",
    "table_rows = []\n",
    "for op, (total_correct, percentage_correct, _) in pred_summaries.items():\n",
    "    table_rows.append([op, total_correct, f\"{percentage_correct:.2f}%\"])\n",
    "\n",
    "print(tabulate(table_rows, headers=[\"Op\", \"Total Correct\", \"Percentage Correct\"], tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Per Batch Ablation Effect Graphs [Optional]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if artifact_cfg.latent_effects_graphs and it.logit_diffs_attr_ablation.name in tutorial_active_ops:\n",
    "    # TODO: add note that only latent effects associated with correct answers currently displayed\n",
    "    # TODO: allow toggling correct filtering during runs\n",
    "    analysis_results[it.logit_diffs_attr_ablation.name].plot_latent_effects(\n",
    "        per_batch=artifact_cfg.latent_effects_graphs_per_batch\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Per-SAE Ablation Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if it.logit_diffs_attr_ablation.name in tutorial_active_ops:\n",
    "    ablation_batch_preds = pred_summaries[it.logit_diffs_attr_ablation.name].batch_predictions\n",
    "    activation_summary = analysis_results[it.logit_diffs_sae.name].calc_activation_summary()\n",
    "\n",
    "    ablation_metrics = analysis_results[it.logit_diffs_attr_ablation.name].calculate_latent_metrics(\n",
    "        pred_summ=pred_summaries[it.logit_diffs_attr_ablation.name],\n",
    "        activation_summary=activation_summary,\n",
    "        # filter_by_correct=True,\n",
    "        run_name=\"logit_diffs.attribution.ablation\",\n",
    "    )\n",
    "\n",
    "    tables = ablation_metrics.create_attribution_tables(\n",
    "        top_k=artifact_cfg.top_k_latents_table, filter_type=\"both\", per_sae=artifact_cfg.latents_table_per_sae\n",
    "    )\n",
    "\n",
    "    for title, table in tables.items():\n",
    "        print(f\"\\n{title}\\n{table}\\n\")\n",
    "\n",
    "    sl_test_module.display_latent_dashboards(\n",
    "        ablation_metrics,\n",
    "        title=\"Ablation-Mediated Latent Analysis\",\n",
    "        sae_release=runner.run_cfg.sae_analysis_targets.sae_release,\n",
    "        top_k=artifact_cfg.top_k_latent_dashboards,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Per-SAE Attribution Patching Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if it.logit_diffs_attr_grad.name in tutorial_active_ops:\n",
    "    # per-SAE activation summaries are calculated using our AnalysisStore since the relevant keys are present,\n",
    "    # no need to provide a separate activation summary from another comparison cache in this case as with ablation\n",
    "    activation_summary = analysis_results[it.logit_diffs_attr_grad.name].calc_activation_summary()\n",
    "    attribution_patching_metrics = analysis_results[it.logit_diffs_attr_grad.name].calculate_latent_metrics(\n",
    "        pred_summ=pred_summaries[it.logit_diffs_attr_grad.name], run_name=\"logit_diffs.attribution.grad_based\"\n",
    "    )\n",
    "\n",
    "    tables = attribution_patching_metrics.create_attribution_tables(\n",
    "        top_k=artifact_cfg.top_k_latents_table, filter_type=\"both\", per_sae=artifact_cfg.latents_table_per_sae\n",
    "    )\n",
    "\n",
    "    for title, table in tables.items():\n",
    "        print(f\"\\n{title}\\n{table}\\n\")\n",
    "\n",
    "    sl_test_module.display_latent_dashboards(\n",
    "        attribution_patching_metrics,\n",
    "        title=\"Attribution Patching-Mediated Latent Analysis\",\n",
    "        sae_release=runner.run_cfg.sae_analysis_targets.sae_release,\n",
    "        top_k=artifact_cfg.top_k_latent_dashboards,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Per-SAE Ablation vs Attribution-Patching Effect Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if {it.logit_diffs_attr_grad.name, it.logit_diffs_attr_ablation.name}.issubset(tutorial_active_ops):\n",
    "    from interpretune.analysis import latent_metrics_scatter\n",
    "\n",
    "    # Visualize results for each hook\n",
    "    # Call the function with our metrics\n",
    "    latent_metrics_scatter(\n",
    "        ablation_metrics, attribution_patching_metrics, label1=\"Ablation\", label2=\"Attribution Patching\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "it_latest (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
