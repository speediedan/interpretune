{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac8695c0",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/speediedan/interpretune/blob/main/src/it_examples/notebooks/publish/example_op_collections/op_collection_example.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05a8f2a",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Uncomment to run installation steps if you do not have a development\n",
    "# editable install and want to run this notebook in a fresh environment.\n",
    "# %pip install uv\n",
    "# %uv pip install --upgrade pip setuptools wheel && \\\n",
    "# %uv pip install 'git+https://github.com/speediedan/interpretune.git@main[examples]'\n",
    "# %uv pip install --group git-deps\n",
    "#\n",
    "# NOTE: This cell is intentionally commented out. We will uncomment these\n",
    "# install commands once we no longer need to preserve editable installs\n",
    "# for active developer venvs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadfea7f",
   "metadata": {},
   "source": [
    "# Example Hub and Local Operation Collections\n",
    "\n",
    "This notebook demonstrates the complete workflow for uploading and downloading operations collections using the \n",
    "`HubAnalysisOpManager` and loading local operations via `IT_ANALYSIS_OP_PATHS`. The workflow includes:\n",
    "\n",
    "1. Setting up local op collection path via IT_ANALYSIS_OP_PATHS\n",
    "2. Copying the current hub_op_collection folder to /tmp/\n",
    "3. Uploading operations to HuggingFace Hub as a private repository\n",
    "4. Downloading the uploaded collection to the default cache\n",
    "5. Re-importing interpretune to verify both hub and local operations are available\n",
    "6. Testing the loaded operations\n",
    "7. Cleaning up downloaded operations and re-importing\n",
    "8. Verifying only local operations remain available\n",
    "9. Final cleanup of the local operations collection\n",
    "\n",
    "```python\n",
    "\n",
    "**Note**: This example requires HuggingFace Hub authentication and will create a private repository.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24d51b0",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395a631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Import interpretune components\n",
    "import interpretune\n",
    "from interpretune.analysis.ops.hub_manager import HubAnalysisOpManager\n",
    "from interpretune.analysis import IT_ANALYSIS_CACHE, IT_ANALYSIS_HUB_CACHE, IT_ANALYSIS_OP_PATHS, IT_MODULES_CACHE\n",
    "from interpretune.base.components.cli import IT_BASE\n",
    "\n",
    "# Import utility functions for op collection demo setup/cleanup\n",
    "import it_examples.notebooks.publish.example_op_collections.op_collection_demo_utils as op_demo_utils\n",
    "\n",
    "example_op_collections_dir = Path(IT_BASE / \"notebooks\" / \"publish\" / \"example_op_collections\")\n",
    "example_hub_op_collection_dir = Path(example_op_collections_dir / \"hub_op_collection\")\n",
    "example_local_op_collection_dir = Path(example_op_collections_dir / \"local_op_collection\")\n",
    "\n",
    "# Print environment summary\n",
    "op_demo_utils.print_env_summary(\n",
    "    interpretune.version,\n",
    "    IT_ANALYSIS_CACHE,\n",
    "    IT_MODULES_CACHE,\n",
    "    IT_ANALYSIS_HUB_CACHE,\n",
    "    IT_ANALYSIS_OP_PATHS,\n",
    "    example_hub_op_collection_dir,\n",
    "    example_local_op_collection_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19463a7b",
   "metadata": {},
   "source": [
    "## Step 1: Stage example local op collections to a temporary directory\n",
    "\n",
    "Copy the local_op_collection to /tmp/ and add it to IT_ANALYSIS_OP_PATHS so local operations are loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76259b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define source and destination paths for local ops\n",
    "source_local_op_collection = example_local_op_collection_dir\n",
    "tmp_local_op_collection = Path(\"/tmp/local_op_collection\")\n",
    "\n",
    "# copy our local op collection to `tmp_local_op_collection` and that path to our IT_ANALYSIS_OP_PATHS env var\n",
    "original_op_paths_env, new_op_paths = op_demo_utils.setup_local_op_collection(\n",
    "    source_local_op_collection=source_local_op_collection, tmp_local_op_collection=tmp_local_op_collection\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bc534c",
   "metadata": {},
   "source": [
    "## Step 2: Copy hub op_collection to /tmp/\n",
    "\n",
    "Copy the hub op_collection folder to /tmp/ for upload to the hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4946f954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define source and destination paths for hub ops\n",
    "source_op_collection = example_hub_op_collection_dir\n",
    "tmp_op_collection = Path(\"/tmp/hub_op_collection\")\n",
    "\n",
    "# Stage a hub op collection using utility function\n",
    "op_demo_utils.setup_hub_op_collection(source_op_collection=source_op_collection, tmp_op_collection=tmp_op_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5ec51b",
   "metadata": {},
   "source": [
    "## Step 3: Upload operations to HuggingFace Hub\n",
    "\n",
    "Upload the hub op_collection to HuggingFace Hub as a private repository named \"trivial_op_repo\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467c6fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import whoami\n",
    "\n",
    "current_user = whoami()[\"name\"]\n",
    "\n",
    "# Initialize the hub manager\n",
    "hub_manager = HubAnalysisOpManager()\n",
    "\n",
    "# Repository configuration\n",
    "repo_name = \"trivial_op_repo\"\n",
    "private = True\n",
    "\n",
    "print(\"Uploading op_collection to HuggingFace Hub...\")\n",
    "print(f\"Current HF user: {current_user}\")\n",
    "print(f\"Repository: {repo_name}\")\n",
    "print(f\"Private: {private}\")\n",
    "print(f\"Source folder: {tmp_op_collection}\")\n",
    "\n",
    "# Ensure the user is authenticated\n",
    "repo_id = f\"{current_user}/{repo_name}\"\n",
    "try:\n",
    "    # Upload operations to hub\n",
    "    # 1. This will create the specified repository if it doesn't exist\n",
    "    # 2. If the repo exists, it will clean existing operations and upload the new ones in a single commit\n",
    "    #    - If no files have changed, it will skip the commit and leave the repository unchanged\n",
    "\n",
    "    upload_result = hub_manager.upload_ops(\n",
    "        local_dir=tmp_op_collection, repo_id=repo_id, private=private, clean_existing=True\n",
    "    )\n",
    "\n",
    "    print(f\"‚úì Successfully uploaded operations (if necessary) to {repo_name}\")\n",
    "    print(f\"Upload result (new or latest op repo commit sha): {upload_result}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error uploading operations: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e3bafb",
   "metadata": {},
   "source": [
    "## Step 4: Download operations to default hub cache\n",
    "\n",
    "Download the uploaded operations collection to the default `IT_ANALYSIS_HUB_CACHE` location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ffd7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Downloading operations from {repo_id} to default cache...\")\n",
    "print(f\"Cache location: {IT_ANALYSIS_HUB_CACHE}\")\n",
    "\n",
    "# Initialize download_result to None so we can safely check it in cleanup step\n",
    "download_result = None\n",
    "\n",
    "try:\n",
    "    # Download operations from hub to default cache\n",
    "    download_result = hub_manager.download_ops(repo_id=repo_id)  # no cache_dir default IT_ANALYSIS_HUB_CACHE is used\n",
    "\n",
    "    print(\"‚úì Successfully downloaded operations to cache\")\n",
    "    print(f\"Download result: {download_result}\")\n",
    "\n",
    "    # Check what was downloaded\n",
    "    cache_path = Path(IT_ANALYSIS_HUB_CACHE)\n",
    "    if cache_path.exists():\n",
    "        print(\"\\nContents of hub cache:\")\n",
    "        for item in cache_path.rglob(\"*\"):\n",
    "            if item.is_file():\n",
    "                rel_path = item.relative_to(cache_path)\n",
    "                print(f\"  - {rel_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error downloading operations: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f31c25",
   "metadata": {},
   "source": [
    "## Step 5: Re-import interpretune and verify hub and local operations\n",
    "\n",
    "Re-import interpretune to pick up both hub and local operations and verify they are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1754e16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Re-importing interpretune to pick up hub and local operations...\")\n",
    "# Remove interpretune modules from sys.modules to force reimport\n",
    "op_demo_utils.purge_it_modules_from_sys()\n",
    "\n",
    "# ruff: noqa: E402\n",
    "\n",
    "# Re-import interpretune\n",
    "import interpretune as it\n",
    "from interpretune import DISPATCHER\n",
    "\n",
    "print(\"‚úì Interpretune re-imported\")\n",
    "\n",
    "# Get operation definitions and generate summary\n",
    "operation_definitions = DISPATCHER.registered_ops\n",
    "op_demo_utils.generate_op_summary(operation_definitions)\n",
    "\n",
    "# Show operations by type\n",
    "canonical_ops, alias_map, hub_ops, local_ops, composed_ops, builtin_ops = op_demo_utils.categorize_operations(\n",
    "    operation_definitions\n",
    ")\n",
    "\n",
    "# Demo lazy operation instantiation\n",
    "op_demo_utils.demo_lazy_op_instantiation(it, hub_ops, local_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61dc24e",
   "metadata": {},
   "source": [
    "## Step 6: Test executing the loaded operations\n",
    "\n",
    "Test executing simple hub and local operations both individually executed and as part of a composite operation to ensure loading and execution works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cb901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüß™ Testing loaded operations with demo data...\")\n",
    "\n",
    "# Import required components\n",
    "from interpretune import trivial_test_op, trivial_local_test_op, composite_trivial_test_op\n",
    "\n",
    "NUM_BATCHES = 2  # Number of test batches to generate\n",
    "VERBOSE_OP_OUTPUTS = False  # Set to True to log operation outputs\n",
    "\n",
    "# Test the operations\n",
    "print(f\"\\nüìã Testing operation pipeline parity of composite vs individual component ops (over {NUM_BATCHES} batches):\")\n",
    "individual_op_output_batches = []\n",
    "composite_op_output_batches = []\n",
    "\n",
    "for batch_name, individual_test_batch, composite_test_batch in op_demo_utils.generate_test_batches(NUM_BATCHES):\n",
    "    print(\"\\nComposite op execution...\")\n",
    "    if VERBOSE_OP_OUTPUTS:\n",
    "        print(f\"\\n--- {batch_name} ---\")\n",
    "        print(f\"Input batch: {individual_test_batch}\")\n",
    "    composite_output_batch = composite_trivial_test_op(analysis_batch=composite_test_batch)\n",
    "    op_demo_utils.maybe_print_output(f\"Composite op output batch: {composite_output_batch}\", VERBOSE_OP_OUTPUTS)\n",
    "    composite_op_output_batches.append(composite_output_batch)\n",
    "\n",
    "    print(\"\\nRe-running with individual component ops...\")\n",
    "    local_batch_output = trivial_local_test_op(analysis_batch=individual_test_batch)\n",
    "    op_demo_utils.maybe_print_output(f\"Local op batch output: {local_batch_output}\", VERBOSE_OP_OUTPUTS)\n",
    "    individual_output_batch = trivial_test_op(analysis_batch=local_batch_output)\n",
    "    op_demo_utils.maybe_print_output(f\"Hub output batch: {individual_output_batch}\", VERBOSE_OP_OUTPUTS)\n",
    "    individual_op_output_batches.append(individual_output_batch)\n",
    "\n",
    "# Compare outputs using utility function\n",
    "all_match = op_demo_utils.compare_operation_outputs(individual_op_output_batches, composite_op_output_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e04c8a4",
   "metadata": {},
   "source": [
    "## Step 7: Clean up hub operations and re-import\n",
    "\n",
    "Delete the downloaded hub operations folder and re-import interpretune to verify only local operations remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b99c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cleaning up downloaded hub operations...\")\n",
    "\n",
    "# Remove only the specific repository we downloaded, not the entire hub cache\n",
    "op_demo_utils.cleanup_hub_repository(download_result)\n",
    "\n",
    "# Re-import interpretune again\n",
    "print(\"\\nRe-importing interpretune after cleanup...\")\n",
    "\n",
    "# Capture stdout and stderr during import to check for the expected warning\n",
    "stdout_output, stderr_output, DISPATCHER = op_demo_utils.reimport_interpretune_with_capture()\n",
    "\n",
    "op_demo_utils.inspect_err_for_composite_op_warning(stderr_output)\n",
    "\n",
    "print(\"\\n ‚úì Interpretune re-imported after cleanup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d16772",
   "metadata": {},
   "source": [
    "## Step 8: Verify only local operations remain\n",
    "\n",
    "Verify that only the local and built-in operations are available after hub cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f0636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Verifying operations after cleanup...\")\n",
    "\n",
    "# Get operation definitions after cleanup and verify cleanup status\n",
    "operation_definitions_after = DISPATCHER.registered_ops\n",
    "op_demo_utils.verify_cleanup_status(operation_definitions_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb0b647",
   "metadata": {},
   "source": [
    "## Cleanup temporary files\n",
    "\n",
    "Clean up the temporary files created during this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a37ad14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up using utility function\n",
    "op_demo_utils.cleanup_op_collections(\n",
    "    tmp_op_collection=tmp_op_collection,\n",
    "    tmp_local_op_collection=tmp_local_op_collection,\n",
    "    original_op_paths_env=original_op_paths_env,\n",
    ")\n",
    "\n",
    "print(\"\\nüéâ Hub and Local operations workflow example completed successfully!\")\n",
    "print(\"\\nSummary of what was demonstrated:\")\n",
    "print(\"1. ‚úì Setup local op collection path via IT_ANALYSIS_OP_PATHS environment variable\")\n",
    "print(\"2. ‚úì Copied hub op_collection to /tmp/ with overwrite warning\")\n",
    "print(\"3. ‚úì Uploaded operations to HuggingFace Hub as private repo\")\n",
    "print(\"4. ‚úì Downloaded operations to default hub cache\")\n",
    "print(\"5. ‚úì Re-imported interpretune and verified both hub and local operations\")\n",
    "print(\"6. ‚úì Tested operation instantiation and execution with demo data\")\n",
    "print(\"7. ‚úì Cleaned up hub operations and re-imported\")\n",
    "print(\"8. ‚úì Verified only local and built-in operations remain available\")\n",
    "print(\"9. ‚úì Restored original IT_ANALYSIS_OP_PATHS environment variable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150f93a1",
   "metadata": {},
   "source": [
    "## Step 9: Final verification after environment cleanup\n",
    "\n",
    "Re-import interpretune one final time to verify that local operations are no longer available after unsetting IT_ANALYSIS_OP_PATHS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b881ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final verification: Re-importing interpretune after environment cleanup...\")\n",
    "\n",
    "# Remove interpretune modules from sys.modules to force reimport\n",
    "op_demo_utils.purge_it_modules_from_sys()\n",
    "\n",
    "# Re-import interpretune one final time\n",
    "import interpretune\n",
    "from interpretune import DISPATCHER\n",
    "\n",
    "print(\"‚úì Interpretune re-imported after environment cleanup\")\n",
    "\n",
    "# Get operation definitions after complete cleanup and generate final summary\n",
    "operation_definitions_final = DISPATCHER.registered_ops\n",
    "canonical_ops_final, alias_map_final, hub_ops_final, local_ops_final, composed_ops_final, builtin_ops = (\n",
    "    op_demo_utils.categorize_operations(operation_definitions_final)\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Final Operation Summary (after complete cleanup):\")\n",
    "print(f\"  Total registered names: {len(operation_definitions_final)}\")\n",
    "print(f\"  Unique operations: {len(canonical_ops_final)}\")\n",
    "print(f\"  Hub operations: {len(hub_ops_final)}\")\n",
    "print(f\"  Local operations: {len(local_ops_final)}\")\n",
    "print(f\"  Composed operations: {len(composed_ops_final)}\")\n",
    "print(f\"  Built-in operations: {len(builtin_ops)}\")\n",
    "\n",
    "# Verify complete cleanup\n",
    "if len(hub_ops_final) == 0 and len(local_ops_final) == 0:\n",
    "    print(\"\\nüéØ Perfect! Complete cleanup successful - only built-in and composed operations remain!\")\n",
    "elif len(hub_ops_final) == 0:\n",
    "    print(f\"\\n‚ö†Ô∏è Hub operations cleaned up, but {len(local_ops_final)} local operations still present:\")\n",
    "    for op_name, op_def in local_ops_final.items():\n",
    "        aliases = alias_map_final.get(op_name, [])\n",
    "        all_names = [op_name] + aliases\n",
    "        print(f\"    - {op_name} (accessible as: {', '.join(all_names)})\")\n",
    "elif len(local_ops_final) == 0:\n",
    "    print(f\"\\n‚ö†Ô∏è Local operations cleaned up, but {len(hub_ops_final)} hub operations still present:\")\n",
    "    for op_name, op_def in hub_ops_final.items():\n",
    "        aliases = alias_map_final.get(op_name, [])\n",
    "        all_names = [op_name] + aliases\n",
    "        print(f\"    - {op_name} (accessible as: {', '.join(all_names)})\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Cleanup incomplete: {len(hub_ops_final)} hub ops and {len(local_ops_final)} local ops still present\")\n",
    "\n",
    "print(\"\\nEnvironment verification:\")\n",
    "print(f\"  Current IT_ANALYSIS_OP_PATHS env var: '{os.environ.get('IT_ANALYSIS_OP_PATHS', 'Not set')}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "it_latest (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
