{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/speediedan/interpretune/blob/main/src/it_examples/notebooks/publish/attribution_analysis/attribution_analysis.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install interpretune with examples\n",
    "!pip install interpretune[examples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attribution Analysis with Interpretune, Circuit Tracer, and Neuronpedia\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/speediedan/interpretune/blob/main/src/it_examples/notebooks/publish/attribution_analysis/attribution_analysis.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install interpretune with examples and circuit_tracer\n",
    "# Once interpretune and circuit_tracer are published to PyPI, the conditional installation\n",
    "# logic below can be simplified to just: !pip install interpretune[examples]\n",
    "# The pip installer will automatically detect and preserve editable installations.\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "\n",
    "def should_install_package(package_name):\n",
    "    \"\"\"Check if package should be installed.\n",
    "\n",
    "    Returns False if package is already installed in editable mode (to preserve dev environments).\n",
    "    Returns True otherwise.\n",
    "\n",
    "    NOTE: Once circuit_tracer is on PyPI, this function can be removed. When interpretune[examples]\n",
    "    depends on circuit_tracer via PyPI (instead of git URL), pip will automatically check for\n",
    "    existing installations and won't reinstall over editable installs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [sys.executable, \"-m\", \"pip\", \"show\", package_name],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=False,\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            # Package is installed - check if it's editable\n",
    "            if \"Editable project location:\" in result.stdout:\n",
    "                print(f\"‚úì {package_name} already installed in editable mode - skipping installation\")\n",
    "                return False\n",
    "            print(f\"‚Ñπ {package_name} installed but not editable - will reinstall\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö† Error checking {package_name}: {e}\")\n",
    "        return True\n",
    "\n",
    "\n",
    "# Install interpretune from git with examples extra\n",
    "if should_install_package(\"interpretune\"):\n",
    "    !python -m pip install 'git+https://github.com/speediedan/interpretune.git@main[examples]'\n",
    "\n",
    "# Install circuit_tracer from git (required until it's published to PyPI)\n",
    "if should_install_package(\"circuit-tracer\"):\n",
    "    # Note: Using line continuation for long git URL\n",
    "    !python -m pip install \\\n",
    "        'git+https://github.com/speediedan/circuit-tracer.git@b228bf190fadb3cb30f6a5ba6691dc4c86d76ba3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Attribution Graphs with Interpretune, Circuit Tracer and Neuronpedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Interpretune](https://github.com/speediedan/interpretune) is a flexible framework for exploring, analyzing and tuning \n",
    "llm world models. In this tutorial, we'll walk through a simple example of using Interpretune to pursue interpretability \n",
    "research with Circuit Tracer. As we'll see, Interpretune handles the required execution context composition, allowing us to \n",
    "use the same code in a variety of contexts, depending upon the level of abstraction required.\n",
    "\n",
    "As a long-time PyTorch and PyTorch Lightning contributor, I've found the PyTorch Lightning framework is the right level \n",
    "of abstraction for a large variety of ML research contexts, but some contexts benefit from using core PyTorch directly. \n",
    "Additionally, some users may prefer to use the core PyTorch framework directly for a wide variety of reasons including \n",
    "maximizing portability. As will be demonstrated here, Interpretune maximizes flexibility and portability by adhering to \n",
    "a well-defined protocol that allows auto-composition of our research module with the adapters required for execution in \n",
    "a wide variety of contexts. In this example, we'll be executing the same module with core PyTorch and PyTorch Lightning, \n",
    "demonstrating the use of `Circuit Tracer` w/ Interpretune for circuit discovery and interpretability research.\n",
    "\n",
    "> Note - **this is a WIP**, but this is the core idea. If you have any feedback, please let me know!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters - These will be injected by papermill during parameterized test runs\n",
    "use_baseline_salient_logits = True  # logits computation mode: True->salient logits, False->specific logits\n",
    "use_baseline_transcoder_arch = True  # transcoder architecture: True->SingleLayerTranscoder, False->CrossLayerTranscoder\n",
    "enable_analysis_injection = True  # Toggle analysis injection for analyzing attribution flow\n",
    "core_log_dir = None  # Directory to save analysis logs (if None, a temp directory will be created)\n",
    "analysis_config_path = \"analysis_injection_config.yaml\"  # Base YAML config merged with notebook overrides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretune Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import interpretune as it  # registered analysis ops will be available as it.<op> when analysis is imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Import circuit tracer and required modules\n",
    "from transformer_lens import ActivationCache  # noqa: F401\n",
    "from pprint import pformat\n",
    "from datetime import datetime\n",
    "\n",
    "from it_examples import _ACTIVE_PATCHES  # noqa: F401  # TODO: add note about this unless patched in SL before release\n",
    "from it_examples.example_module_registry import MODULE_EXAMPLE_REGISTRY  # TODO: move to hub once implemented\n",
    "from interpretune import ITSessionConfig, ITSession\n",
    "from interpretune.base.call import it_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable Analysis Injection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretune includes a lightweight in-notebook analysis injection utility that can temporarily instrument adapter packages (for this notebook, `circuit_tracer`) with small, configurable hooks named analysis points.\n",
    "\n",
    "This tooling is intentionally experimental and most useful for short-lived, exploratory work: it lets you quickly inspect intermediate values, add ad-hoc diagnostics and exposition (as in this notebook), and prototype ideas without changing the adapter's source tree.\n",
    "\n",
    "By default, we set `enable_analysis_injection = True` in our parameters cell above to enable the runtime patching. The orchestrator validates hooks, applies them to the target package path, and registers the analysis point functions you provide (whether sourced from files or notebook cells).\n",
    "\n",
    "<details>\n",
    "<summary>More on Analysis Injection</summary>\n",
    "\n",
    "Key characteristics:\n",
    "\n",
    "- Ephemeral: patches are applied in-process for the notebook session only and do not persist to source files.\n",
    "- Configuration-driven: hooks are registered via YAML configs plus analysis function mappings that can live in this notebook, external files, or any mix of the two.\n",
    "- Flexible composition: you can load the base YAML and analysis hook functions from external files, then override or extend them inline in subsequent notebook cells.\n",
    "- Notebook-friendly output: each analysis point's collected data is available via various helper methods on the `orchestrator` (e.g. `get_analysis_data`, `get_output`), allowing later cells to display the captured values without re-running the instrumentation (as is done in the attribution analysis exposition of this notebook). Analysis events are also written to the configured log (e.g. `/tmp/attribution_flow_analysis_<timestamp>.log`) and optionally to the console when enabled.\n",
    "\n",
    "The `analysis_injection` utility is a powerful exploratory and expository tool, but for ongoing or production workflows you should upstream a proper hook interface in the relevant adapter package.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Guidance on Analysis Injection Usage</summary>\n",
    "\n",
    "If you or other users find a recurring need to access intermediate analysis variables, the more robust and maintainable solution is to add a small hook interface to the adapter package itself.\n",
    "\n",
    "Why prefer adapter-level hooks over regex-based patching:\n",
    "- Stability: explicit APIs are far less brittle than runtime regex patching.\n",
    "- Maintainability: adapter maintainers can review, test, and document hook APIs so they remain supported across releases.\n",
    "- Performance and safety: built-in hooks can be designed to avoid unintended side-effects or excessive overhead.\n",
    "\n",
    "If an adapter would benefit from exposing intermediate analysis variables, Interpretune recommends opening an issue or a pull request against the adapter repository proposing a small, well-scoped hook API (describe the use case, example call signatures, and what guarantees callers should expect). This is the recommended path for any capability you expect to use repeatedly or across teams.\n",
    "\n",
    "When to use which approach:\n",
    "- Use `analysis_injection` for short experiments, expository notebooks like this one, ad-hoc debugging, or rapid iteration on ideas.\n",
    "- Propose adapter hooks (issue/PR) when you want a repeatable, supported, and long-term inspection facility.\n",
    "\n",
    "For more details and guidance on safe usage patterns, see the Interpretune project documentation and repo: https://github.com/speediedan/interpretune\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Any, Dict\n",
    "\n",
    "import torch\n",
    "\n",
    "from it_examples.utils.analysis_injection.analysis_hook_patcher import HOOK_REGISTRY\n",
    "from it_examples.utils.analysis_injection.orchestrator import analysis_log_point, sample_tensor_output\n",
    "from it_examples.utils.example_helpers import collect_shapes, VarAnnotate\n",
    "from it_examples.utils.raw_graph_analysis import plot_ridgeline_convergence\n",
    "from IPython.display import display\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "raw_config_path = Path(analysis_config_path)\n",
    "base_config_path = raw_config_path if raw_config_path.is_absolute() else NOTEBOOK_DIR / raw_config_path\n",
    "if not base_config_path.exists():\n",
    "    raise FileNotFoundError(f\"Expected config file at {base_config_path}\")\n",
    "base_config_path = base_config_path.resolve()\n",
    "\n",
    "print(f\"Using base analysis injection config at {base_config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define/Customize Analysis Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook override: add the attribution setup analysis point locally while still\n",
    "# reusing the module-defined defaults. This demonstrates how to extend the\n",
    "# external analysis_points module from within the notebook.\n",
    "def ap_setup_attribution_end(local_vars: Dict[str, Any]) -> None:\n",
    "    data: Dict[str, Any] = {}\n",
    "    # Collect shapes from ctx attributes with descriptions\n",
    "    collect_shapes(\n",
    "        data,\n",
    "        local_vars,\n",
    "        [\n",
    "            VarAnnotate(\"ctx.activation_matrix\", \"n_layers, n_pos, d_transcoder\"),\n",
    "            VarAnnotate(\"ctx.decoder_vecs\", \"num_active_features, d_model\"),\n",
    "            VarAnnotate(\"ctx.encoder_vecs\", \"num_active_features, d_model\"),\n",
    "            VarAnnotate(\"ctx.logits\", \"n_examples (usually 1), n_pos, d_vocab\"),\n",
    "            VarAnnotate(\"ctx.token_vectors\", \"n_pos, d_model\"),\n",
    "            VarAnnotate(\"ctx.error_vectors\", \"n_layers, n_pos, d_model\"),\n",
    "            VarAnnotate(\"ctx.encoder_to_decoder_map\", \"num_active_features\"),\n",
    "            VarAnnotate(\n",
    "                \"ctx.decoder_locations\",\n",
    "                \"dims activation_matrix, num_active_features (sparse indices into activation_matrix)\",\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "    ctx = local_vars.get(\"ctx\")\n",
    "    # Add non-shape attributes\n",
    "    data[\"n_layers\"] = getattr(ctx, \"n_layers\", None)\n",
    "    data[\"_row_size\"] = VarAnnotate(\n",
    "        \"ctx._row_size\", ctx._row_size, \"total_active_feats + error_nodes ((n_layers + 1) * n_pos)  # + logits later\"\n",
    "    )\n",
    "    analysis_log_point(\"AttributionContext summary after precomputing activations and vectors\", data)\n",
    "\n",
    "\n",
    "NOTEBOOK_ANALYSIS_FUNCTIONS = {\"ap_setup_attribution_end\": ap_setup_attribution_end}\n",
    "print(\"Registered notebook override analysis point: ap_setup_attribution_end\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define/Customize Analysis Injection Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can customize the base config (`analysis_injection_config.yaml` in this case) with overrides to tweak settings and\n",
    "# manipulate hook definitions declaratively.\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure the log directory is reachable for this session.\n",
    "target_log_dir = Path(core_log_dir).expanduser() if core_log_dir else Path(tempfile.gettempdir())\n",
    "target_log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Demonstrate notebook-based overrides: add (or replace) the\n",
    "# `ap_setup_attribution_end` hook definition directly via config overrides.\n",
    "analysis_config_overrides = f\"\"\"\n",
    "settings:\n",
    "  log_dir: {target_log_dir.as_posix()}\n",
    "\n",
    "file_hooks:\n",
    "  ap_setup_attribution_end:\n",
    "    file_path: attribution/attribute.py\n",
    "    enable: true\n",
    "    regex_pattern: '^\\\\s*ctx\\\\s*=\\\\s*model\\\\.setup_attribution'\n",
    "    insert_after: true\n",
    "    description: \"AttributionContext summary at end of phase 0 (added from notebook)\"\n",
    "\"\"\"\n",
    "\n",
    "print(\"Configured log directory:\", target_log_dir.as_posix())\n",
    "print(\"Prepared analysis injection config overrides:\")\n",
    "print(analysis_config_overrides)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate Our Analysis Injector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis Injection ‚Äî Setup\n",
    "# The orchestrator loads the configured analysis point module automatically. We only pass the\n",
    "# additional notebook-defined hooks via `analysis_functions`.\n",
    "# Centralized setup: optional env_path (default None), uses the base config from NOTEBOOK_DIR.\n",
    "from it_examples.utils.example_helpers import required_os_env\n",
    "\n",
    "# Optional: the user can set `env_path` to a specific .env file path before running this cell.\n",
    "# If left as None, load_dotenv() will be called without a path so it can auto-discover the .env file.\n",
    "env_path: str | None = None  # set to '/full/path/to/.env' to override\n",
    "\n",
    "if enable_analysis_injection:\n",
    "    # Load environment variables. If env_path is provided, use it; otherwise let load_dotenv auto-discover.\n",
    "    os_env_reqs = None\n",
    "    assert required_os_env(env_path=env_path, env_reqs=os_env_reqs)\n",
    "\n",
    "    # Import orchestrator from the analysis_injection package\n",
    "    from it_examples.utils.analysis_injection import orchestrator\n",
    "\n",
    "    print(\"Setting up analysis injection using base config:\", base_config_path)\n",
    "\n",
    "    # Create the orchestrator which performs the patching. The simplified API handles validation,\n",
    "    # module loading, and registration automatically.\n",
    "    analysis_injector = orchestrator.setup_analysis_injection(\n",
    "        config_path=base_config_path,\n",
    "        target_package=\"circuit_tracer\",\n",
    "        config_overrides=analysis_config_overrides,\n",
    "        analysis_functions=NOTEBOOK_ANALYSIS_FUNCTIONS,\n",
    "    )\n",
    "\n",
    "    print(\"Analysis injection ready. Active patched modules:\")\n",
    "    if analysis_injector.patched_modules:\n",
    "        for module_name in analysis_injector.patched_modules.keys():\n",
    "            print(f\"  - {module_name}\")\n",
    "    else:\n",
    "        print(\"  (No modules patched; check configuration)\")\n",
    "\n",
    "    print(\"Hook registry status:\")\n",
    "    print(f\"  Enabled: {orchestrator.HOOK_REGISTRY._enabled}\")\n",
    "    print(f\"  Registered hooks: {len(orchestrator.HOOK_REGISTRY._hooks)}\")\n",
    "\n",
    "    print(\"\\nYou can inspect collected analysis data via orchestrator.get_analysis_data().\")\n",
    "else:\n",
    "    print(\"Analysis injection disabled via parameters; skipping setup.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure our IT Session\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define or customize our session configuration, which includes:\n",
    "1. Experiment/task module and datamodule (in this case, 'rte' for the RTE task) \n",
    "    * We can customize any module, datamodule, or adapter-specific configuration options we want to use. In this case, we set target `circuit_tracer_cfg` that we want to use for our analysis. We also could customize generation parameters, tokenization, the pretrained/config-based model we want to use (in this case, GPT2) etc.\n",
    "2. The adapter context we want to use. In this case, `core` PyTorch (vs e.g. Lightning) and `circuit_tracer` (vs e.g. `transformer_lens` or `sae_lens`). \n",
    "\n",
    "When an `ITSession` is created, the selected adapter context will trigger composition of the relevant adapters with our experiment/task module and datamodule. The intention of this abstraction is to enable the same experiment/task logic to be used unchanged across a broad variety of PyTorch framework and analytical package contexts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Load our demo config (this will be done from the hub once that is available)\n",
    "base_itdm_cfg, base_it_cfg, dm_cls, m_cls = MODULE_EXAMPLE_REGISTRY.get(\"gemma2.rte_demo.circuit_tracer\")\n",
    "# Optionally override base_it_cfg.core_log_dir with the notebook parameter if provided\n",
    "if core_log_dir:\n",
    "    base_it_cfg.core_log_dir = core_log_dir\n",
    "# If the user requests the baseline salient-logits path, clear the explicit target\n",
    "# token configuration so the adapter will fall back to the default compute_salient_logits\n",
    "# implementation. This forces usage of the baseline salient-logits computation.\n",
    "if use_baseline_salient_logits:\n",
    "    # Clear any explicit token selection so compute_salient_logits() runs its default path\n",
    "    base_it_cfg.circuit_tracer_cfg.analysis_target_tokens = None\n",
    "    base_it_cfg.circuit_tracer_cfg.target_token_ids = None\n",
    "    print(\n",
    "        \"use_baseline_salient_logits=True: cleared analysis_target_tokens and \"\n",
    "        \"target_token_ids -> using default compute_salient_logits path\"\n",
    "    )\n",
    "else:\n",
    "    print(\"use_baseline_salient_logits=False: keeping configured analysis_target_tokens / target_token_ids (if any)\")\n",
    "\n",
    "if enable_analysis_injection:\n",
    "    try:\n",
    "        base_it_cfg.circuit_tracer_cfg.verbose = False\n",
    "        print(\"‚úì Analysis injection enabled: disabled circuit_tracer verbose logging to avoid duplicate logs\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not disable circuit_tracer verbose logging: {e}\")\n",
    "\n",
    "\n",
    "# Configure transcoder architecture selection based on the toggle. When True, use the 'gemma'\n",
    "# CrossLayerTranscoder (demo). When False, point to the HF SingleLayerTranscoder checkpoint URL.\n",
    "if use_baseline_transcoder_arch:\n",
    "    base_it_cfg.circuit_tracer_cfg.transcoder_set = \"gemma\"\n",
    "    print(\n",
    "        \"use_baseline_transcoder_arch=True: set transcoder_set='gemma' -> \"\n",
    "        \"set transcoder_set to HF URL -> using the SingleLayerTranscoder checkpoint\"\n",
    "    )\n",
    "else:\n",
    "    base_it_cfg.circuit_tracer_cfg.transcoder_set = \"mntss/clt-gemma-2-2b-426k\"\n",
    "    print(\n",
    "        \"use_baseline_transcoder_arch=False: demo will use CrossLayerTranscoder \"\n",
    "        \"instead of the default TranscoderSet of `SingleLayerTranscoder`s\"\n",
    "    )\n",
    "\n",
    "print(pformat(base_it_cfg.circuit_tracer_cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# configure our session with our desired adapter composition, core and circuit_tracer in this case\n",
    "session_cfg = ITSessionConfig(\n",
    "    adapter_ctx=(it.Adapter.core, it.Adapter.circuit_tracer),\n",
    "    datamodule_cfg=base_itdm_cfg,\n",
    "    module_cfg=base_it_cfg,\n",
    "    datamodule_cls=dm_cls,\n",
    "    module_cls=m_cls,\n",
    ")\n",
    "\n",
    "# start our session\n",
    "it_session = ITSession(session_cfg)\n",
    "print(\"\\nIT Session created successfully!\")\n",
    "\n",
    "# manual init for now\n",
    "it_init(**it_session)\n",
    "print(\"\\nIT Session initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "limit_analysis_batches = 1\n",
    "test_token_limit = -1\n",
    "force_manual_debug_prompts = True  # Set to True to use manual debug prompts instead of random samples\n",
    "# specific tokens to analyze, will use tokens associated with top `max_n_logits` if `None`\n",
    "# analysis_target_tokens: Optional[torch.Tensor] = None\n",
    "\n",
    "example_prompts = []\n",
    "ct_module = it_session.module\n",
    "if not force_manual_debug_prompts:\n",
    "    dataloader = it_session.datamodule.test_dataloader()\n",
    "    for epoch_idx in range(1):  # Run for a single epoch for simplicity\n",
    "        ct_module.current_epoch = epoch_idx\n",
    "        for batch_idx, batch in tqdm(enumerate(dataloader)):\n",
    "            if batch_idx >= limit_analysis_batches >= 0:\n",
    "                break\n",
    "            # fetch the first test_token_limit from the first example in the batch\n",
    "            first_ex_in_batch = batch[:1]\n",
    "            first_ex_in_batch = first_ex_in_batch[\"input\"]\n",
    "            first_ex_in_batch.squeeze_()\n",
    "            if test_token_limit > 0:\n",
    "                first_ex_in_batch = first_ex_in_batch[-test_token_limit:]\n",
    "            first_ex_in_batch = first_ex_in_batch[first_ex_in_batch != 0]\n",
    "            example_prompts.append(first_ex_in_batch)\n",
    "else:\n",
    "    # Generate attribution graphs for a few example prompts\n",
    "    example_prompts = [\n",
    "        # \"The capital of France is\",\n",
    "        \"The capital of the state containing Dallas is\",\n",
    "        # \"When I look at the sky, I see\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set tokenizer context for analysis (if hooks are enabled)\n",
    "if enable_analysis_injection and analysis_injector:\n",
    "    from it_examples.utils.analysis_injection.analysis_hook_patcher import HOOK_REGISTRY\n",
    "    from it_examples.utils.example_helpers import TargetTokenAnalysis\n",
    "\n",
    "    # Convert target_tokens to IDs using the model's tokenizer\n",
    "    if analysis_injector.config.shared_context[\"target_tokens\"]:\n",
    "        target_tokens = analysis_injector.config.shared_context[\"target_tokens\"]\n",
    "        target_token_analysis = TargetTokenAnalysis(\n",
    "            tokens=target_tokens, tokenizer=ct_module.model.tokenizer, default_device=ct_module.device\n",
    "        )\n",
    "\n",
    "        HOOK_REGISTRY.set_context(\n",
    "            target_token_ids=target_token_analysis.token_ids, target_token_analysis=target_token_analysis\n",
    "        )\n",
    "        print(f\"‚úì Target tokens set: {target_token_analysis.tokens} ‚Üí IDs: {target_token_analysis.token_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Basic Attribution Graph\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating attribution graphs for example prompts...\")\n",
    "slug_base = \"it_circuit_tracer_compute_specific_logits_demo\"\n",
    "results = []\n",
    "\n",
    "for i, prompt in enumerate(example_prompts):\n",
    "    print(f\"\\nProcessing prompt {i + 1}: '{prompt}'\")\n",
    "    slug = f\"{slug_base}_{i + 1}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    # Process the batch using the session, the adapter will handle tokenization and graph generation\n",
    "    try:\n",
    "        graph, local_graph_path, _ = ct_module.generate_graph(prompt=prompt, slug=slug)\n",
    "        results.append(local_graph_path)\n",
    "    except Exception as e:\n",
    "        print(f\"  - Error processing prompt: {e}\")\n",
    "\n",
    "print(f\"\\nProcessed {len(results)} prompts successfully\")\n",
    "# Check and display analysis log file location if available\n",
    "if enable_analysis_injection and analysis_injector.analysis_log:\n",
    "    print(f\"üìù Analysis log available for inspection: {analysis_injector.analysis_log}\")\n",
    "    print(\n",
    "        \"   The subsequent cells in the `Annotated Attribution Flow Analysis` will display key analysis points values\"\n",
    "        \" with any associated annotations in context. You can also inspect the file above for the raw analysis point\"\n",
    "        \" values and any additional debug information collected during graph generation.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotated Attribution Flow Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When proceeding with the subsequent annotated attribution analysis, it may be helpful to refer to the following diagram outlining the `transformer_lens` hook architecture and nomenclature (might want to click [here](https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/transformer-full-updated.png) to open it in a new tab):\n",
    "\n",
    "<details>\n",
    "<summary>Expand Diagram</summary>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/transformer-full-updated.png\" alt=\"Transformer diagram\" width=\"90%\"/>\n",
    "</details>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReplacementModel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module is common for `TranscoderSet` (set of `SingleLayerTranscoders`) and `CrossLayerTranscoder` transcoder architectures\n",
    "<details>\n",
    "  <summary>ReplacementModel</summary>\n",
    "\n",
    "  ```python\n",
    "    ReplacementModel(\n",
    "      (embed): Embed()\n",
    "      (hook_embed): HookPoint()\n",
    "      (blocks): ModuleList((0-25): 26 x TransformerBlock())  # see `TransformerBlock`\n",
    "      (ln_final): RMSNorm(\n",
    "        (hook_scale): HookPoint()\n",
    "        (hook_normalized): HookPoint()\n",
    "      )\n",
    "      (unembed): ReplacementUnembed(\n",
    "        (old_unembed): Unembed()\n",
    "        (hook_pre): HookPoint()\n",
    "        (hook_post): HookPoint()\n",
    "      )\n",
    "      (transcoders): ... # see relevant transcoder architecture below (e.g. `TranscoderSet` or `CrossLayerTranscoder`)\n",
    "    )\n",
    "  ```\n",
    "</details>\n",
    "<br/>\n",
    "\n",
    "- `TransformerBlock` is a TransformerLens transformer block with additional hooks and a replacement MLP \n",
    "  <details>\n",
    "    <summary>TransformerBlock</summary>\n",
    "\n",
    "    ```python\n",
    "    (ln1): RMSNorm(\n",
    "      (hook_scale): HookPoint()\n",
    "      (hook_normalized): HookPoint()\n",
    "    )\n",
    "    (ln1_post): RMSNorm(\n",
    "      (hook_scale): HookPoint()\n",
    "      (hook_normalized): HookPoint()\n",
    "    )\n",
    "    (ln2): RMSNorm(\n",
    "      (hook_scale): HookPoint()\n",
    "      (hook_normalized): HookPoint()\n",
    "    )\n",
    "    (ln2_post): RMSNorm(\n",
    "      (hook_scale): HookPoint()\n",
    "      (hook_normalized): HookPoint()\n",
    "    )\n",
    "    (attn): GroupedQueryAttention(\n",
    "      (hook_k): HookPoint()\n",
    "      (hook_q): HookPoint()\n",
    "      (hook_v): HookPoint()\n",
    "      (hook_z): HookPoint()\n",
    "      (hook_attn_scores): HookPoint()\n",
    "      (hook_pattern): HookPoint()\n",
    "      (hook_result): HookPoint()\n",
    "      (hook_rot_k): HookPoint()\n",
    "      (hook_rot_q): HookPoint()\n",
    "    )\n",
    "    (mlp): ReplacementMLP(\n",
    "      (old_mlp): GatedMLP(\n",
    "        (hook_pre): HookPoint()\n",
    "        (hook_pre_linear): HookPoint()\n",
    "        (hook_post): HookPoint()\n",
    "      )\n",
    "      (hook_in): HookPoint()\n",
    "      (hook_out): HookPoint()\n",
    "    )\n",
    "    (hook_attn_in): HookPoint()\n",
    "    (hook_q_input): HookPoint()\n",
    "    (hook_k_input): HookPoint()\n",
    "    (hook_v_input): HookPoint()\n",
    "    (hook_mlp_in): HookPoint()\n",
    "    (hook_attn_out): HookPoint()\n",
    "    (hook_mlp_out): HookPoint(\n",
    "      (hook_out_grad): HookPoint()\n",
    "    )\n",
    "    (hook_resid_pre): HookPoint()\n",
    "    (hook_resid_mid): HookPoint()\n",
    "    (hook_resid_post): HookPoint()\n",
    "  ```\n",
    "  </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transcoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When using a `TranscoderSet` set of `SingleLayerTranscoders` as the transcoder architecture:\n",
    "\n",
    "  <details>\n",
    "    <summary>TranscoderSet</summary>\n",
    "\n",
    "    ```python\n",
    "        (transcoders): TranscoderSet(\n",
    "          (transcoders): ModuleList(\n",
    "            (0): SingleLayerTranscoder(\n",
    "              (activation_function): JumpReLU(\n",
    "                threshold=Parameter containing:\n",
    "                tensor(0.5664, device='cuda:0', dtype=torch.bfloat16), bandwidth=0.1\n",
    "              )\n",
    "            )\n",
    "            ...\n",
    "            (25): SingleLayerTranscoder(\n",
    "              (activation_function): JumpReLU(\n",
    "                threshold=Parameter containing:\n",
    "                tensor(6.1250, device='cuda:0', dtype=torch.bfloat16), bandwidth=0.1\n",
    "              )\n",
    "            )\n",
    "          )\n",
    "        )\n",
    "    ```\n",
    "  </details>\n",
    "  <br/>\n",
    "- When using a `CrossLayerTranscoder` as the transcoder architecture:\n",
    "  ```python\n",
    "    (transcoders): CrossLayerTranscoder()\n",
    "  ```\n",
    "\n",
    "  <details>\n",
    "    <summary>Note on Lazy Decoders</summary>\n",
    "\n",
    "    - `lazy_decoders` is by default `True` so to get W_dec (without excessive memory demands) we log W_dec manually from within `compute_attribution_components`\n",
    "      ```python\n",
    "      {n:p.shape for n,p in model.transcoders.named_parameters()}\n",
    "      {'W_enc': torch.Size([26, 16384, 2304]), 'b_dec': torch.Size([26, 2304]), 'b_enc': torch.Size([26, 16384])}\n",
    "      ```\n",
    "    - notice since each CLT feature has a single encoder weight but decoder weights that output to each subqeuent layer MLP output, our W_dec are shaped accordingly, `n_output_layers = self.n_layers - layer_id`\n",
    "    - the W_dec then is shaped `(d_transcoder, n_output_layers, d_model)`\n",
    "    - in addition to lazily loading decoder weights on demand to save memory, only the needed feature ids are loaded as well\n",
    "      ```python\n",
    "      w_dec_shapes = {}\n",
    "      for l in range(self.n_layers):\n",
    "          path = os.path.join(self.clt_path, f\"W_dec_{l}.safetensors\")\n",
    "          with safe_open(path, framework=\"pt\", device=self.device.type) as f:\n",
    "              w_dec_shapes[l] = f.get_slice(f\"W_dec_{l}\")[:].to(device).to(dtype).shape\n",
    "      print(w_dec_shapes)\n",
    "      {\n",
    "      0: torch.Size([16384, 26, 2304]), \n",
    "      1: torch.Size([16384, 25, 2304]), \n",
    "      ...\n",
    "      24: torch.Size([16384, 2, 2304]), \n",
    "      25: torch.Size([16384, 1, 2304])\n",
    "      }\n",
    "      ```\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precompute Activations, Setup Hooks and Generate `AttributionContext`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attribution Hooks Setup and Activation Precomputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- This phase precomputes the ReplacementModel and transcoder activations as well as the error vectors, saving them and the token embeddings.\n",
    "```python\n",
    "    ctx = model.setup_attribution(input_ids)\n",
    "```\n",
    "\n",
    "- `get_caching_hooks` is used to get mlp in and out caching hooks\n",
    "\n",
    "    ```python\n",
    "            mlp_in_cache, mlp_in_caching_hooks, _ = self.get_caching_hooks(\n",
    "                lambda name: self.feature_input_hook in name\n",
    "            ) \n",
    "            mlp_out_cache, mlp_out_caching_hooks, _ = self.get_caching_hooks(\n",
    "                lambda name: self.feature_output_hook in name\n",
    "            )\n",
    "    ```\n",
    "- `get_caching_hooks`: normal TL caching hook for getting targeted mlp activations\n",
    "\n",
    "- `run_with_hooks` is called with just the mlp in and out caching hooks\n",
    "\n",
    "    - this collects the original model's activations and mlp in and out hook points (without using the trained transcoders)\n",
    "    ```python\n",
    "    logits = self.run_with_hooks(tokens, fwd_hooks=mlp_in_caching_hooks + mlp_out_caching_hooks)\n",
    "    ```\n",
    "- Note `ReplacementModel` when configured adds a `hook_out_grad` `HookPoint` to the subblock that handles the skip connection after mlp_out and enables hooking into the gradients of the function (that wouldn't be possible with backward since the acts are detached)\n",
    "- the `hook_out_grad` hook is important as it is the output of the mlp put to the residual (after including the skip connection), used for lots of subsequent computation (error, scores)\n",
    "    - e.g. when error is calculated below using `mlp_out_cache`, it uses the special `hook_out_grad` that was added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transcoder Architecture-Specific Attribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `compute_attribution_components` is called to collect all attribution_data required for `AttributionContext`\n",
    "- this method is transcoder type specific, single layer transcoders form a `TranscoderSet` that have this method, `CrossLayerTranscoders` have a different version\n",
    "```python\n",
    "    attribution_data = self.transcoders.compute_attribution_components(mlp_in_cache)\n",
    "```\n",
    "- we construct the `activation_matrix` using `compute_attribution_components` which allows multiple different transcoder architectures to be used\n",
    "\n",
    "- For `TranscoderSet` (set of `SingleLayerTranscoder`)\n",
    "\n",
    "    <details>\n",
    "    <summary>TranscoderSet Attribution Context</summary><br/>\n",
    "\n",
    "    - note that this implementation is for the `SingleLayerTranscoders` not cross-layer transcoders. CLTs have same number of encoder parameters but `num_layers/2 `times more decoder parameters and have separate decoder vectors for each subsequent layer. We can see the current reconstruction uses just the corresponding input to that layer and collects that SLT's output. See the `CrossLayerTranscoder` version of `compute_attribution_components` for CLT mechanics\n",
    "\n",
    "    - we construct per layer sparse activations (zeroing out bos activations also) using our trained transcoders (remember ReplacementMLP is the original model with specially instrumented MLP hooks for inspection/replacement of activations etc)\n",
    "\n",
    "    - here is where the reconstruction is calculated by passing in the captured/cached mlp_input (mlp_in_cache) to the relevant trained transcoders \n",
    "\n",
    "    - `compute_attribution_components` uses `encode_sparse` and `decode_sparse` methods on each transcoder layer to collect our required attribution context and package it in a `AttributionContext` dataclass:\n",
    "        - `activation_matrix`: Sparse (n_layers, n_pos, d_transcoder) activations\n",
    "        - `reconstruction`: (n_layers, n_pos, d_model) reconstructed outputs\n",
    "        - `encoder_vecs`: Concatenated encoder vectors for active features\n",
    "        - `decoder_vecs`: Concatenated decoder vectors (scaled by activations)\n",
    "        - `encoder_to_decoder_map`: Mapping from encoder to decoder indices\n",
    "\n",
    "        - `encode_sparse`\n",
    "            - accepts incoming activations (for SLT, feature_input_hook is `ln2.hook_normalized`) and uses the current layer's self.W_enc and b_enc to calc pre_acts and the activation function to calc the acts, sets the bos acts to 0 and calculates the active (nnz, indices()) encoders\n",
    "            - these are the local replacement model transcoder preactivations\n",
    "            - gets non-zero indices, getting the trained transcoder encoder layers (transposed) for active features\n",
    "            - **NOTE**: crucially, these target transcoder feature preactivations are linear in each upstream source transcoder feature activations since we freeze attention patterns and normalization denominators!\n",
    "            - example sparse_acts, active_encoder shapes for gemma layer 0:\n",
    "            ```python\n",
    "            sparse_acts.shape  \n",
    "            torch.Size([9, 16384])  # n_pos, d_transcoder\n",
    "            active_encoders.shape\n",
    "            torch.Size([634, 2304]) # num_active_features for the layer, d_model\n",
    "            ```\n",
    "        - `decode_sparse`\n",
    "            - accepts the decoded output activations and scales the relevant W_dec indices by the activations\n",
    "            - Return decoder rows for **active** features only from the trained transcoders \n",
    "            - uses `indices()` to get non-zero indices so requires sparse tensor\n",
    "            - for each active feature index for each layer, creates a [n_active_features_for_layer, d_model] tensor\n",
    "            - example shapes for gemma layer 0\n",
    "                ```python\n",
    "                W_dec.shape\n",
    "                torch.Size([16384, 2304])\n",
    "                transcoders[layer].W_dec[feat_idx].shape\n",
    "                torch.Size([634, 2304])\n",
    "                ```\n",
    "    </details>\n",
    "    <br/>\n",
    "\n",
    "- For `CrossLayerTranscoder` Attribution Context\n",
    "\n",
    "    <details>\n",
    "    <summary>CrossLayerTranscoder Attribution Context</summary><br/>\n",
    "    \n",
    "    - a crucial difference between SLT and CLT attribution flows is that for SLT, `decode_sparse` returns a per-layer reconstruction, whereas for CLT, a separate `compute_reconstruction` step is required to calculate and store the reconstruction separately for each subsequent layer it outputs to (since each layer has a separate decoder for each subsequent layer)\n",
    "    - encode_sparse\n",
    "    ```python\n",
    "        sparse_acts, active_encoders = transcoder.encode_sparse(mlp_inputs[layer], zero_first_pos=True)\n",
    "    ```\n",
    "    - select_decoder_vectors\n",
    "    ```python\n",
    "            pos_ids, layer_ids, feat_ids, decoder_vectors, encoder_to_decoder_map = (\n",
    "                self.select_decoder_vectors(features)\n",
    "        )\n",
    "    ```\n",
    "    - compute_reconstruction\n",
    "    ```python\n",
    "        reconstruction = self.compute_reconstruction(pos_ids, layer_ids, decoder_vectors)    \n",
    "    ```\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis Point Data**: See below sampled data for the current attribution example at the end of Transcoder-Specific attribution described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "analysis_injector.get_output(\"ap_compute_attribution_end\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error Vectors and Reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finally, we then compute the error vectors, the actual (replacement)MLP layers, the mlp_out_cache are the original acts and the reconstructed-from transcoder acts are the reconstruction\n",
    "    ```python\n",
    "    error_vectors = mlp_out_cache - attribution_data[\"reconstruction\"]\n",
    "    ```\n",
    "- we also save the token vector positions\n",
    "    ```python\n",
    "    token_vectors = self.W_E[tokens].detach()  # (n_pos, d_model)\n",
    "    ```\n",
    "- all the per-layer active feature decoder vectors are scaled by how much each transcoder feature was activated \n",
    "- at the end of setup_attribution we, package the attribution components into an `AttributionContext`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Analysis Point Data**: See below sampled data for the current attribution example at the end of `Precompute Activations, Setup Hooks and Generate AttributionContext`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "analysis_injector.get_output(\"ap_setup_attribution_end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "analysis_injector.get_output(\"ap_precomputation_phase_end\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When we run the forward pass, we do so under the `install_hooks` context manager of the `AttributionContext` object we created above:\n",
    "\n",
    "    ```python\n",
    "        def install_hooks(self, model: \"ReplacementModel\"):\n",
    "            \"\"\"Context manager instruments the hooks for the forward and backward passes.\"\"\"\n",
    "            with model.hooks(\n",
    "                fwd_hooks=self._caching_hooks(model.feature_input_hook),  # type: ignore\n",
    "                bwd_hooks=self._make_attribution_hooks(model.feature_output_hook),  # type: ignore\n",
    "            ):\n",
    "                yield\n",
    "    ```\n",
    "- `AttributionContext._caching_hooks` `fwd` hooks are installed\n",
    "\n",
    "    - these cache the layerwise residual activations for the replacement model in `AttributionContext._resid_activations` (as well as the `unembed.hook_pre` acts after the last layer)\n",
    "    - for this gemma example, the hook currently used for SLT is the feature_input_hook `ln2.hook_normalized` and for CLT `hook_resid_mid` (so the input for the MLP/transcoder)\n",
    "\n",
    "- `AttributionContext._make_attribution_hooks` `bwd` hooks are installed\n",
    "    - these are the `bwd` hooks installed below via `_make_attribution_hooks`\n",
    "    - a `model.forward` is run (stopping at the last layer), note the input is expanded to the `batch_size` num of nodes to process per position, so with context size 9 and a batch_size of 256 for this example, our input would be `torch.Size([256, 9])`\n",
    "    - the final residual activations are set on the context manager `ctx._resid_activations[-1] = model.ln_final(residual)`\n",
    "    - after the forward, the `ReplacementModel` MLP block modules can be offloaded (all original module MLP blocks `[block.mlp for block in model.blocks]`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Key Bwd Hooks: `_make_attribution_hooks`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `_make_attribution_hooks` is a function that associates the bwd attribution hook factory function `_compute_score_hook` with the feature/logit, error and token node types. Inside the TransformerLens `hooks` context manager, these hooks are enabled and installed during the forward pass in this phase (phase 1)\n",
    "- These hooks are subsequently used for both logit attribution (Phase 3) and feature attribution (Phase 4)\n",
    "    - for phase 3, the gradient of the pre-softmax logit (minus the mean logit) is injected as the demeaned logits are passed to `compute_batch` and `backward` is run while injecting the relevant gradients to einsum with the source decoder vectors\n",
    "    - for phase 4, the upstream target encoder vectors are similarly injected (see `key attribution gradient flow summary` below)\n",
    "    - see `compute_batch` below for the mechanics of using backward hooks to orchestrate logit/feature/error/token node attribution\n",
    "\n",
    "- Attribution Hook Details\n",
    "    <details>\n",
    "    <summary>Node Attribution Hook Construction</summary><br/>\n",
    "\n",
    "    - the feature/logit node attribution bwd hooks are constructed:\n",
    "        1. layerwise using active features (non-zero activations) from the  precomputed `activation_matrix` sparse indices\n",
    "            ```python\n",
    "            nnz_layers, nnz_positions = self.decoder_locations\n",
    "            ```\n",
    "            - `nnz_layers, nnz_positions` are the non-zero indices for layers and positions dimensions of `activation_matrix.indices()`\n",
    "            - for this gemma example, layer 25 has 265 active features distributed across all 8 positions (position 0 is bos so is zeroed out), e.g. 31 features active at position 8:\n",
    "                ```python\n",
    "                nnz_layers[-265:].unique()\n",
    "                tensor([25], device='cuda:0')\n",
    "                nnz_positions[-265:].unique()\n",
    "                tensor([1, 2, 3, 4, 5, 6, 7, 8], device='cuda:0')\n",
    "                nnz_positions[-31:].unique()\n",
    "                tensor([8], device='cuda:0')\n",
    "                ```\n",
    "        2. by passing in the appropriate feature decoder vectors (from the trained transcoders)\n",
    "            ```python\n",
    "            # Feature nodes\n",
    "            feature_hooks = [\n",
    "                self._compute_score_hook(\n",
    "                    f\"blocks.{layer}.{feature_output_hook}\",\n",
    "                    self.decoder_vecs[layer_mask],\n",
    "                    write_index=self.encoder_to_decoder_map[layer_mask],  # type: ignore\n",
    "                    read_index=np.s_[:, nnz_positions[layer_mask]],  # type: ignore\n",
    "                )\n",
    "                for layer in range(n_layers)\n",
    "                if (layer_mask := nnz_layers == layer).any()\n",
    "            ]\n",
    "            ```\n",
    "        - The `np.s_` indexing functionality is used to construct a slice that can be used to select the appropriate \"gradient\" vectors from the injected `logit_vecs` in our backward pass.\n",
    "            - All rows in the first dimension (the batch dimension, in this case our `logit_vecs` batch where only 10 of the 256 rows will be active) are selected.\n",
    "            - For the second dim, the residual activation position indices that are associated with non-zero feature activations in our `activation_matrix` for each layer. \n",
    "        - This will result in the appropriate `logit_vec` \"grads\" vectors getting selected repeatedly for the appropriate positions that have active features for that layer.\n",
    "        - Concretely, for layer 25 in our gemma SLT example, we see the `logit_vec` injected \"grads\" we einsum with our `output_vecs` `decoder_vecs` slice (265 active decoder vecs associated with features for that layer) for the given layer results in the desired shapes for our einsum. Each target `logit_vec` will be repeated 31 times for position 8 so the corresponding 31 active feature decoder vectors for position 8 will be einsum'd with them and written to the appropriate positions in our attribution score tensor:\n",
    "            ```python\n",
    "            # these are the pos indices associated with the active features for layer 25\n",
    "            read_index[1]\n",
    "            tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,\n",
    "                    2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
    "                    2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
    "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
    "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
    "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
    "                    5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7,\n",
    "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8,\n",
    "                    8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
    "                    8], device='cuda:0')\n",
    "            read_index[1].shape\n",
    "            torch.Size([265])\n",
    "\n",
    "            write_index.shape  # corresponding active feature decoder vec indices\n",
    "            torch.Size([265])\n",
    "            write_index.min()\n",
    "            tensor(6850, device='cuda:0')\n",
    "            write_index.max()\n",
    "            tensor(7114, device='cuda:0')\n",
    "            \n",
    "            grads.shape\n",
    "            torch.Size([256, 9, 2304])  # batch_size, n_pos, d_model\n",
    "            grads.to(output_vecs.dtype)[read_index].shape\n",
    "            torch.Size([256, 265, 2304])  # batch_size, num active position/feature combos for layer, d_model\n",
    "            \n",
    "            output_vecs.shape\n",
    "            torch.Size([265, 2304])  # num active position/feature combos for layer, d_model\n",
    "            ```\n",
    "\n",
    "    - the error and token node hooks are constructed similarly, except:\n",
    "        1. an `error_offset` function is used to calculate the appropriate node indices\n",
    "            ```python\n",
    "            def error_offset(layer: int) -> int:  # starting row for this layer\n",
    "                return self.activation_matrix._nnz() + layer * n_pos\n",
    "            ```\n",
    "        2. instead of binding the scaled decoder vectors to the hooks, the error and token node hooks use the precomputed `error_vectors` and `token_vectors` respectively\"\n",
    "            ```python\n",
    "            error_hooks = [\n",
    "                self._compute_score_hook(\n",
    "                    f\"blocks.{layer}.{feature_output_hook}\",\n",
    "                    self.error_vectors[layer],\n",
    "                    write_index=np.s_[error_offset(layer) : error_offset(layer + 1)],\n",
    "                )\n",
    "                for layer in range(n_layers)\n",
    "            ]\n",
    "\n",
    "            # Token-embedding nodes\n",
    "            tok_start = error_offset(n_layers)\n",
    "            token_hook = [\n",
    "                self._compute_score_hook(\n",
    "                    \"hook_embed\",\n",
    "                    self.token_vectors,\n",
    "                    write_index=np.s_[tok_start : tok_start + n_pos],\n",
    "                )\n",
    "            ]\n",
    "            ```\n",
    "\n",
    "    - The actual bwd hook binds our `AttributionContext` ref to form the closure we use to output the attribution scores to the correct rows in our attribution score edge matrix. The scores are buffered on a per-batch basis, each batch calculating the source attribution scores for all nodes w.r.t. the current batch size of target nodes:\n",
    "\n",
    "        ```python\n",
    "            def _compute_score_hook(\n",
    "                self,\n",
    "                hook_name: str,\n",
    "                output_vecs: torch.Tensor,\n",
    "                write_index: slice,\n",
    "                read_index: slice | np.ndarray = np.s_[:],\n",
    "            ) -> tuple[str, Callable]:\n",
    "                \"\"\"\n",
    "                Factory that contracts *gradients* with an **output vector set**.\n",
    "                The hook computes A_{s->t} and writes the result into an in-place buffer row.\n",
    "                \"\"\"\n",
    "\n",
    "                proxy = weakref.proxy(self)\n",
    "\n",
    "                def _hook_fn(grads: torch.Tensor, hook: HookPoint) -> None:\n",
    "                    proxy._batch_buffer[write_index] += einsum(\n",
    "                        grads.to(output_vecs.dtype)[read_index],\n",
    "                        output_vecs,\n",
    "                        \"batch position d_model, position d_model -> position batch\",\n",
    "                    )\n",
    "\n",
    "                return hook_name, _hook_fn\n",
    "        ```\n",
    "    </details>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis Point Data**: See below sampled data for the current attribution example at the end of `Forward Pass`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "analysis_injector.get_output(\"ap_forward_pass_end\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Input Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter active features of our cached transcoder activation_matrix and depending on our target logits mode, generate our feature matrices we want to use in subsequent analysis by using either the:\n",
    "\n",
    "1. configured cumulative probability and `max_n_logits` if in the baseline default `compute_salient_logits` target logits mode\n",
    "2. specific logits specified by token id or token if we use our `compute_specific_logits` target logits mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis Point Data**: See below sampled data for the current attribution example at the end of `Build Input Vectors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "analysis_injector.get_output(\"ap_build_input_vectors_end\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core `compute_batch` Attribution Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `Compute Logit Attributions` and `Compute Feature Attributions` phases, it's important to note we aren't using conventional gradient propagation but rather using `backward()` as a convenient orchestration mechanism to calculate our desired node attributions via custom gradient injection.\n",
    "\n",
    "The core mechanics of this are:\n",
    "\n",
    "1. The `compute_batch` method of `AttributionContext` which is shared among both the Logit and Feature Node Attribution phases: \n",
    "    <details>\n",
    "    <summary>Snapshot of `compute_batch`</summary><br/>\n",
    "\n",
    "    ```python\n",
    "        def compute_batch(\n",
    "            self,\n",
    "            layers: torch.Tensor,\n",
    "            positions: torch.Tensor,\n",
    "            inject_values: torch.Tensor,\n",
    "            retain_graph: bool = True,\n",
    "        ) -> torch.Tensor:\n",
    "            \"\"\"Return attribution rows for a batch of (layer, pos) nodes.\n",
    "\n",
    "            The routine overrides gradients at **exact** residual-stream locations\n",
    "            triggers one backward pass, and copies the rows from the internal buffer.\n",
    "\n",
    "            Args:\n",
    "                layers: 1-D tensor of layer indices *l* for the source nodes.\n",
    "                positions: 1-D tensor of token positions *c* for the source nodes.\n",
    "                inject_values: `(batch, d_model)` tensor with outer product\n",
    "                    a_s * W^(enc/dec) to inject as custom gradient.\n",
    "\n",
    "            Returns:\n",
    "                torch.Tensor: ``(batch, row_size)`` matrix - one row per node.\n",
    "            \"\"\"\n",
    "\n",
    "            assert self._resid_activations[0] is not None, \"Residual activations are not cached\"\n",
    "            batch_size = self._resid_activations[0].shape[0]\n",
    "            self._batch_buffer = torch.zeros(\n",
    "                self._row_size,\n",
    "                batch_size,\n",
    "                dtype=inject_values.dtype,\n",
    "                device=inject_values.device,\n",
    "            )\n",
    "\n",
    "            # Custom gradient injection (per-layer registration)\n",
    "            batch_idx = torch.arange(len(layers), device=layers.device)\n",
    "\n",
    "            def _inject(grads, *, batch_indices, pos_indices, values):\n",
    "                grads_out = grads.clone().to(values.dtype)\n",
    "                grads_out.index_put_((batch_indices, pos_indices), values)\n",
    "                return grads_out.to(grads.dtype)\n",
    "\n",
    "            handles = []\n",
    "            layers_in_batch = layers.unique().tolist()\n",
    "\n",
    "            for layer in layers_in_batch:\n",
    "                mask = layers == layer\n",
    "                if not mask.any():\n",
    "                    continue\n",
    "                fn = partial(\n",
    "                    _inject,\n",
    "                    batch_indices=batch_idx[mask],\n",
    "                    pos_indices=positions[mask],\n",
    "                    values=inject_values[mask],\n",
    "                )\n",
    "                resid_activations = self._resid_activations[int(layer)]\n",
    "                assert resid_activations is not None, \"Residual activations are not cached\"\n",
    "                handles.append(resid_activations.register_hook(fn))\n",
    "\n",
    "            try:\n",
    "                last_layer = max(layers_in_batch)\n",
    "                self._resid_activations[last_layer].backward(\n",
    "                    gradient=torch.zeros_like(self._resid_activations[last_layer]),\n",
    "                    retain_graph=retain_graph,\n",
    "                )\n",
    "            finally:\n",
    "                for h in handles:\n",
    "                    h.remove()\n",
    "\n",
    "            buf, self._batch_buffer = self._batch_buffer, None\n",
    "            return buf.T[: len(layers)]\n",
    "    ```\n",
    "    </details><br/>\n",
    "2. Logit and Feature Attribution-Specific invocations of `compute_batch`\n",
    "    <details>\n",
    "    <summary>Logit Attribution Computation</summary><br/>\n",
    "\n",
    "    ```python\n",
    "        for i in range(0, len(logit_idx), batch_size):\n",
    "            batch = logit_vecs[i : i + batch_size]\n",
    "            rows = ctx.compute_batch(\n",
    "                layers=torch.full((batch.shape[0],), n_layers),\n",
    "                positions=torch.full((batch.shape[0],), n_pos - 1),\n",
    "                inject_values=batch,\n",
    "            )\n",
    "            edge_matrix[i : i + batch.shape[0], :logit_offset] = rows.cpu()\n",
    "            row_to_node_index[i : i + batch.shape[0]] = (\n",
    "                torch.arange(i, i + batch.shape[0]) + logit_offset\n",
    "            )\n",
    "    ```\n",
    "    </details><br/>\n",
    "\n",
    "    <details>\n",
    "    <summary>Feature Attribution Computation</summary><br/>\n",
    "\n",
    "    ```python\n",
    "    queue = [pending[i : i + batch_size] for i in range(0, len(pending), batch_size)]\n",
    "\n",
    "    for idx_batch in queue:\n",
    "        n_visited += len(idx_batch)\n",
    "\n",
    "        rows = ctx.compute_batch(\n",
    "            layers=feat_layers[idx_batch],\n",
    "            positions=feat_pos[idx_batch],\n",
    "            inject_values=ctx.encoder_vecs[idx_batch],\n",
    "            retain_graph=n_visited < max_feature_nodes,\n",
    "        )\n",
    "\n",
    "        end = min(st + batch_size, st + rows.shape[0])\n",
    "        edge_matrix[st:end, :logit_offset] = rows.cpu()\n",
    "        row_to_node_index[st:end] = idx_batch\n",
    "        visited[idx_batch] = True\n",
    "        st = end\n",
    "        pbar.update(len(idx_batch))\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Attribution Flow Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `_compute_score_hook` invoked by `compute_batch` as described below will calculate the tensor product of:\n",
    "1. the the unembed vectors (for target logit node attribution) or the relevant target layer encoder vecs (for target feature node attribution) \n",
    "2. and (for feature nodes) the relevant layer's activation-scaled transcoder decoder vectors (which represent the feature space transformation).\n",
    "\n",
    "**This allows us to score how much each feature (scaled by activation) contributes to the residual stream in the unembed (for logit attribution) or target transcoder encoder vec feature (for feature-feature attribution) vector's direction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Logit Attributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remembering our score computation hooks were registered layer-wise with individual per-layer hook registration, our custom gradient injection hooks are similarly registered layer-wise in `compute_batch`\n",
    "\n",
    "In the case of the gemma SLT example with `max_n_logits = 10`, we only have 1 set of `resid_activations` (from layer 26) to register our hooks for since we're computing source node attributions for all nodes w.r.t. only the 10 `logit_vecs` (`logit_vecs.shape = max_n_logits, d_model`), so `layers_in_batch` = [26]\n",
    "- concretely: \n",
    "    ```python\n",
    "    batch.shape\n",
    "    torch.Size([10, 2304])\n",
    "    logit_vecs.shape\n",
    "    torch.Size([10, 2304])\n",
    "    (layer, len(inject_values[mask]), batch_idx[mask], positions[mask])\n",
    "    (26, 10, tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8])) \n",
    "    ```\n",
    "- each batch will only have non-zero entries in the first 10 rows of dim 0:\n",
    "    ```python\n",
    "    has_nonzero = torch.any(grads.to(output_vecs.dtype)[read_index] != 0, dim=(1, 2))\n",
    "    nz_indices = torch.where(has_nonzero)[0]\n",
    "    nz_indices\n",
    "    tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], device='cuda:0')\n",
    "    ```\n",
    "We register a backward hook for the last layer (layer 26 in this case) `ctx._resid_activations[int(layer)]` that injects our custom `logit_vecs` (demeaned and transposed logit unembed col vectors) as gradient.\n",
    "\n",
    "For each batch, as `backward()` is executed the relevant layer-wise hooks are triggered filling in the corresponding rows of our attribution score buffer layer-by-layer via the einsum in our `_compute_score_hook` bwd hook. The number of elements filled per-hook correspond to the per-layer number of active nodes (feature, error or token): \n",
    "```python\n",
    "def _compute_score_hook(\n",
    "# ...\n",
    "\n",
    "    proxy = weakref.proxy(self)\n",
    "\n",
    "    def _hook_fn(grads: torch.Tensor, hook: HookPoint) -> None:\n",
    "        proxy._batch_buffer[write_index] += einsum(\n",
    "            grads.to(output_vecs.dtype)[read_index],\n",
    "            output_vecs,\n",
    "            \"batch position d_model, position d_model -> position batch\",\n",
    "        )\n",
    "    return hook_name, _hook_fn\n",
    "```\n",
    "\n",
    "Note we always `retain_graph` for these backward hooks for subsequent use. `compute_batch` then returns the batch buffer (which is simultaneously zeroed out on the object) which includes the feature nodes + error nodes + token nodes attributions for each of the logits (as `rows`)\n",
    "\n",
    "The edge matrix is then updated using the calculated node attributions in rows (while leaving the last `max_n_logit` (10) logit entries 0), so the first 10 rows in `edge_matrix` are populated except for the last 10 columns, the remaining elements are all zero at this point since we're just building the logit attribution entries.\n",
    "\n",
    "We finally update the `row_to_node_index` with the logit node mapping, so in this case, logit rows 0:10 map to the last 10 edge_matrix columns, while the 11th flattened node is the first feature node and doesn't have a mapping yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reproducing A Specific Node Attribution Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's inspect the attribution of a specific node (e.g. 6334), to make this concrete. We can set a break in our custom bwd hook and validate the calculation written to our buffer for the target index 6334. This is the einsum referenced above:\n",
    "    ```python\n",
    "    proxy._batch_buffer[write_index] += einsum(grads.to(output_vecs.dtype)[read_index], output_vecs,\n",
    "        \"batch position d_model, position d_model -> position batch\",\n",
    "    )\n",
    "    ```\n",
    "- The relevant shapes of the tensors involved in this calculation are:\n",
    "    ```python\n",
    "    grads.to(output_vecs.dtype)[read_index].shape # logit_vecs (demeaned and transposed unembedding vectors)\n",
    "    torch.Size([256, 81, 2304])  # batch_size, layer 20 active nodes, d_model\n",
    "    output_vecs.shape  # active decoder_vecs for layer\n",
    "    torch.Size([81, 2304])       # layer 20 active nodes, d_model\n",
    "    write_index.shape\n",
    "    torch.Size([81]) # layer 20 active nodes\n",
    "    read_index[1].shape\n",
    "    torch.Size([81]) # layer 20 active nodes\n",
    "    ```\n",
    "- In this case, we only run `compute_batch` one time since we have fewer than 256 target nodes we want to calculate source attribution scores for. Since we're using backward() for orchestration, we only need to trigger a single \n",
    "`backward()` that processes all layer-wise score hooks as appropriate for our source nodes. The write_index writes to our score buffer which for each batch (again, just 1 in this case) is of shape `(n_logits, logit_offset)`\n",
    "- All feature, error and token node scores are calculated, we show just the feature_hooks below. The returned rows for each logit computation batch (just 1) is shape (10, 7358) in this case because we have 10 target logits and 7358 total source nodes (feature, error and token nodes).\n",
    "- We isolate the relevant indices for our target node 6334, we only have 10 nodes in this batch since we're only calculating source attributions for the 10 target logit nodes.:\n",
    "    ```python\n",
    "    target_node_id = 6334\n",
    "    target_buf_index = torch.where(write_index == target_node_id)\n",
    "    orig_grad_slice = grads.to(output_vecs.dtype)[read_index].detach()\n",
    "    grads_slice = orig_grad_slice[:10, target_buf_index, :].squeeze()\n",
    "    output_vecs_slice = output_vecs[target_buf_index].squeeze()\n",
    "    (grads_slice.shape, output_vecs_slice.shape)\n",
    "    (torch.Size([10, 2304]), torch.Size([2304]))\n",
    "    ```\n",
    "- We can then reproduce the einsum calculation directly to validate the attribution for our target node 6334\n",
    "    ```python\n",
    "    torch.matmul(grads_slice, output_vecs_slice)\n",
    "    tensor([ 6.5625e+00, -1.4258e-01, -2.7344e-02,  8.3750e+00,  2.7656e+00,\n",
    "            -2.0020e-01, -6.8359e-03,  6.0312e+00, -2.2339e-02,  2.8931e-02],\n",
    "        device='cuda:0', dtype=torch.bfloat16)\n",
    "    ```\n",
    "- So what are the attributions to our feature node 6334 for our `target_logit_indices`? \n",
    "\n",
    "    - We projected our decoder vector (associated with node 6334) into the unembed vec direction for each of our top logits individually.\n",
    "    - For our target token ('‚ñÅAustin', '‚ñÅDallas') logit indexes, elements 0 and 7, we see 6.5625 and 6.0312.\n",
    "    - As expected, our initial attribution for active feature index 6334 after compute logit attribution:\n",
    "        - returned buffer:\n",
    "        ```python\n",
    "        buf.T[:10][[0,7], 6334]  # returned buffer from `compute_batch`\n",
    "        tensor([6.5625, 6.0312], device='cuda:0', dtype=torch.bfloat16)\n",
    "        edge_matrix[[0, 7], 6334]  # logit nodes are initially the first `max_n_logits` target nodes\n",
    "        tensor([6.5625, 6.0312])\n",
    "        ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attribution Node Id to (Graph) Feature Id Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- If you want to associate these internal node attributions to feature ids on our attribution graphs, we need to map the node ids back to feature ids (we have helper functions for this but it's good to understand the mechanics)\n",
    "- remember the `ctx.activation_matrix` saves non-zero activations by feature_idx (`0:d_transcoder`) for each layer for each position:\n",
    "\n",
    "```python\n",
    "        ctx.activation_matrix\n",
    "        tensor(indices=tensor([[    0,     0,     0,  ...,    25,    25,    25],\n",
    "                        [    1,     1,     1,  ...,     8,     8,     8],\n",
    "                        [   41,    96,   253,  ..., 16014, 16302, 16326]]),\n",
    "        values=tensor([ 2.5156,  3.5312,  0.7070,  ...,  7.4375, 40.7500,\n",
    "                        12.2500]),\n",
    "        device='cuda:0', size=(26, 9, 16384), nnz=7115, dtype=torch.bfloat16,\n",
    "        layout=torch.sparse_coo)\n",
    "```\n",
    "- the `encoder_to_decoder_map` map is just a vector `activation_matrix._nnz` long (for SLT) that can be used to associate active encoder and decoder vecs to the activation matrix.\n",
    "- for example for layer 20, find the positions in our `activation_matrix._nnz` length vector of active decoders\n",
    "```python\n",
    "        per_layer_mask = (nnz_layers == 20)\n",
    "        nnz_positions[per_layer_mask]\n",
    "        tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
    "                3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
    "                6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8,\n",
    "                8, 8, 8, 8, 8, 8, 8, 8, 8], device='cuda:0')\n",
    "```\n",
    "- so our active decoders for layer 20 are\n",
    "\n",
    "```python\n",
    "        ctx.decoder_vecs[per_layer_mask].shape\n",
    "        torch.Size([81, 2304])\n",
    "```\n",
    "\n",
    "- and the appropriate activation_matrix indices using encoder_to_decoder_map\n",
    "\n",
    "```python\n",
    "        ctx.encoder_to_decoder_map[per_layer_mask]\n",
    "        tensor([6254, 6255, 6256, 6257, 6258, 6259, 6260, 6261, 6262, 6263, 6264, 6265,\n",
    "                6266, 6267, 6268, 6269, 6270, 6271, 6272, 6273, 6274, 6275, 6276, 6277,\n",
    "                6278, 6279, 6280, 6281, 6282, 6283, 6284, 6285, 6286, 6287, 6288, 6289,\n",
    "                6290, 6291, 6292, 6293, 6294, 6295, 6296, 6297, 6298, 6299, 6300, 6301,\n",
    "                6302, 6303, 6304, 6305, 6306, 6307, 6308, 6309, 6310, 6311, 6312, 6313,\n",
    "                6314, 6315, 6316, 6317, 6318, 6319, 6320, 6321, 6322, 6323, 6324, 6325,\n",
    "                6326, 6327, 6328, 6329, 6330, 6331, 6332, 6333, 6334], device='cuda:0')\n",
    "```\n",
    "- for the our example feature we're probing in `ctx.encoder_to_decoder_map`, what is the active feature index that we got our active encoder and decoder vecs from in activation_matrix?\n",
    "\n",
    "```python\n",
    "        ctx.activation_matrix.indices().T[6334]\n",
    "        tensor([   20,     8, 15589], device='cuda:0')\n",
    "```\n",
    "- so **layer 20**, **position 8**, **feature id 15589** is the feature that corresponds to node id 6334\n",
    "- using `activation_matrix` directly to probe all features active at layer 20, position 8 and their corresponding values, we see we can expect feature 15589 to have an activation of 52.0 in our graph UI\n",
    "\n",
    "```python\n",
    "        active_indices = ctx.activation_matrix.indices()\n",
    "        test_mask = (active_indices[0] == 20) & (active_indices[1] == 8)\n",
    "        filtered_indices = active_indices[:, test_mask]\n",
    "        filtered_values = ctx.activation_matrix.values()[test_mask]\n",
    "\n",
    "        filtered_indices.T\n",
    "        tensor([[   20,     8,   114],\n",
    "                [   20,     8,   438],\n",
    "                [   20,     8,  3094],\n",
    "                [   20,     8,  5433],\n",
    "                [   20,     8,  5916],\n",
    "                [   20,     8,  6026],\n",
    "                [   20,     8, 10118],\n",
    "                [   20,     8, 10254],\n",
    "                [   20,     8, 15133],\n",
    "                [   20,     8, 15276],\n",
    "                [   20,     8, 15366],\n",
    "                [   20,     8, 15589]], device='cuda:0')\n",
    "        filtered_values\n",
    "        tensor([15.5625,  7.8125, 16.7500,  7.5312, 45.7500, 14.8750, 12.1875, 19.0000,\n",
    "                8.0000, 14.4375,  7.2812, 52.0000], device='cuda:0',\n",
    "        dtype=torch.bfloat16)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis Point Data**: See below sampled data for the current attribution example at the end of `Compute Logit Attributions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_injector.get_output(\"ap_compute_logit_attribution_end\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Feature Attributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that the edge_matrix (adjacency matrix) analyzed below is indexed as (target, source) (targets are rows)\n",
    "- When the graph is pruned, source input edges are normalized for each source node so that they sum to 1 for each target. \n",
    "- if we are operating on all features (since our `max_feature_nodes` has been set to equal our `total_active_feats`) we can run `compute_batch` below on all the features\n",
    "- if we are only operating on a subset of features, we first have to run `compute_partial_influences` to sort all the features by logit influence and then score only those features\n",
    "- `compute_batch` is run once per batch (256 in this example) of nodes to analyze, similar to logit attribution above but instead of injecting `logit_vecs`, we inject the relevant target feature (transcoder encoder vecs) as the gradient. \n",
    "- Also, instead of just a single compute_batch (with only 10 of the 256 rows populated) and a single `backward` call, we have many more batches to process that will be fully populated (except for the last batch)\n",
    "    - e.g., for this gemma SLT example, we need to compute the source influence vectors for all target active features (7115) which requires 28 batches of 256 nodes each (the last batch will only have 203 nodes)\n",
    "    \n",
    "        ```python\n",
    "        len(queue)\n",
    "        28\n",
    "        ```\n",
    "- Our node source vectors are the same as described in logits attribution above (ctx bound in the first forward pass) but we einsum the injected target `encoder_vecs` with them in our bwd hooks to calculate the feature-to-feature attributions\n",
    "- each batch involves registering the relevant cached resid activations for the layers in a given batch (a batch can cross layer boundaries)\n",
    "- the max layer in the batch then has backward() called which allows us to calculate the `matmul` of the scaled decoder vec for each active feature (or the token_vecs for the token_vec range, which is the case for layer 0 encoders) with the injected target `encoder_vecs` for active features. \n",
    "- Another non-feature case are attribution hooks where the `output_vecs` are the `error_vectors` `(n_layers, n_pos, d_model)`, playing the same role as the scaled decoder vecs do in the feature node attributions\n",
    "- we retain_graph for as long as we have another batch to compute\n",
    "- we queue up `update_interval` (default 4) * batch_size (so 1024 in this example) nodes unless we have fewer than that left to process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post-Attribution Edge Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `edge_matrix` inspection after feature attributions but before reshaping to `full_edge_matrix` (so first 10 rows are still logit nodes)\n",
    "- at this point we have non-normalized influence scores for all logit/feature feature/feature edges so the target vectors are pretty dense:\n",
    "\n",
    "```python\n",
    "        edge_matrix[0, :].count_nonzero()\n",
    "        tensor(7091)\n",
    "        edge_matrix[6344, :].count_nonzero()\n",
    "        tensor(6423)\n",
    "```\n",
    "- An error node exists for every token for every layer. We also have the token nodes themselves.\n",
    "```python\n",
    "# n_error_nodes = (num_layers + 1) * num_tokens = num_layers * num_tokens + num_tokens\n",
    "```\n",
    "- for the gemma SLT example, `edge_matrix` at this point is shaped as (7125, 7368): \n",
    "```python\n",
    "# n_logits(10) + n_feature_nodes(7115) -> 7125\n",
    "# n_feature_nodes(7115) + n_error_nodes(234) + n_tokens(9) + n_logits(10) -> 7368\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis Point Data**: See below sampled data for the current attribution example at the end of `Compute Feature Attributions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_injector.get_output(\"ap_compute_feature_attributions_end\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Packaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before packaging the our attribution matrices into a `circuit-tracer` `Graph` object, note our top influencing features are not the same as the raw activations you'll see in the graph from `activation_matrix`. They are our computed feature influences (depending on context, normalized or not-yet-normalized)\n",
    "\n",
    "In our analysis point below you can see our top inspected token first order and second order feature attributions.\n",
    "\n",
    "- Note this is not yet converted to abs values and normalized so that the input edges sum to 1. The target logit attribution sums do not correspond directly to logit probs but are rather the non-normalized influence scores\n",
    "- We refer to the top influencing nodes of the top nodes influencing our inspected logits as per token `pre_prune_2nd_order` matrices.\n",
    "- Our helper functions mask out the error and token nodes from these 2nd order influence matrices since by definition those nodes won't have input feature nodes.\n",
    "\n",
    "Prior to graph packaging, we reshape our `edge_matrix` to have the logit nodes at the end and use it to populate the input `full_edge_matrix` which will be passed to the `Graph` constructor as our initial (pre-pruned) adjacency matrix.\n",
    "\n",
    "Our adjacency matrix will have the following shape (n_total_nodes, n_total_nodes). Where `n_total_nodes` is:\n",
    "```\n",
    "n_feature_nodes + n_error_nodes + n_token_nodes + n_logits\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis Point Data**: See below sampled data for the current attribution example prior to `Graph Packaging`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_injector.get_output(\"ap_graph_creation_start\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Pruning, Creation and Saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prune by Node/Edge Influence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next inspect graph pruning process `prune_graph` following the transformation of our target logits raw target (logit) node attribution through the `compute_node_influence` and `compute_edge_influence` functions.\n",
    "\n",
    "The first step to computing our node influences is to normalize our adjacency matrix above. We then use this normalized adjacency matrix to compute node influences influence via a numerical approach to Neumann series calculation.\n",
    "\n",
    "Iterative computing of our node influence vector starts with seeding our `logit_weights` via our direct logit probabilities. We then multiply those weights with our adjacency matrix  `logit_weights @ A` to construct the initial `current_influence` vector and proceed to iteratively update this influence vector until it converges, multiplying increasing degrees of our adjacency matrix by the `current_influence`.\n",
    "```python\n",
    "    current_influence = logit_weights @ A\n",
    "    influence = current_influence\n",
    "    iterations = 0\n",
    "    while current_influence.any():\n",
    "        if iterations >= max_iter:\n",
    "            raise RuntimeError(\n",
    "                f\"Influence computation failed to converge after {iterations} iterations\"\n",
    "            )\n",
    "        current_influence = current_influence @ A\n",
    "        influence += current_influence\n",
    "        iterations += 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Computing node influences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "For this example, we first inspect the initial adjacency matrix post-normalization within the `compute_influence` function.\n",
    "\n",
    "Note we are not updating the entire adjacency matrix in compute_influence, just a single axis of the matrix, an influence/current_influence vector (size n_total_nodes), so we won't see 2nd order nodes updated in A.\n",
    "\n",
    "`logit_weights` starts off with as the targets axis of the adjacency matrix (dim 0) and then sets the last `n_logit` weights to the logit probabilities, all the other initial values are initially zero:\n",
    "```python\n",
    "    logit_weights = torch.zeros(\n",
    "        graph.adjacency_matrix.shape[0], device=graph.adjacency_matrix.device\n",
    "    )\n",
    "    logit_weights[-n_logits:] = graph.logit_probabilities\n",
    "```\n",
    "\n",
    "As we multiply by the adjacency matrix in `compute_influence`, the first iteration will result in the last `n_logits` elements of the first column (the source weights of the first feature) contributing to the dot product with the logit probs, yielding the weighted sum of contributions of the first feature node to the logits (and so on for the remaining nodes).\n",
    "See the validated/sampled calculation for the first iteration and first feature concretely:\n",
    "\n",
    "Observe the first iteration consists of the weighted sum of the direct influences for each source feature on the logit probs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis Point Data**: See below sampled data for the current attribution example at the beginning of the `Prune by Node Compute Influence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_injector.get_output(\"ap_node_compute_influence_init\", skip=[\"trace_dict\", \"context\", \"iteration\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See below the evolution of `current_influences` in the node influence computation for this example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Neumann Series Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_dict = analysis_injector[\"ap_node_compute_influence\"][\"trace_dict\"][\"node\"]\n",
    "stacked_trace = torch.stack(trace_dict)\n",
    "fig = plot_ridgeline_convergence(\n",
    "    data=stacked_trace, stats=None, title=\"Neumann Series Convergence Trace Ridgeline Plot\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note you should see very little marginal distributional change after the first few iterations.\n",
    "For most examples, you should see that after around the first few iterations, the distribution stabilizes, which suggests contributions from longer paths\n",
    "are not significantly contributing to logit values.\n",
    "\n",
    "- Iteration 0 (initial logit prob dot product) captures the weighted sum of the direct influences for each feature on each of the logit probs\n",
    "- Iteration 1 captures for each feature, that feature's weighted influence on the iteration 0 direct logit prob influences (in other words, the weighted direct logit prob influences of features mediated one-hop through each feature)\n",
    "- Iteration 2 the weighted one-hop prob influences of features mediated two-hop through each feature (i.e. the second iteration reflects the marginal influence of two-hop paths, multiplying the adjacency matrix feature influences by all the 1-hop current_influence vector elements)\n",
    "- `current_influences` continues to get smaller and smaller as the order/path length increases!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nicely format top-k influence values and indices by iteration using orchestrator helper\n",
    "convergence_iteration = analysis_injector[\"ap_node_compute_influence\"][\"iteration\"]\n",
    "print(f\"Convergence occurred at iteration: {convergence_iteration}\")\n",
    "\n",
    "sample_tensor_output(stacked_trace, (0, 2, 4, 8), [\"Iteration\", \"Top k Values\", \"Top k Indices\"], 5, tablefmt=\"html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Computing `node_mask`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then calculate a node_mask by finding a threshold (`find_threshold`) that meets our targeted explained influence using our `node_influence` and configured node_threshold.\n",
    "\n",
    "```python\n",
    "def find_threshold(scores: torch.Tensor, threshold: float):\n",
    "    # Find score threshold that keeps the desired fraction of total influence\n",
    "    sorted_scores = torch.sort(scores, descending=True).values\n",
    "    cumulative_score = torch.cumsum(sorted_scores, dim=0) / torch.sum(sorted_scores)\n",
    "    threshold_index = torch.searchsorted(cumulative_score, threshold)\n",
    "    # make sure we don't go out of bounds (only really happens at threshold=1.0)\n",
    "    threshold_index = min(threshold_index, len(cumulative_score) - 1)\n",
    "    return sorted_scores[threshold_index]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we compute our `edge_mask`, our `node_mask` has filtered the number of non-zero src and target features to include only features that explain the specified `node_threshold` of influence\n",
    "\n",
    "```python\n",
    "pruned_matrix = graph.adjacency_matrix.clone()\n",
    "pruned_matrix[~node_mask] = 0\n",
    "pruned_matrix[:, ~node_mask] = 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis Point Data**: See below sampled data for the current attribution example at the end of `Prune by Node Influence`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_injector.get_output(\"ap_graph_prune_node_influence_end\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Computing `edge_mask`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further reduce the graph size by filtering on the cumulative influence of all edges (0.98 threshold in this case).\n",
    "\n",
    "- `compute_edge_influence` starts with a node-threshold-pruned adjacency matrix:\n",
    "    ```python\n",
    "    edge_scores = compute_edge_influence(pruned_matrix, logit_weights)\n",
    "    ```\n",
    "- The pruned_matrix is not initially normalized, but `compute_edge_influence` will normalize it prior to `compute_influence`\n",
    "- After `computing_influence` (same fn/process as above) we see we have the feature influences vector for nodes that influence logits (weighted average of all collectively) above the given threshold\n",
    "- Since we want to have our completed edge matrix to include the logit probs, we add in logit_weights to the `pruned_influence` vector before returning `edge_scores` as normalized_pruned matrix elementwise multiplied by the each element of the `pruned_influence` vector.\n",
    "- We'll see below at the end we no longer have normalized target influence vectors but rather the sum of the src features will be the total influence that target feature had on the overall logits\n",
    "\n",
    "- So our new `edge_scores` matrix will have rows where each target feature is the `pruned_influence` value for that feature multiplied by the `normalize_pruned` vector for each feature (from the `normalized_pruned` matrix which is the threshold-pruned adjacency matrix) \n",
    "- As we see below, this effectively scales our normalized feature scores for each target feature by our `pruned_influence` scores (sum of all paths above a threshold)\n",
    "- By multiplying the normalized vector of influences for each target feature by every other source feature by the total influence of that target feature on the logits, **we scale for each target feature the source feature influences so they are weighted for that target feature's influence on logits by all paths**\n",
    "\n",
    "- Because we scaled our normalized features by the pruned_influence values, *the sum of our target feature rows will now be the pruned_influence values*.\n",
    "\n",
    "- You should observe below that the feature influences are quite distributed. In the case of the default example prompt, for the top logit, only 0.19 of the 0.29 logit probability (64%) is accounted for by the top 100 features, and the top feature only accounted for 0.0144 (4.8%) of the logit prob for our top logit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis Point Data**: See below sampled data for the current attribution example at the end of `Compute Edge Influence`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_injector.get_output(\"ap_graph_prune_edge_influence_post_norm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then compute our `edge_mask` using the same `find_threshold` function as above to meet our `edge_threshold` target.\n",
    "As we'll see, we're able to dramatically reduce the number of nonzero elements even while retaining a very large fraction of total edge influence.\n",
    "Note below the sum of our edge_score attributions to target logits will equal the aggregate target logit probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_injector.get_output(\"ap_graph_prune_edge_influence_pre_mask\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Applying Edge Mask and Finalizing Score Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We next ensure proper graph connection by ensuring all feature and error nodes have outgoing edges and all feature nodes have incoming edges\n",
    "\n",
    "```python\n",
    "    old_node_mask = node_mask.clone()\n",
    "    # Ensure feature and error nodes have outgoing edges\n",
    "    node_mask[: -n_logits - n_tokens] &= edge_mask[:, : -n_logits - n_tokens].any(0)\n",
    "    # Ensure feature nodes have incoming edges\n",
    "    node_mask[:n_features] &= edge_mask[:n_features].any(1)\n",
    "```\n",
    "\n",
    "- `node_mask[: -n_logits - n_tokens] &= edge_mask[:, : -n_logits - n_tokens].any(0)`\n",
    "  - **Slices** `node_mask` to exclude the last `n_logits + n_tokens` elements.\n",
    "  - Uses `&=` (in-place logical AND) to update the mask.\n",
    "  - `edge_mask[:, : -n_logits - n_tokens]` selects columns corresponding to the same nodes.\n",
    "  - `.any(0)` checks if **any edge exists** for each node (across all rows dim=0), returning a boolean tensor.\n",
    "  - The mask is updated so that only nodes with at least one outgoing edge remain `True`. (source nodes (columns) that are not currently masked and do not have any outgoing edges to target nodes will now be masked)\n",
    "- For the default example prompt, concretely, we examine the 7349 source influence columns in this case and return whether any target node (outgoing edge) exists\n",
    "\n",
    "  ```python\n",
    "  node_mask.count_nonzero()\n",
    "  tensor(1584, device='cuda:0')\n",
    "  edge_mask[:, : -n_logits - n_tokens].shape\n",
    "  torch.Size([7368, 7349])\n",
    "  edge_mask[:, : -n_logits - n_tokens].any(0).shape\n",
    "  torch.Size([7349])\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `node_mask[:n_features] &= edge_mask[:n_features].any(1)`\n",
    "    - Slices `node_mask` to the first `n_features` elements.\n",
    "    - `edge_mask[:n_features]` selects rows for feature nodes.\n",
    "    - `.any(1)` checks if **any incoming edge exists** for each feature node (across all columns).\n",
    "    - Updates the mask so that only feature nodes with at least one incoming edge remain `True`.\n",
    "\n",
    "- We prune iteratively all nodes that are missing incoming or outgoing edges (in this case, no further pruning was necessary because our pruned node_mask already equaled our original mask, meaning our prune operations didn't make any changes)\n",
    "    ```python\n",
    "    torch.all(node_mask == old_node_mask)\n",
    "    ```\n",
    "- We finally calculate the cumulative influence scores, sorting by `node_influence` descending and calculating the cumulative scores as a fraction of the total sorted scores and return our calculated node_mask, `edge_mask` and final_scores as a `PruneResult`\n",
    "```python\n",
    "    # Calculate cumulative influence scores\n",
    "    sorted_scores, sorted_indices = torch.sort(node_influence, descending=True)\n",
    "    cumulative_scores = torch.cumsum(sorted_scores, dim=0) / torch.sum(sorted_scores)\n",
    "    final_scores = torch.zeros_like(node_influence)\n",
    "    final_scores[sorted_indices] = cumulative_scores\n",
    "```\n",
    "\n",
    "- Be aware that pytorch will round representations to 4 digits by default, but many of our non-zero contributing influences will appear to be 0 with this granularity.\n",
    "- In the case of the default example:\n",
    "    - 7089 of our 7368 nodes contribute to the cumulative total score (279 zero scores in cumulative_scores)\n",
    "    - The top 30 feature influences account for 34% of our aggregate logit probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis Point Data**: See below sampled data for the current attribution example at the end of `Applying Edge Mask and Finalizing Score Matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_injector.get_output(\"ap_graph_prune_edge_influence_end\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Graph Creation and Saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We then use our node_mask, edge_mask and cumulative_scores to construct our graph:\n",
    "    ```python\n",
    "        tokenizer = AutoTokenizer.from_pretrained(graph.cfg.tokenizer_name)\n",
    "        nodes = create_nodes(graph, node_mask, tokenizer, cumulative_scores, scan)\n",
    "        used_nodes, used_edges = create_used_nodes_and_edges(graph, nodes, edge_mask)\n",
    "        model = build_model(graph, used_nodes, used_edges, slug, scan, node_threshold, tokenizer)\n",
    "\n",
    "        # Write the output locally\n",
    "        with open(os.path.join(output_path, f\"{slug}.json\"), \"w\") as f:\n",
    "            f.write(model.model_dump_json(indent=2))\n",
    "        add_graph_metadata(model.metadata.model_dump(), output_path)\n",
    "        logger.info(f\"Graph data written to {output_path}\")\n",
    "\n",
    "        total_time_ms = (time.time() - total_start_time) * 1000\n",
    "        logger.info(f\"Total execution time: {total_time_ms=:.2f} ms\")\n",
    "    ```\n",
    "- When the graph is initially constructed, the `graph.active_features` tensor is created from this mapping in the Graph instantiation:\n",
    "    ```python\n",
    "    ...\n",
    "    active_features=activation_matrix.indices().T,\n",
    "    activation_values=activation_matrix.values(),\n",
    "    ...\n",
    "    adjacency_matrix=full_edge_matrix,\n",
    "    ```\n",
    "- defined by:\n",
    "    ```python\n",
    "        active_features (torch.Tensor): A tensor of shape (n_active_features, 3)\n",
    "            containing the indices (layer, pos, feature_idx) of the non-zero features\n",
    "            of the model on the given input string.\n",
    "        adjacency_matrix (torch.Tensor): The adjacency matrix. Organized as\n",
    "            [active_features, error_nodes, embed_nodes, logit_nodes], where there are\n",
    "            model.cfg.n_layers * len(input_tokens) error nodes, len(input_tokens) embed\n",
    "            nodes, len(logit_tokens) logit nodes. The rows represent target nodes, while\n",
    "            columns represent source nodes.\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanly teardown analysis injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teardown hooks after graph generation\n",
    "if enable_analysis_injection:\n",
    "    print(\"\\nDisabling analysis hooks...\")\n",
    "    try:\n",
    "        analysis_injector.teardown()\n",
    "        print(\"‚úì Analysis injector cleaning torn down.\")\n",
    "        if log_path := getattr(analysis_injector, \"analysis_log\", None):\n",
    "            print(f\"Analysis log available for inspection: {log_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error while tearing down analysis injector: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Visualizing Attribution Graphs\n",
    "\n",
    "In this section, we'll demonstrate how to save the generated attribution graphs and prepare them for visualization. The CircuitTracerAdapter integrates with Interpretune's AnalysisStore to persistently store graph data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from circuit_tracer.frontend.local_server import serve\n",
    "\n",
    "\n",
    "enable_iframe = False  # whether to enable the IFrame display or not\n",
    "\n",
    "port = 8046\n",
    "server = serve(data_dir=ct_module.circuit_tracer_cfg.graph_output_dir, port=port)\n",
    "# TODO: make this configurable at the top or request the user setup port forwarding and use localhost\n",
    "port_forwarding = False  # whether to use port forwarding or not\n",
    "# hostname = 'localhost'  # the hostname of the server where the graph files are hosted\n",
    "hostname = \"speediedl\"\n",
    "\n",
    "if port_forwarding:\n",
    "    hostname = \"localhost\"  # use localhost for port forwarding\n",
    "    print(\n",
    "        f\"Using port forwarding (ensure it is configured) and localhost. \"\n",
    "        f\"Open your graph here at http://{hostname}:{port}/index.html\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        f\"Not using port forwarding. Use the IFrame below, or open your graph here \"\n",
    "        f\"directly at http://{hostname}:{port}/index.html\"\n",
    "    )\n",
    "\n",
    "if enable_iframe:\n",
    "    from IPython.display import IFrame\n",
    "\n",
    "    # Display the IFrame with the graph visualization\n",
    "    print(f\"Displaying graph visualization in IFrame at http://{hostname}:{port}/index.html\")\n",
    "    display(IFrame(src=f\"http://{hostname}:{port}/index.html\", width=\"100%\", height=\"800px\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps and Future Extensions\n",
    "\n",
    "This notebook demonstrates the basic scaffolding for the CircuitTracerAdapter. The current implementation provides:\n",
    "\n",
    "1. **Basic Integration**: CircuitTracerAdapter integrates with Interpretune's session management\n",
    "2. **Configuration**: CircuitTracerConfig allows customization of attribution parameters\n",
    "3. **Protocol Support**: CircuitAnalysisBatchProtocol defines the interface for batch processing\n",
    "4. **Adapter Composition**: Seamless integration with other Interpretune adapters\n",
    "\n",
    "#### Future Extensions:\n",
    "\n",
    "1. **Full Implementation**: Complete the adapter methods to actually generate attribution graphs\n",
    "2. **Batch Processing**: Support for efficient batch attribution analysis\n",
    "3. **Advanced Analysis**: Integration with AnalysisOp for complex circuit analysis workflows\n",
    "4. **Visualization**: Built-in support for graph visualization and exploration\n",
    "5. **Caching**: Intelligent caching of attribution results for faster iteration\n",
    "6. **Model Support**: Extended support for different model architectures beyond GPT-2\n",
    "\n",
    "#### Resources:\n",
    "\n",
    "- [Circuit Tracer Documentation](https://github.com/jacobdunefsky/circuit-tracer)\n",
    "- [Interpretune Documentation](https://github.com/speediedan/interpretune)\n",
    "- [Attribution Methods Paper](https://arxiv.org/abs/2310.10348)\n",
    "\n",
    "This scaffold provides a solid foundation for building sophisticated circuit analysis workflows with Interpretune and Circuit Tracer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "it_latest (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
