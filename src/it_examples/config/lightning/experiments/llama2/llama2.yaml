data.class_path: it_examples.models.lightning.llama2.Llama2BoolRTELightningDataModule
model.class_path: it_examples.models.lightning.llama2.Llama2ITLightningModule
itdm_cfg:
  model_name_or_path: meta-llama/Llama-2-7b-chat-hf
  task_name: rte
  os_env_model_auth_key: LLAMA2_AUTH_KEY
  tokenizer_id_overrides:
    pad_token_id: 32000
  special_tokens_dict: {"pad_token": "<PAD>"}
  prompt_cfg:
    class_path: it_examples.models.core.llama2.Llama2PromptConfig
  tokenizers_parallelism: false
  tokenizer_kwargs:
    use_fast: true  # by default true
    local_files_only: false
    padding_side: left
    model_input_names: ['input_ids', 'attention_mask']
  # defer_model_init: true  # uncomment along with signature_columns to test deferred model init mode
  # signature_columns: ['input_ids', 'attention_mask', 'position_ids', 'past_key_values', 'inputs_embeds', 'labels',
  #                     'use_cache', 'output_attentions', 'output_hidden_states', 'return_dict']
it_cfg:
  lora_cfg:
    r: 8
    lora_alpha: 32
    target_modules: ["q_proj", "v_proj"]
    lora_dropout: 0.05
    bias: none
    task_type: "CAUSAL_LM"
