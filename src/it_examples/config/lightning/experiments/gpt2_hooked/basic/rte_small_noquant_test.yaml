itdm_cfg:
  model_name_or_path: gpt2
  task_name: rte
  dataset_path: /home/speediedan/.cache/huggingface/datasets/rte
  train_batch_size: 32
  eval_batch_size: 32
  prepare_validation_set_only: true
  tokenizer_id_overrides:
    pad_token_id: 50256
  prompt_cfg:
    class_path: it_examples.models.core.gpt2.GPT2PromptConfig
    init_args:
      cust_task_prompt:
        context: '<|PREMISE|>:'
        question: '<|HYPOTHESIS|>:'
it_cfg:
  auto_model_cfg:
    model_head: transformers.GPT2LMHeadModel
  tlens_from_pretrained_cfg:
    enabled: true
    model_name: gpt2-small
    fold_ln: true
    center_writing_weights: true
    center_unembed: true
    refactor_factored_attn_matrices: false
    checkpoint_index: null
    checkpoint_value: null
    device: cuda
    n_devices: 1
    move_to_device: true
    fold_value_biases: true
    default_prepend_bos: true
    default_padding_side: right
    dtype: bfloat16
  activation_checkpointing: false
trainer:
  accumulate_grad_batches: 1
  devices: 1
  logger:
    class_path: lightning.pytorch.loggers.TensorBoardLogger
    init_args:
      name: gpt2_tlens_rte_small_noquant_test
