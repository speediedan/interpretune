session_cfg:
  adapter_ctx: [lightning]
  datamodule_cls: it_examples.experiments.rte_boolq.RTEBoolqDataModule
  module_cls: it_examples.experiments.rte_boolq.RTEBoolqModule
  datamodule_cfg:
    class_path: interpretune.base.config.datamodule.ITDataModuleConfig
    init_args:
      model_name_or_path: meta-llama/Llama-3.2-3B-Instruct
      task_name: rte
      train_batch_size: 2
      eval_batch_size: 2
      cust_tokenization_pattern: llama3-chat
      os_env_model_auth_key: HF_GATED_PUBLIC_REPO_AUTH_KEY
      tokenizer_id_overrides:
        pad_token_id: 128004
      special_tokens_dict: {"pad_token": "<|finetune_right_pad_id|>"}
      enable_datasets_cache: true
      prepare_data_map_cfg:
        batched: true
      data_collator_cfg:
        collator_class: transformers.DataCollatorWithPadding
      prompt_cfg:
        class_path: it_examples.example_prompt_configs.RTEBoolqLlama3PromptConfig
      tokenizers_parallelism: false
      tokenizer_kwargs:
        use_fast: true  # by default true
        local_files_only: false
        add_bos_token: false
        padding_side: left  # NOTE: this should be changed to right for finetuning it seems for llama3 family
        model_input_names: ['input_ids', 'attention_mask']
  module_cfg:
    class_path: it_examples.experiments.rte_boolq.RTEBoolqConfig
    init_args:
      model_cfg:
        _attn_implementation: eager
      zero_shot_cfg:
        class_path: it_examples.experiments.rte_boolq.RTEBoolqZeroShotClassificationConfig
        init_args:
          enabled: true
          lm_generation_cfg:
            class_path: interpretune.base.config.mixins.HFGenerationConfig
            init_args:
              model_config:
                max_new_tokens: 5
                pad_token_id: 128004
      hf_from_pretrained_cfg:
        class_path: interpretune.base.config.module.HFFromPretrainedConfig
        init_args:
          use_model_cache: false
          model_head: transformers.LlamaForCausalLM
          activation_checkpointing: false
          bitsandbytesconfig:
            load_in_4bit: True
            bnb_4bit_use_double_quant: True
            bnb_4bit_quant_type: nf4
            bnb_4bit_compute_dtype: bfloat16
          lora_cfg:
            r: 8
            lora_alpha: 32
            target_modules: ["q_proj", "v_proj"]
            lora_dropout: 0.05
            bias: none
            task_type: "CAUSAL_LM"
          pretrained_kwargs:
            device_map: 0
            torch_dtype: bfloat16
trainer:
  accumulate_grad_batches: 1
  precision: bf16-true
  devices: 1
  logger:
    class_path: lightning.pytorch.loggers.TensorBoardLogger
    init_args:
      name: llama3_chat_rte_3b_qlora_zero_shot_test_only
      save_dir: lightning_logs
