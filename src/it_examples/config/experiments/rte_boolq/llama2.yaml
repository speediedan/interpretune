it_session:
  datamodule_cls: it_examples.experiments.rte_boolq.datamodules.Llama2RTEBoolqDataModule
  module_cls: it_examples.experiments.rte_boolq.modules.RTEBoolqLMHeadModule
  datamodule_cfg:
    class_path: interpretune.base.config.datamodule.ITDataModuleConfig
    init_args:
      model_name_or_path: meta-llama/Llama-2-7b-chat-hf
      task_name: rte
      train_batch_size: 2
      eval_batch_size: 2
      cust_tokenization_pattern: llama2-chat
      os_env_model_auth_key: LLAMA2_AUTH_KEY
      tokenizer_id_overrides:
        pad_token_id: 32000
      special_tokens_dict: {"pad_token": "<PAD>"}
      enable_datasets_cache: true
      prepare_data_map_cfg:
        batched: true
      data_collator_cfg:
        collator_class: transformers.DataCollatorWithPadding
      prompt_cfg:
        class_path: it_examples.experiments.rte_boolq.config.Llama2PromptConfig
      tokenizers_parallelism: false
      tokenizer_kwargs:
        use_fast: true  # by default true
        local_files_only: false
        padding_side: left
        model_input_names: ['input_ids', 'attention_mask']
      # defer_model_init: true  # uncomment along with signature_columns to test deferred model init mode
      # signature_columns: ['input_ids', 'attention_mask', 'position_ids', 'past_key_values', 'inputs_embeds', 'labels',
      #                     'use_cache', 'output_attentions', 'output_hidden_states', 'return_dict']
  module_cfg:
    class_path: it_examples.experiments.rte_boolq.config.RTEBoolqConfig
    init_args:
      zero_shot_cfg:
        class_path: it_examples.experiments.rte_boolq.config.RTEBoolqZeroShotClassificationConfig
        init_args:
          enabled: true
          lm_generation_cfg:
            class_path: interpretune.base.config.mixins.HFGenerationConfig
            init_args:
              kwargs:
                max_new_tokens: 5
      hf_from_pretrained_cfg:
        class_path: interpretune.base.config.module.HFFromPretrainedConfig
        init_args:
          use_model_cache: false
          model_head: transformers.LlamaForCausalLM
          activation_checkpointing: false
          lora_cfg:
            r: 8
            lora_alpha: 32
            target_modules: ["q_proj", "v_proj"]
            lora_dropout: 0.05
            bias: none
            task_type: "CAUSAL_LM"
