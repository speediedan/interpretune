it_session:
  session_cfg:
    adapter_ctx: [core]
    init_args:
      module_cfg:
        experiment_tag: rte_small_noquant_gen_ft_schedule_only
        init_args:
          hf_from_pretrained_cfg:
            class_path: interpretune.base.config.module.HFFromPretrainedConfig
            init_args:
              pretrained_kwargs:
                device_map: cpu
                torch_dtype: bfloat16
trainer:
  devices: 1
  callbacks:
  - class_path: finetuning_scheduler.FinetuningScheduler
    init_args:
      gen_ft_sched_only: true
      # ft_schedule: /home/speediedan/repos/interpretune/src/it_examples/config/lightning/ft_schedules/ITModule_ft_schedule_llama2_7b.yaml
      # max_depth: 1
  - class_path: finetuning_scheduler.FTSCheckpoint
    init_args:
      save_top_k: 1
      monitor: val_loss
      verbose: true
  - class_path: finetuning_scheduler.FTSEarlyStopping
    init_args:
      monitor: val_loss
      min_delta: 0.001
      patience: 2 # limited patience for example
      verbose: true
      mode: min
  logger:
    class_path: lightning.pytorch.loggers.TensorBoardLogger
    init_args:
      name: gpt2_notl_rte_small_noquant_lm_gen_only
