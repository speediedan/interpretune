it_session:
  module_cfg:
    init_args:
      experiment_tag: lightning_tl_rte_small_noquant_test
      # N.B. We can't share device_map: cpu at experiment-level because the full from_pretrained_cfg because while
      #      jsonargparse will update/inherit init_args, that behavior doesn't work with normal objects like this dict
      from_pretrained_cfg:
        device_map: cpu
        torch_dtype: bfloat16
      tl_from_pretrained_cfg:
        class_path: interpretune.plugins.transformer_lens.ITLensFromPretrainedConfig
        init_args:
          device: cuda
          dtype: bfloat16
trainer:
  precision: bf16-true
  accumulate_grad_batches: 1
  accelerator: gpu  # TODO: optimize tl model device loading/movement
  strategy: auto
  devices: 1
  logger:
    class_path: lightning.pytorch.loggers.TensorBoardLogger
    init_args:
      name: gpt2_tl_rte_small_noquant_test
