data.class_path: it_examples.experiments.rte_boolq.lightning.GPT2RTEBoolqLightningDataModule
model.class_path: it_examples.experiments.rte_boolq.lightning.GPT2ITLensLightningModule
itdm_cfg:
  class_path: interpretune.base.config.datamodule.ITDataModuleConfig
  init_args:
    model_name_or_path: gpt2
    #model_name_or_path: gpt2-large
    task_name: rte
    train_batch_size: 32
    eval_batch_size: 32
    tokenizer_id_overrides:
      pad_token_id: 50256
    prompt_cfg:
      class_path: it_examples.experiments.rte_boolq.core.RTEBoolqPromptConfig
      init_args:
        cust_task_prompt:
          context: '<|PREMISE|>:'
          question: '<|HYPOTHESIS|>:'
it_cfg:
  class_path: interpretune.plugins.transformer_lens.ITLensConfig
  init_args:
    # zero_shot_cfg:
    #   enabled: true
    #   lm_generation_cfg:
    #     max_new_tokens: 5
    from_pretrained_cfg:
      #device_map: {"":0}
      #device_map: 0
      #device_map: cpu  # set already in shared gpt2
      torch_dtype: float32
    auto_model_cfg:
      model_head: transformers.GPT2LMHeadModel
    tl_from_pretrained_cfg:
      class_path: interpretune.plugins.transformer_lens.ITLensFromPretrainedConfig
      init_args:
        enabled: true
        model_name: gpt2-small
        fold_ln: true
        center_writing_weights: true
        center_unembed: true
        refactor_factored_attn_matrices: false
        checkpoint_index: null
        checkpoint_value: null
        # hf_model: transformers.GPT2LMHeadModel
        device: cuda
        n_devices: 1
        #tokenizer: null # pass tokenizer during setup
        move_to_device: true
        fold_value_biases: true
        default_prepend_bos: true
        default_padding_side: right
        dtype: float32
    activation_checkpointing: false
    #bitsandbytesconfig: null
    #lora_cfg: null
    # auto_model_cfg:
    #   model_head: transformers.GPT2ForSequenceClassification
trainer:
  precision: 32
  #max_epochs: 600
  accumulate_grad_batches: 1
  devices: 1
  # callbacks:
  # - class_path: finetuning_scheduler.FinetuningScheduler
  #   init_args:
  #     ft_schedule: /home/speediedan/repos/interpretune/src/it_examples/config/lightning/ft_schedules/ITModule_ft_schedule_gpt2_small_sc_notl_max_trans_warmup.yaml
  #     max_depth: 1
  #     epoch_transitions_only: true
  #     logging_level: 10  # enable DEBUG logging to trace all reinitializations
  # - class_path: finetuning_scheduler.FTSCheckpoint
  #   init_args:
  #     save_top_k: 1
  #     monitor: val_loss
  #     verbose: true
  # - class_path: finetuning_scheduler.FTSEarlyStopping
  #   init_args:
  #     monitor: val_loss
  #     min_delta: 0.001
  #     patience: 2 # limited patience for example
  #     verbose: true
  #     mode: min
  # note logging is disabled in predict mode (might consider adding a special interpret subcommand to the PL trainer)
  logger:
    class_path: lightning.pytorch.loggers.TensorBoardLogger
    init_args:
      name: gpt2_tl_rte_small_noquant_predict
