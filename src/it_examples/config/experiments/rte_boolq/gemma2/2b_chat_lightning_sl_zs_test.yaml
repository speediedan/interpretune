session_cfg:
  adapter_ctx: [lightning, sae_lens]
  datamodule_cls: it_examples.experiments.rte_boolq.RTEBoolqDataModule
  module_cls: it_examples.experiments.rte_boolq.RTEBoolqModule
  datamodule_cfg:
    class_path: interpretune.base.config.datamodule.ITDataModuleConfig
    init_args:
      model_name_or_path: google/gemma-2-2b-it
      task_name: rte
      train_batch_size: 2
      eval_batch_size: 2
      cust_tokenization_pattern: gemma2-chat
      os_env_model_auth_key: HF_GATED_PUBLIC_REPO_AUTH_KEY
      enable_datasets_cache: true
      prepare_data_map_cfg:
        batched: true
      data_collator_cfg:
        collator_class: transformers.DataCollatorWithPadding
      prompt_cfg:
        class_path: it_examples.example_prompt_configs.RTEBoolqGemma2PromptConfig
      tokenizers_parallelism: false
      signature_columns: ['input', 'attention_mask', 'labels']
      tokenizer_kwargs:
        #padding_side: right  # to pad on right
        padding_side: left  # to pad on left
        add_bos_token: true
        local_files_only: false
        model_input_names:
        - input
        - attention_mask
  module_cfg:
    class_path: it_examples.experiments.rte_boolq.RTEBoolqSLConfig
    init_args:
      experiment_tag: gemma2_2b_chat_lightning_sl_zs_test  # TODO: cli tie experiment tag and logging name
      model_cfg:
        _attn_implementation: eager
      tl_cfg:
        class_path: interpretune.adapters.transformer_lens.ITLensFromPretrainedNoProcessingConfig
        init_args:
          model_name: gemma-2-2b-it
          device: cuda
          dtype: bfloat16
          #default_padding_side: right  # to pad on right
          default_padding_side: left  # to pad on left
      sae_cfgs:
        class_path: interpretune.adapters.sae_lens.SAELensFromPretrainedConfig
        init_args:
          release: gemma-scope-2b-pt-res-canonical
          sae_id: layer_25/width_16k/canonical
          device: cuda
          dtype: torch.bfloat16
      zero_shot_cfg:
        class_path: it_examples.experiments.rte_boolq.RTEBoolqZeroShotClassificationConfig
        init_args:
          enabled: true
          lm_generation_cfg:
            class_path: interpretune.adapters.transformer_lens.TLensGenerationConfig
            init_args:
              ###################### to pad on left
              padding_side: left
              stop_at_eos: true
              ###################### to pad on right
              #padding_side: right
              #stop_at_eos: false
              ######################################
              use_past_kv_cache: false  # there was a dtype issue when using cache + bfloat16
              max_new_tokens: 5
      hf_from_pretrained_cfg:
        class_path: interpretune.base.config.module.HFFromPretrainedConfig
        init_args:
          use_model_cache: false
          model_head: transformers.Gemma2ForCausalLM
          activation_checkpointing: false
          pretrained_kwargs:
            device_map: 0
            torch_dtype: bfloat16
trainer:
  accumulate_grad_batches: 1
  precision: bf16-true
  devices: 1
  logger:
    class_path: lightning.pytorch.loggers.TensorBoardLogger
    init_args:
      name: gemma2_2b_chat_lightning_sl_zs_test
      save_dir: lightning_logs
