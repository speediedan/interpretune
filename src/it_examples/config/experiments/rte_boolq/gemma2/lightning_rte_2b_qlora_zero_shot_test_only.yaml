session_cfg:
  adapter_ctx: [lightning]
  datamodule_cls: it_examples.experiments.rte_boolq.RTEBoolqDataModule
  module_cls: it_examples.experiments.rte_boolq.RTEBoolqModule
  datamodule_cfg:
    class_path: interpretune.base.config.datamodule.ITDataModuleConfig
    init_args:
      model_name_or_path: google/gemma-2-2b
      task_name: rte
      train_batch_size: 2
      eval_batch_size: 2
      #cust_tokenization_pattern: llama3-chat
      os_env_model_auth_key: HF_GATED_PUBLIC_REPO_AUTH_KEY
      #tokenizer_id_overrides:
        #pad_token_id: 128004
      #special_tokens_dict: {"pad_token": "<|finetune_right_pad_id|>"}
      enable_datasets_cache: true
      prepare_data_map_cfg:
        batched: true
      data_collator_cfg:
        collator_class: transformers.DataCollatorWithPadding
      prompt_cfg:
        class_path: it_examples.experiments.rte_boolq.RTEBoolqPromptConfig
      tokenizers_parallelism: false
      tokenizer_name: google/gemma-2-2b
      tokenizer_kwargs:
        use_fast: true  # by default true
        local_files_only: false
        add_bos_token: true
        padding_side: left  # NOTE: this should be changed to right for finetuning it seems for llama3 family
        model_input_names: ['input_ids', 'attention_mask']
      # defer_model_init: true  # uncomment along with signature_columns to test deferred model init mode
      # signature_columns: ['input_ids', 'attention_mask', 'position_ids', 'past_key_values', 'inputs_embeds', 'labels',
      #                     'use_cache', 'output_attentions', 'output_hidden_states', 'return_dict']
  module_cfg:
    class_path: it_examples.experiments.rte_boolq.RTEBoolqConfig
    init_args:
      model_cfg:
        _attn_implementation: eager
      zero_shot_cfg:
        class_path: it_examples.experiments.rte_boolq.RTEBoolqZeroShotClassificationConfig
        init_args:
          enabled: true
          lm_generation_cfg:
            class_path: interpretune.base.config.mixins.HFGenerationConfig
            init_args:
              model_config:
                use_cache: false  # there was a dtype issue when using cache + bfloat16
                max_new_tokens: 5
      hf_from_pretrained_cfg:
        class_path: interpretune.base.config.module.HFFromPretrainedConfig
        init_args:
          use_model_cache: false
          model_head: transformers.Gemma2ForCausalLM
          activation_checkpointing: false
          # note: naive use of the non IT gemma2 version quantized only reduces (already terrible performance relative
          #       to llama3_2 IT) by ~1% relative to bf16 non-quantized
          bitsandbytesconfig:
            load_in_4bit: True
            bnb_4bit_use_double_quant: True
            bnb_4bit_quant_type: nf4
            bnb_4bit_compute_dtype: bfloat16
          lora_cfg:
            r: 8
            target_modules: ["q_proj", "o_proj", "k_proj", "v_proj", "gate_proj", "up_proj", "down_proj"]
            task_type: "CAUSAL_LM"
          pretrained_kwargs:
            device_map: 0
            torch_dtype: bfloat16
trainer:
  accumulate_grad_batches: 1
  precision: bf16-true
  devices: 1
  logger:
    class_path: lightning.pytorch.loggers.TensorBoardLogger
    init_args:
      name: gemma2_rte_2b_qlora_zero_shot_test_only
      save_dir: lightning_logs
