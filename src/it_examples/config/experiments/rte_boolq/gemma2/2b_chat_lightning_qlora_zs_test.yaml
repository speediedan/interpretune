session_cfg:
  adapter_ctx: [lightning]
  datamodule_cls: it_examples.experiments.rte_boolq.RTEBoolqDataModule
  module_cls: it_examples.experiments.rte_boolq.RTEBoolqModule
  datamodule_cfg:
    class_path: interpretune.config.datamodule.ITDataModuleConfig
    init_args:
      model_name_or_path: google/gemma-2-2b-it
      task_name: rte
      train_batch_size: 2
      eval_batch_size: 2
      cust_tokenization_pattern: gemma2-chat
      os_env_model_auth_key: HF_GATED_PUBLIC_REPO_AUTH_KEY
      enable_datasets_cache: true
      prepare_data_map_cfg:
        batched: true
      data_collator_cfg:
        collator_class: transformers.DataCollatorWithPadding
      prompt_cfg:
        class_path: it_examples.example_prompt_configs.RTEBoolqGemma2PromptConfig
      tokenizers_parallelism: false
      tokenizer_name: google/gemma-2-2b-it
      tokenizer_kwargs:
        local_files_only: false
        add_bos_token: true
        padding_side: left
        model_input_names: ['input_ids', 'attention_mask']
  module_cfg:
    class_path: it_examples.experiments.rte_boolq.RTEBoolqConfig
    init_args:
      experiment_tag: gemma2_2b_chat_lightning_qlora_zs_test
      model_cfg:
        _attn_implementation: eager
      generative_step_cfg:
        class_path: it_examples.experiments.rte_boolq.RTEBoolqGenerativeClassificationConfig
        init_args:
          enabled: true
          lm_generation_cfg:
            class_path: interpretune.config.mixins.HFGenerationConfig
            init_args:
              model_config:
                use_cache: false  # there was a dtype issue when using cache + bfloat16
                max_new_tokens: 5
      hf_from_pretrained_cfg:
        class_path: interpretune.config.module.HFFromPretrainedConfig
        init_args:
          use_model_cache: false
          model_head: transformers.Gemma2ForCausalLM
          activation_checkpointing: false
          bitsandbytesconfig:
            load_in_4bit: True
            bnb_4bit_use_double_quant: True
            bnb_4bit_quant_type: nf4
            bnb_4bit_compute_dtype: bfloat16
          lora_cfg:
            r: 8
            target_modules: ["q_proj", "o_proj", "k_proj", "v_proj", "gate_proj", "up_proj", "down_proj"]
            task_type: "CAUSAL_LM"
          pretrained_kwargs:
            device_map: 0
            torch_dtype: bfloat16
trainer:
  accumulate_grad_batches: 1
  precision: bf16-true
  devices: 1
  logger:
    class_path: lightning.pytorch.loggers.TensorBoardLogger
    init_args:
      name: gemma2_2b_chat_lightning_qlora_zs_test
      save_dir: lightning_logs
