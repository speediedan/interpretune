##################################
# Cust Transformer
##################################
cust.rte:
  reg_info:
    model_src_key: cust
    task_name: rte
    adapter_combinations:
      - core
      - lightning
    description: Basic example, custom transformer with supported adapter compositions
  shared_config:
    task_name: pytest_rte_pt
    tokenizer_name: gpt2
    tokenizer_id_overrides:
      pad_token_id: 50256
    tokenizer_kwargs:
        model_input_names: ['tokens']
        padding_side: left
        add_bos_token: true
  registered_cfg:
    datamodule_cfg:
      prompt_cfg:
        class_path: it_examples.experiments.rte_boolq.RTEBoolqPromptConfig
      signature_columns: ['tokens', 'labels']
      text_fields: ["premise", "hypothesis"]
      enable_datasets_cache: True
      train_batch_size: 2
      eval_batch_size: 2
    module_cfg:
      class_path: it_examples.experiments.rte_boolq.RTEBoolqConfig
      init_args:
        model_cfg:
          device: cpu
          dtype: float32
          model_args:
            max_seq_len: 200
        generative_step_cfg:
          class_path: it_examples.experiments.rte_boolq.RTEBoolqGenerativeClassificationConfig
          init_args:
            enabled: True
            lm_generation_cfg:
              class_path: interpretune.config.mixins.CoreGenerationConfig
              init_args:
                max_new_tokens: 2
    datamodule_cls:
      class_path: tests.modules.FingerprintTestITDataModule
cust.rte.transformer_lens:
  reg_info:
    model_src_key: cust
    task_name: rte
    adapter_combinations:
      - [core, transformer_lens]
      - [lightning, transformer_lens]
    description: Custom transformer with Transformer Lens and supported adapter compositions
  shared_config:
    task_name: pytest_rte_tl
    model_name_or_path: gpt2
    tokenizer_id_overrides:
      pad_token_id: 50256
    tokenizer_kwargs:
      model_input_names: ['input']
      padding_side: left
      add_bos_token: true
  registered_cfg:
    datamodule_cfg:
      prompt_cfg:
        class_path: it_examples.experiments.rte_boolq.RTEBoolqPromptConfig
      signature_columns: ['input', 'labels']
      text_fields: ["premise", "hypothesis"]
      enable_datasets_cache: True
      train_batch_size: 2
      eval_batch_size: 2
    module_cfg:
      class_path: it_examples.experiments.rte_boolq.RTEBoolqTLConfig
      init_args:
        generative_step_cfg:
          class_path: it_examples.experiments.rte_boolq.RTEBoolqGenerativeClassificationConfig
          init_args:
            enabled: True
            lm_generation_cfg:
              class_path: interpretune.config.transformer_lens.TLensGenerationConfig
              init_args:
                  max_new_tokens: 1
        tl_cfg:
          class_path: interpretune.config.transformer_lens.ITLensCustomConfig
          init_args:
            cfg:
              n_layers: 2
              d_mlp: 10
              d_model: 10
              d_head: 5
              n_heads: 2
              n_ctx: 200
              act_fn: 'relu'
              tokenizer_name: 'gpt2'
    datamodule_cls:
      class_path: tests.modules.FingerprintTestITDataModule
cust.rte.sae_lens:
  reg_info:
    model_src_key: cust
    task_name: rte
    adapter_combinations:
      - [core, sae_lens]
      - [lightning, sae_lens]
    description: Basic SL example, GPT2 with supported adapter compositions
  shared_config:
    task_name: pytest_rte_tl
    tokenizer_name: gpt2
    tokenizer_id_overrides:
      pad_token_id: 50256
    tokenizer_kwargs:
      model_input_names: ['input']
      padding_side: left
      add_bos_token: True
  registered_cfg:
    datamodule_cfg:
      prompt_cfg:
        class_path: it_examples.experiments.rte_boolq.RTEBoolqPromptConfig
      signature_columns: ['input', 'labels']
      text_fields: ["premise", "hypothesis"]
      enable_datasets_cache: True
      train_batch_size: 2
      eval_batch_size: 2
    module_cfg:
      class_path: it_examples.experiments.rte_boolq.RTEBoolqSLConfig
      init_args:
        generative_step_cfg:
          class_path: it_examples.experiments.rte_boolq.RTEBoolqGenerativeClassificationConfig
          init_args:
            enabled: True
            lm_generation_cfg:
              class_path: interpretune.config.transformer_lens.TLensGenerationConfig
              init_args:
                max_new_tokens: 1
        tl_cfg:
          class_path: interpretune.config.transformer_lens.ITLensCustomConfig
          init_args:
            cfg:
              n_layers: 1
              d_mlp: 10
              d_model: 10
              d_head: 5
              n_heads: 2
              n_ctx: 200
              act_fn: 'relu'
              tokenizer_name: 'gpt2'
        sae_cfgs:
          class_path: interpretune.config.sae_lens.SAELensCustomConfig
          init_args:
            cfg:
              architecture: standard
              d_in: 10
              d_sae: 20
              dtype: float32
              device: cuda
              model_name: cust
              hook_name: blocks.0.hook_resid_pre
              hook_layer: 0
              hook_head_index: None
              activation_fn_str: relu
              prepend_bos: True
              context_size: 200
              dataset_path: test
              apply_b_dec_to_input: False
              finetuning_scaling_factor: False
              sae_lens_training_version: None
              normalize_activations: none
##################################
# GPT2
##################################
gpt2.rte:
  reg_info:
    model_src_key: gpt2
    task_name: rte
    adapter_combinations:
      - core
      - lightning
    description: Basic example, GPT2 with supported adapter compositions
  shared_config:
    task_name: pytest_rte_hf
    model_name_or_path: gpt2
    tokenizer_id_overrides:
      pad_token_id: 50256
    tokenizer_kwargs:
        model_input_names: ['input_ids', 'attention_mask']
        padding_side: left
        add_bos_token: true
  registered_cfg:
    datamodule_cfg:
      prompt_cfg:
        class_path: it_examples.experiments.rte_boolq.RTEBoolqPromptConfig
      signature_columns: ['input_ids', 'attention_mask', 'labels']
      text_fields: ["premise", "hypothesis"]
      enable_datasets_cache: True
      train_batch_size: 2
      eval_batch_size: 2
    module_cfg:
      class_path: it_examples.experiments.rte_boolq.RTEBoolqConfig
      init_args:
        generative_step_cfg:
          class_path: it_examples.experiments.rte_boolq.RTEBoolqGenerativeClassificationConfig
          init_args:
            enabled: True
            lm_generation_cfg:
              class_path: interpretune.config.mixins.HFGenerationConfig
              init_args:
                model_config:
                  max_new_tokens: 3
        hf_from_pretrained_cfg:
          class_path: interpretune.config.mixins.HFFromPretrainedConfig
          init_args:
            pretrained_kwargs:
              device_map: cpu
              torch_dtype: float32
            model_head: transformers.GPT2LMHeadModel
    datamodule_cls:
      class_path: tests.modules.FingerprintTestITDataModule
gpt2.rte.transformer_lens:
  reg_info:
    model_src_key: gpt2
    task_name: rte
    adapter_combinations:
      - [core, transformer_lens]
      - [lightning, transformer_lens]
    description: Basic TL example, GPT2 with supported adapter compositions
  shared_config:
    task_name: pytest_rte_tl
    model_name_or_path: gpt2
    tokenizer_id_overrides:
      pad_token_id: 50256
    tokenizer_kwargs:
      model_input_names: ['input']
      padding_side: left
      add_bos_token: false
  registered_cfg:
    datamodule_cfg:
      prompt_cfg:
        class_path: it_examples.experiments.rte_boolq.RTEBoolqPromptConfig
      signature_columns: ['input', 'labels']
      text_fields: ["premise", "hypothesis"]
      enable_datasets_cache: True
      train_batch_size: 2
      eval_batch_size: 2
    module_cfg:
      class_path: interpretune.config.module.ITConfig
      init_args:
        auto_comp_cfg:
          class_path: interpretune.config.shared.AutoCompConfig
          init_args:
            module_cfg_name: RTEBoolqConfig
            module_cfg_mixin:
              class_path: it_examples.experiments.rte_boolq.RTEBoolqEntailmentMapping
              import_only: True
            target_adapters: transformer_lens
        generative_step_cfg:
          class_path: it_examples.experiments.rte_boolq.RTEBoolqGenerativeClassificationConfig
          init_args:
            enabled: True
            lm_generation_cfg:
              class_path: interpretune.config.transformer_lens.TLensGenerationConfig
              init_args:
                  max_new_tokens: 1
        hf_from_pretrained_cfg:
          class_path: interpretune.config.mixins.HFFromPretrainedConfig
          init_args:
            pretrained_kwargs:
              device_map: cpu
              torch_dtype: float32
            model_head: transformers.GPT2LMHeadModel
        tl_cfg:
          class_path: interpretune.config.transformer_lens.ITLensFromPretrainedConfig
    datamodule_cls:
      class_path: tests.modules.FingerprintTestITDataModule
    module_cls:
      class_path: tests.modules.TestITModule  # validate explicit class_path based module_cls registration
gpt2.rte.sae_lens:
  reg_info:
    model_src_key: gpt2
    task_name: rte
    adapter_combinations:
      - [core, sae_lens]
      - [lightning, sae_lens]
    description: Basic SL example, GPT2 with supported adapter compositions
  shared_config:
    task_name: pytest_rte_tl
    model_name_or_path: gpt2
    tokenizer_id_overrides:
      pad_token_id: 50256
    tokenizer_kwargs:
      model_input_names: ['input']
      padding_side: left
      add_bos_token: true
  registered_cfg:
    datamodule_cfg:
      prompt_cfg:
        class_path: it_examples.experiments.rte_boolq.RTEBoolqPromptConfig
      signature_columns: ['input', 'labels']
      text_fields: ["premise", "hypothesis"]
      enable_datasets_cache: True
      train_batch_size: 2
      eval_batch_size: 2
    module_cfg:
      class_path: interpretune.config.module.ITConfig
      init_args:
        auto_comp_cfg:
          class_path: interpretune.config.shared.AutoCompConfig
          init_args:
            module_cfg_name: RTEBoolqConfig
            module_cfg_mixin:
              class_path: it_examples.experiments.rte_boolq.RTEBoolqEntailmentMapping
              import_only: True
        generative_step_cfg:
          class_path: it_examples.experiments.rte_boolq.RTEBoolqGenerativeClassificationConfig
          init_args:
            enabled: True
            lm_generation_cfg:
              class_path: interpretune.config.transformer_lens.TLensGenerationConfig
              init_args:
                max_new_tokens: 1
        hf_from_pretrained_cfg:
          class_path: interpretune.config.mixins.HFFromPretrainedConfig
          init_args:
            pretrained_kwargs:
              device_map: cpu
              torch_dtype: float32
            model_head: transformers.GPT2LMHeadModel
        tl_cfg:
          class_path: interpretune.config.transformer_lens.ITLensFromPretrainedNoProcessingConfig
        sae_cfgs:
          - class_path: interpretune.config.sae_lens.SAELensFromPretrainedConfig
            init_args:
              release: gpt2-small-res-jb
              sae_id: blocks.0.hook_resid_pre
              device: cuda
          - class_path: interpretune.config.sae_lens.SAELensFromPretrainedConfig
            init_args:
              release: gpt2-small-res-jb
              sae_id: blocks.1.hook_resid_pre
              device: cuda
gpt2.rte_demo.sae_lens:
  reg_info:
    model_src_key: gpt2
    task_name: rte_demo
    adapter_combinations:
      - [core, sae_lens]
      - [lightning, sae_lens]
    description: Basic SL example, GPT2 with supported adapter compositions
  shared_config:
    task_name: rte
    model_name_or_path: gpt2
    tokenizer_id_overrides:
      pad_token_id: 50256
    tokenizer_kwargs:
      model_input_names: ['input']
      padding_side: left
      add_bos_token: true
  registered_cfg:
    datamodule_cfg:
      prompt_cfg:
        class_path: it_examples.experiments.rte_boolq.RTEBoolqPromptConfig
      signature_columns: ['input', 'labels']
      text_fields: ["premise", "hypothesis"]
      enable_datasets_cache: True
      train_batch_size: 2
      eval_batch_size: 2
    module_cfg:
      class_path: interpretune.config.module.ITConfig
      init_args:
        auto_comp_cfg:
          class_path: interpretune.config.shared.AutoCompConfig
          init_args:
            module_cfg_name: RTEBoolqConfig
            module_cfg_mixin:
              class_path: it_examples.experiments.rte_boolq.RTEBoolqEntailmentMapping
              import_only: True
        generative_step_cfg:
          class_path: it_examples.experiments.rte_boolq.RTEBoolqGenerativeClassificationConfig
          init_args:
            enabled: True
            lm_generation_cfg:
              class_path: interpretune.config.transformer_lens.TLensGenerationConfig
              init_args:
                max_new_tokens: 1
        hf_from_pretrained_cfg:
          class_path: interpretune.config.mixins.HFFromPretrainedConfig
          init_args:
            pretrained_kwargs:
              device_map: cpu
              torch_dtype: float32
            model_head: transformers.GPT2LMHeadModel
        tl_cfg:
          class_path: interpretune.config.transformer_lens.ITLensFromPretrainedNoProcessingConfig
        sae_cfgs:
          - class_path: interpretune.config.sae_lens.SAELensFromPretrainedConfig
            init_args:
              release: gpt2-small-res-jb
              sae_id: blocks.0.hook_resid_pre
              device: cuda
          - class_path: interpretune.config.sae_lens.SAELensFromPretrainedConfig
            init_args:
              release: gpt2-small-res-jb
              sae_id: blocks.1.hook_resid_pre
              device: cuda
    datamodule_cls:
      class_path: it_examples.experiments.rte_boolq.RTEBoolqDataModule
    module_cls:
      class_path: it_examples.experiments.rte_boolq.RTEBoolqModule
##################################
# Gemma2
##################################
gemma2.rte:
  reg_info:
    model_src_key: gemma2
    task_name: rte
    adapter_combinations:
      - core
      - lightning
    description: Basic example, Gemma2 with supported adapter compositions
  shared_config:
    task_name: pytest_rte_hf
    os_env_model_auth_key: HF_GATED_PUBLIC_REPO_AUTH_KEY
    model_name_or_path: google/gemma-2-2b-it
    tokenizer_kwargs:
        model_input_names: ['input_ids', 'attention_mask']
        padding_side: left
        add_bos_token: true
  registered_cfg:
    datamodule_cfg:
      prompt_cfg:
        class_path: it_examples.example_prompt_configs.RTEBoolqGemma2PromptConfig
      signature_columns: ['input_ids', 'attention_mask', 'labels']
      text_fields: ["premise", "hypothesis"]
      enable_datasets_cache: True
      train_batch_size: 2
      eval_batch_size: 2
    module_cfg:
      class_path: it_examples.experiments.rte_boolq.RTEBoolqConfig
      init_args:
        generative_step_cfg:
          class_path: it_examples.experiments.rte_boolq.RTEBoolqGenerativeClassificationConfig
          init_args:
            enabled: True
            lm_generation_cfg:
              class_path: interpretune.config.mixins.HFGenerationConfig
              init_args:
                model_config:
                  max_new_tokens: 3
        hf_from_pretrained_cfg:
          class_path: interpretune.config.mixins.HFFromPretrainedConfig
          init_args:
            pretrained_kwargs:
              device_map: cpu
              torch_dtype: float32
            model_head: transformers.Gemma2ForCausalLM
gemma2.rte.sae_lens:
  reg_info:
    model_src_key: gemma2
    task_name: rte
    adapter_combinations:
      - [core, sae_lens]
      - [lightning, sae_lens]
    description: SL example, gemma2 with supported adapter compositions
  shared_config:
    task_name: pytest_rte_tl
    model_name_or_path: google/gemma-2-2b-it
    tokenizer_kwargs:
      model_input_names: ['input']
      padding_side: left
      add_bos_token: true
  registered_cfg:
    datamodule_cfg:
      prompt_cfg:
        class_path: it_examples.experiments.rte_boolq.RTEBoolqPromptConfig
      signature_columns: ['input', 'labels']
      text_fields: ["premise", "hypothesis"]
      enable_datasets_cache: True
      train_batch_size: 2
      eval_batch_size: 2
    module_cfg:
      class_path: it_examples.experiments.rte_boolq.RTEBoolqSLConfig
      init_args:
        generative_step_cfg:
          class_path: it_examples.experiments.rte_boolq.RTEBoolqGenerativeClassificationConfig
          init_args:
            enabled: True
            lm_generation_cfg:
              class_path: interpretune.config.transformer_lens.TLensGenerationConfig
              init_args:
                max_new_tokens: 1
        hf_from_pretrained_cfg:
          class_path: interpretune.config.mixins.HFFromPretrainedConfig
          init_args:
            pretrained_kwargs:
              device_map: cpu
              torch_dtype: float32
            model_head: transformers.Gemma2ForCausalLM
        tl_cfg:
          class_path: interpretune.config.transformer_lens.ITLensFromPretrainedNoProcessingConfig
        sae_cfgs:
          class_path: interpretune.config.sae_lens.SAELensFromPretrainedConfig
          init_args:
            release: gemma-scope-2b-pt-res-canonical
            sae_id: layer_25/width_16k/canonical
            device: cuda
gemma2.rte_demo.circuit_tracer:
  reg_info:
    model_src_key: gemma2
    task_name: rte
    adapter_combinations:
      - [core, circuit_tracer]
      - [lightning, circuit_tracer]
    description: Basic Circuit Tracer example, Gemma2-2b (non-instruction tuned) with supported adapter compositions
  shared_config:
    task_name: rte
    os_env_model_auth_key: HF_GATED_PUBLIC_REPO_AUTH_KEY
    model_name_or_path: google/gemma-2-2b
    tokenizer_kwargs:
        model_input_names: ['input']
        padding_side: left
        add_bos_token: true
  registered_cfg:
    datamodule_cfg:
      prompt_cfg:
        class_path: it_examples.experiments.rte_boolq.RTEBoolqPromptConfig
      signature_columns: ['input', 'labels']
      text_fields: ["premise", "hypothesis"]
      enable_datasets_cache: False
      train_batch_size: 2
      eval_batch_size: 2
    module_cfg:
      class_path: interpretune.config.module.ITConfig
      init_args:
        auto_comp_cfg:
          class_path: interpretune.config.shared.AutoCompConfig
          init_args:
            module_cfg_name: RTEBoolqConfig
            module_cfg_mixin:
              class_path: it_examples.experiments.rte_boolq.RTEBoolqEntailmentMapping
              import_only: True
        generative_step_cfg:
          class_path: it_examples.experiments.rte_boolq.RTEBoolqGenerativeClassificationConfig
          init_args:
            enabled: True
            lm_generation_cfg:
              class_path: interpretune.config.transformer_lens.TLensGenerationConfig
              init_args:
                max_new_tokens: 1
        hf_from_pretrained_cfg:
          class_path: interpretune.config.mixins.HFFromPretrainedConfig
          init_args:
            pretrained_kwargs:
              device_map: cpu
              torch_dtype: float32
            model_head: transformers.Gemma2ForCausalLM
        tl_cfg:
          class_path: interpretune.config.transformer_lens.ITLensFromPretrainedNoProcessingConfig
          init_args:
            model_name: gemma-2-2b
            default_padding_side: left  # to pad on left
        circuit_tracer_cfg:
          class_path: interpretune.config.circuit_tracer.CircuitTracerConfig
          init_args:
            transcoder_set: "gemma"
            max_n_logits: 10
            analysis_target_tokens: ['▁Dallas','▁Austin']
            desired_logit_prob: 0.95
            max_feature_nodes: 8192
            batch_size: 256
            offload: 'cpu'
            verbose: true
            use_neuronpedia: false
    datamodule_cls:
      class_path: it_examples.experiments.rte_boolq.RTEBoolqDataModule
    module_cls:
      class_path: it_examples.experiments.rte_boolq.RTEBoolqModule
gemma2.rte_demo.circuit_tracer_w_neuronpedia:
  reg_info:
    model_src_key: gemma2
    task_name: rte
    adapter_combinations:
      - [core, circuit_tracer]
      - [lightning, circuit_tracer]
    description: Basic Circuit Tracer example, Gemma2-2b (non-instruction tuned) with supported adapter compositions
  shared_config:
    task_name: rte
    os_env_model_auth_key: HF_GATED_PUBLIC_REPO_AUTH_KEY
    model_name_or_path: google/gemma-2-2b
    tokenizer_kwargs:
        model_input_names: ['input']
        padding_side: left
        add_bos_token: true
  registered_cfg:
    datamodule_cfg:
      prompt_cfg:
        class_path: it_examples.experiments.rte_boolq.RTEBoolqPromptConfig
      signature_columns: ['input', 'labels']
      text_fields: ["premise", "hypothesis"]
      enable_datasets_cache: False
      train_batch_size: 2
      eval_batch_size: 2
    module_cfg:
      class_path: interpretune.config.module.ITConfig
      init_args:
        auto_comp_cfg:
          class_path: interpretune.config.shared.AutoCompConfig
          init_args:
            module_cfg_name: RTEBoolqConfig
            module_cfg_mixin:
              class_path: it_examples.experiments.rte_boolq.RTEBoolqEntailmentMapping
              import_only: True
        generative_step_cfg:
          class_path: it_examples.experiments.rte_boolq.RTEBoolqGenerativeClassificationConfig
          init_args:
            enabled: True
            lm_generation_cfg:
              class_path: interpretune.config.transformer_lens.TLensGenerationConfig
              init_args:
                max_new_tokens: 1
        hf_from_pretrained_cfg:
          class_path: interpretune.config.mixins.HFFromPretrainedConfig
          init_args:
            pretrained_kwargs:
              device_map: cpu
              torch_dtype: float32
            model_head: transformers.Gemma2ForCausalLM
        tl_cfg:
          class_path: interpretune.config.transformer_lens.ITLensFromPretrainedNoProcessingConfig
          init_args:
            model_name: gemma-2-2b
            default_padding_side: left  # to pad on left
        circuit_tracer_cfg:
          class_path: interpretune.config.circuit_tracer.CircuitTracerConfig
          init_args:
            transcoder_set: "gemma"
            max_n_logits: 10
            analysis_target_tokens: ['▁Dallas','▁Austin']
            # target_token_ids: entailment_mapping_indices  # alternate approach to specifying specific target tokens
            desired_logit_prob: 0.95
            max_feature_nodes: 8192
            batch_size: 256
            offload: 'cpu'
            verbose: true
            use_neuronpedia: true
        neuronpedia_cfg:
          class_path: interpretune.extensions.neuronpedia.NeuronpediaConfig
          init_args:
            enabled: true
            auto_transform: true
            # dynamic_np_schema_validation: true
            default_metadata:
              info:
                creator_name: "interpretune-user"
                creator_url: "https://github.com/speediedan/interpretune"
                generator:
                  name: "Interpretune Circuit Tracer Example"
                  version: "0.1.0.dev0"
                  url: "https://github.com/speediedan/interpretune"
              # feature_details:
              #   neuronpedia_source_set: "gemmascope-transcoder-16k"
    datamodule_cls:
      class_path: it_examples.experiments.rte_boolq.RTEBoolqDataModule
    module_cls:
      class_path: it_examples.experiments.rte_boolq.RTEBoolqModule

##################################
# Llama3
##################################
llama3.rte:
  reg_info:
    model_src_key: llama3
    task_name: rte
    adapter_combinations:
      - core
      - lightning
    description: Basic example, Llama3 with supported adapter compositions
  shared_config:
    task_name: pytest_rte_hf
    os_env_model_auth_key: HF_GATED_PUBLIC_REPO_AUTH_KEY
    model_name_or_path: meta-llama/Llama-3.2-3B-Instruct
    tokenizer_id_overrides:
      pad_token_id: 128004
    tokenizer_kwargs:
        model_input_names: ['input_ids', 'attention_mask']
        # NOTE: this configuration is for testing, for finetuning, llama3 should be changed to right padding
        padding_side: left
        add_bos_token: false
  registered_cfg:
    datamodule_cfg:
      prompt_cfg:
        class_path: it_examples.example_prompt_configs.RTEBoolqLlama3PromptConfig
      signature_columns: ['input_ids', 'attention_mask', 'labels']
      text_fields: ["premise", "hypothesis"]
      enable_datasets_cache: True
      train_batch_size: 2
      eval_batch_size: 2
      special_tokens_dict: {"pad_token": "<|finetune_right_pad_id|>"}
      cust_tokenization_pattern: llama3-chat
    module_cfg:
      class_path: it_examples.experiments.rte_boolq.RTEBoolqConfig
      init_args:
        generative_step_cfg:
          class_path: it_examples.experiments.rte_boolq.RTEBoolqGenerativeClassificationConfig
          init_args:
            enabled: True
            lm_generation_cfg:
              class_path: interpretune.config.mixins.HFGenerationConfig
              init_args:
                model_config:
                  max_new_tokens: 3
        hf_from_pretrained_cfg:
          class_path: interpretune.config.mixins.HFFromPretrainedConfig
          init_args:
            use_model_cache: false
            pretrained_kwargs:
              device_map: cpu
              torch_dtype: float32
            model_head: transformers.LlamaForCausalLM
            bitsandbytesconfig:
              load_in_4bit: True
              bnb_4bit_use_double_quant: True
              bnb_4bit_quant_type: nf4
              bnb_4bit_compute_dtype: bfloat16
            lora_cfg:
              r: 8
              lora_alpha: 32
              target_modules: ["q_proj", "v_proj"]
              lora_dropout: 0.05
              bias: none
              task_type: "CAUSAL_LM"
