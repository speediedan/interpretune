trigger:
  branches:
    include:
      - main
      - release/*
      - refs/tags/*
  paths:
    include:
      - setup.*
      - requirements.txt
      - pyproject.toml
      - .codecov.yml
      - tests/**
      - src/**
      - requirements/**
      - .azure-pipelines/**
    exclude:
      - scripts/**

pr:
  branches:
    include:
      # - '*'
      - main
      - release/*
  paths:
    include:
      - setup.*
      - requirements.txt
      - pyproject.toml
      - .codecov.yml
      - tests/**
      - src/**
      - requirements/**
      - .azure-pipelines/**
    exclude:
      - scripts/**
  drafts: false  # Only run for PRs that are "ready for review"

parameters:
  - name: reset_hf_cache
    displayName: Reset HF Cache
    type: boolean
    default: false

jobs:
  - job: pytest
    strategy:
      matrix:
        PyTorch_latest:
          image: speediedan/interpretune:py3.12-pt2.9.0-azpl-init
          scope: ""
    timeoutInMinutes: 100
    cancelTimeoutInMinutes: 2

    pool:
      name: default

    container:
      image: $(image)
      mapDockerSocket: false
      volumes:
        - /var/run/user/998/docker.sock:/var/run/docker.sock
        - /opt/az_pipeline_agent/_work/hf_cache/$(Agent.Name):/opt/interpretune/hf_cache:rw
      options: --gpus all --shm-size=512m
      env:
        HF_HOME: /opt/interpretune/hf_cache
        HF_DATASETS_CACHE: /opt/interpretune/hf_cache/datasets
        HF_HUB_CACHE: /opt/interpretune/hf_cache/hub
        TRANSFORMERS_CACHE: /opt/interpretune/hf_cache/transformers
        XDG_CACHE_HOME: /opt/interpretune/hf_cache
        APPLY_POST_UPGRADES: "false"

    workspace:
      clean: outputs

    steps:
      - bash: |
          echo "=== identity ==="
          id || true
          groups || true
          echo "=== mount/dir status ==="
          ls -ld /opt/interpretune /opt/interpretune/hf_cache 2>/dev/null || true
          stat -c "%n -> %U:%G %a" /opt/interpretune /opt/interpretune/hf_cache 2>/dev/null || true
          echo "=== touch test  ==="
          if touch /opt/interpretune/hf_cache/.write_test 2>/dev/null; then
            echo WRITE_OK
          else
            echo WRITE_FAIL
            echo "ERROR: Unable to write to /opt/interpretune/hf_cache"
            exit 1
          fi

        displayName: 'Verify runner hf_cache mount & permissions'

      - bash: |
          echo "=== Probing HF cache lock file accessibility ==="
          LOCK_FILE=$(find /opt/interpretune/hf_cache/datasets -name "*super_glue*rte*.lock" -type f 2>/dev/null | head -1)
          if [ -n "$LOCK_FILE" ]; then
            echo "Found lock file: $LOCK_FILE"
            ls -l "$LOCK_FILE"
            echo "Testing read access..."
            if cat "$LOCK_FILE" >/dev/null 2>&1; then
              echo "✓ Read access OK"
            else
              echo "✗ Read access FAILED (exit code: $?)"
            fi
            echo "Testing write access..."
            if touch "$LOCK_FILE" 2>&1; then
              echo "✓ Write access OK"
            else
              echo "✗ Write access FAILED (exit code: $?)"
              echo "LOCK FILE INACCESSIBLE - attempting to fix permissions"
              # Try to fix permissions before falling back to cache reset
              echo "Fixing HF cache dataset permissions..."
              # Fix lock files to be group-writable
              sudo find /opt/interpretune/hf_cache/datasets -type f -name "*.lock" -exec chmod 664 {} \; 2>/dev/null || true
              # Ensure directories have SGID and group write
              sudo find /opt/interpretune/hf_cache/datasets -type d -exec chmod g+ws {} \; 2>/dev/null || true
              echo "Permissions fix completed"
              # Test again after fix
              if touch "$LOCK_FILE" 2>&1; then
                echo "✓ Write access restored after permission fix"
              else
                echo "✗ Write access still failed - triggering cache reset"
                echo "##vso[task.setvariable variable=FORCE_CACHE_RESET]1"
              fi
            fi
            echo "Lock file ownership details:"
            stat "$LOCK_FILE" || true
            echo "Current user context:"
            id
          else
            echo "No super_glue RTE lock files found in cache"
          fi
        displayName: 'Debug: Probe HF cache lock file permissions'
        continueOnError: true

      - bash: |
          if [ "$FORCE_CACHE_RESET" = "1" ] || [ "${{ parameters.reset_hf_cache }}" = "true" ]; then
            echo "Clearing HF cache due to: $([ "$FORCE_CACHE_RESET" = "1" ] && echo "inaccessible lock files" || echo "manual reset parameter")"
            rm -rf /opt/interpretune/hf_cache/* || true
          else
            echo "HF cache reset not required"
          fi
        displayName: 'Maybe reset HF cache'

      - bash: |
          . /tmp/venvs/it_dev/bin/activate
          python -m pip install --upgrade pip setuptools setuptools-scm wheel build
          python -m pip install -r requirements/ci/requirements.txt -r requirements/ci/platform_dependent.txt --no-warn-script-location
          python -m pip install -e '.[test,examples,lightning]'  --no-warn-script-location
          if ([ "${APPLY_POST_UPGRADES:-}" = "1" ] || [ "${APPLY_POST_UPGRADES:-}" = "true" ]) && [ -s requirements/ci/post_upgrades.txt ]; then
            echo "Applying post-upgrades (requirements/ci/post_upgrades.txt)..."
            python -m pip install --upgrade -r requirements/ci/post_upgrades.txt --cache-dir "$PIP_CACHE_DIR"
          else
            echo "Skipping post-upgrades (either disabled or file empty)."
          fi
          python -m pip list
        displayName: 'Install dependencies'

      - bash: |
          . /tmp/venvs/it_dev/bin/activate
          python requirements/utils/collect_env_details.py
        displayName: 'Env details and package versions'

      - bash: |
          . /tmp/venvs/it_dev/bin/activate
          python -m coverage run --append --source src/interpretune -m pytest src/interpretune tests -v --junitxml=$(Build.Repository.LocalPath)/test-results.xml --durations=50
        displayName: 'Testing: standard'

      - bash: |
          . /tmp/venvs/it_dev/bin/activate
          export HF_GATED_PUBLIC_REPO_AUTH_KEY=$HF_GATED_PUBLIC_REPO_AUTH_KEY
          bash ./tests/special_tests.sh --mark_type=standalone
        displayName: 'Testing: standalone gpu'
        env:
          HF_GATED_PUBLIC_REPO_AUTH_KEY: $(HF_GATED_PUBLIC_REPO_AUTH_KEY)

      - bash: |
          . /tmp/venvs/it_dev/bin/activate
          bash ./tests/special_tests.sh --mark_type=profile_ci
        displayName: 'Testing: CI Profiling'

      - bash: |
          . /tmp/venvs/it_dev/bin/activate
          python -m coverage report
          python -m coverage xml
          python -m coverage html

          curl https://keybase.io/codecovsecurity/pgp_keys.asc | gpg --no-default-keyring --keyring trustedkeys.gpg --import
          curl -Os https://cli.codecov.io/latest/linux/codecov
          curl -Os https://cli.codecov.io/latest/linux/codecov.SHA256SUM
          curl -Os https://cli.codecov.io/latest/linux/codecov.SHA256SUM.sig
          gpg --no-default-keyring --keyring trustedkeys.gpg --verify codecov.SHA256SUM.sig codecov.SHA256SUM
          shasum -a 256 -c codecov.SHA256SUM
          chmod +x codecov
          ./codecov upload-process --slug 'speediedan/interpretune' -t $CODECOV_TOK --commit-sha $(Build.SourceVersion) --git-service 'github' -n "GPU-coverage" -F 'gpu,pytest' --env 'linux,azure' -f 'coverage.xml'
        displayName: 'Upload Coverage to Codecov'
        env:
          CODECOV_TOK: $(CODECOV_TOKEN)
      # - bash: |
      #     set -e
      #     . /tmp/venvs/it_dev/bin/activate
      #     python -m coverage run --append --source src/interpretune -m pytest src/it_examples -v
      #   # condition: notIn(variables['scope'], '2.0.1')
      #   displayName: 'Testing: Examples'


      # - bash: |
      #     . /tmp/venvs/it_dev/bin/activate
      #     mkdir -p /__w/_temp/kernel_cache
      #     bash ./tests/special_tests.sh --mark_type=profile
      #     bash ./tests/special_tests.sh --mark_type=optional
      #   # condition: notIn(variables['scope'], '2.0.1')
      #   env:
      #     PYTORCH_KERNEL_CACHE_PATH: "/__w/_temp/kernel_cache"
      #   displayName: 'Testing: Extended profile and optional tests'
      - bash: |
          # since we use rootless docker and userns-remapping, we need to ensure all files/directories in previous
          # steps that may have been written with Azure's `az_pipeline_agent_azpcontainer` user (100997 in the host
          # subuid range) are chmod'd or removed
          # some directories created by the userns-remapped container uids are removed by the agent at the beginning
          # of every run (in a step we don't control) so we proactively clean those up in this step (`.pytest_cache`
          # is the only folder in this category currently)
          echo "Adjusting ownership/permissions..."
          sudo chmod -R 775 /__w/2/s || true
          echo "Cleaning up ephemeral directories..."
          sudo rm -rf /__w/2/s/.pytest_cache || true

          echo "Fixing HF cache dataset permissions for next run..."
          # Fix lock files to be group-writable
          sudo find /opt/interpretune/hf_cache/datasets -type f -name "*.lock" -exec chmod 664 {} \; 2>/dev/null || true
          # Ensure directories have SGID and group write
          sudo find /opt/interpretune/hf_cache/datasets -type d -exec chmod g+ws {} \; 2>/dev/null || true

          echo 'Agent workspace and HF cache cleanup completed'
        condition: always()
        displayName: 'Cleaning up agent workspace'
